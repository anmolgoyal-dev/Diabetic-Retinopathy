{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL94jkvcr8Ee",
        "outputId": "8f922b9b-636b-4834-b5ea-58eb1237f857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SENKxzlZo0S5"
      },
      "outputs": [],
      "source": [
        "import os, sys, shutil, random\n",
        "root_address = '/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtZw0A411vyv",
        "outputId": "6fcd978b-cf53-4e3d-d7f9-6af2f3ca7dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash) (8.4.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash) (1.4.1)\n",
            "Installing collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUTEfk5L1qiw"
      },
      "outputs": [],
      "source": [
        "import imagehash\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLltyTYr1trE"
      },
      "outputs": [],
      "source": [
        "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0jRT_yR1uNQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "994f07d0-e73e-4f61-f00f-c3b47c9cfa2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id_code  diagnosis\n",
              "0  000c1434d8d7          2\n",
              "1  001639a390f0          4\n",
              "2  0024cdab0c1e          1\n",
              "3  002c21358ce6          0\n",
              "4  005b95c28852          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d49b094-9da1-4d5b-9dcb-246fe33fc466\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_code</th>\n",
              "      <th>diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000c1434d8d7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001639a390f0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0024cdab0c1e</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002c21358ce6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>005b95c28852</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d49b094-9da1-4d5b-9dcb-246fe33fc466')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d49b094-9da1-4d5b-9dcb-246fe33fc466 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d49b094-9da1-4d5b-9dcb-246fe33fc466');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_address = f'{root_address}/train.csv'\n",
        "data = pd.read_csv(data_address)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gzhdofz16ZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "f8e0bffc-1b82-4af5-b094-e94832e607c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='diagnosis', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.countplot(\n",
        "    data=data,\n",
        "    x='diagnosis'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFaIV8ny18Fx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "eaa06fe3-6b8c-4d85-d825-b714a315ca30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='diagnosis', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "data2 = data.copy()\n",
        "\n",
        "# select and merge every category other than `No_DR`\n",
        "data2['diagnosis'] = data2['diagnosis'] > 0\n",
        "data2['diagnosis'] = data2['diagnosis'].astype('int32')\n",
        "\n",
        "# plot the distribution of categories\n",
        "sns.countplot(data=data2, x='diagnosis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsE52cW1_-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6506c94-b062-4c62-c79e-602d93864e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2196, 2), (916, 2), (550, 2)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_frac, val_frac = 0.25, 0.15\n",
        "\n",
        "# separate `val` data from the `whole`\n",
        "_, val = train_test_split(\n",
        "    data2,\n",
        "    test_size=val_frac,\n",
        "    stratify=data2['diagnosis']\n",
        ")\n",
        "\n",
        "# separate `test` and `train` from the remaining\n",
        "train, test = train_test_split(\n",
        "    _,\n",
        "    test_size=test_frac/(1-val_frac), # to have the same frac on the `whole`\n",
        "    stratify=_['diagnosis']\n",
        ")\n",
        "\n",
        "# verify\n",
        "print(f'{train.shape}, {test.shape}, {val.shape}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bj41lhW8pqJ",
        "outputId": "b39f6cde-0174-465a-974c-af046fb242ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2978\n",
            "Testing set size: 745\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the path to the main folder containing the subfolders\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# List all the subfolders within the main folder\n",
        "subfolders = [folder for folder in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, folder))]\n",
        "\n",
        "# Initialize lists to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each subfolder and collect image paths and labels\n",
        "for label, folder in enumerate(subfolders):\n",
        "    folder_path = os.path.join(main_folder_path, folder)\n",
        "    file_paths = glob.glob(os.path.join(folder_path, \"*.png\"))  # Assuming the images are in JPG format\n",
        "\n",
        "    image_paths.extend(file_paths)\n",
        "    labels.extend([label] * len(file_paths))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(\"Training set size:\", len(train_image_paths))\n",
        "print(\"Testing set size:\", len(test_image_paths))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the path to the main folder containing the subfolders\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# List all the subfolders within the main folder\n",
        "subfolders = [folder for folder in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, folder))]\n",
        "\n",
        "# Initialize lists to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each subfolder and collect image paths and labels\n",
        "for label, folder in enumerate(subfolders):\n",
        "    folder_path = os.path.join(main_folder_path, folder)\n",
        "    file_paths = glob.glob(os.path.join(folder_path, \"*.png\"))  # Assuming the images are in PNG format\n",
        "\n",
        "    image_paths.extend(file_paths)\n",
        "    labels.extend([label] * len(file_paths))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(\"Training set size:\", len(train_image_paths))\n",
        "print(\"Testing set size:\", len(test_image_paths))\n",
        "\n",
        "# Calculate the average pixel values for each image\n",
        "train_heatmap_data = []\n",
        "test_heatmap_data = []\n",
        "\n",
        "# Iterate over training image paths and calculate average pixel values\n",
        "for image_path in train_image_paths:\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    average_pixel_value = np.mean(image)\n",
        "    train_heatmap_data.append(average_pixel_value)\n",
        "\n",
        "# Iterate over testing image paths and calculate average pixel values\n",
        "for image_path in test_image_paths:\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    average_pixel_value = np.mean(image)\n",
        "    test_heatmap_data.append(average_pixel_value)\n",
        "\n",
        "# Create a heatmap for the training data\n",
        "train_heatmap = np.reshape(train_heatmap_data, (-1, 1))\n",
        "sns.heatmap(train_heatmap, cmap='YlGnBu')\n",
        "plt.title('Training Data Heatmap')\n",
        "plt.xlabel('Image')\n",
        "plt.ylabel('Average Pixel Value')\n",
        "plt.show()\n",
        "\n",
        "# Create a heatmap for the testing data\n",
        "test_heatmap = np.reshape(test_heatmap_data, (-1, 1))\n",
        "sns.heatmap(test_heatmap, cmap='YlGnBu')\n",
        "plt.title('Testing Data Heatmap')\n",
        "plt.xlabel('Image')\n",
        "plt.ylabel('Average Pixel Value')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "6ZfGIT01AxHZ",
        "outputId": "fe8a1285-ea16-4ab7-d0af-f3acfd309ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2978\n",
            "Testing set size: 745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the path to the main folder containing the subfolders\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# List all the subfolders within the main folder\n",
        "subfolders = [folder for folder in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, folder))]\n",
        "\n",
        "# Initialize lists to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each subfolder and collect image paths and labels\n",
        "for label, folder in enumerate(subfolders):\n",
        "    folder_path = os.path.join(main_folder_path, folder)\n",
        "    file_paths = glob.glob(os.path.join(folder_path, \"*.png\"))  # Assuming the images are in PNG format\n",
        "\n",
        "    image_paths.extend(file_paths)\n",
        "    labels.extend([label] * len(file_paths))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print(\"Training set size:\", len(train_image_paths))\n",
        "print(\"Testing set size:\", len(test_image_paths))\n",
        "\n",
        "# Apply CLAHE to the images\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "# Iterate over training image paths and apply CLAHE\n",
        "for i, image_path in enumerate(train_image_paths):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    image = clahe.apply(image)\n",
        "    train_image_paths[i] = image\n",
        "\n",
        "# Iterate over testing image paths and apply CLAHE\n",
        "for i, image_path in enumerate(test_image_paths):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    image = clahe.apply(image)\n",
        "    test_image_paths[i] = image\n"
      ],
      "metadata": {
        "id": "L8OxOwX66LYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxIYigkqAlps"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import *\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# def build_generator(input_shape, num_classes):\n",
        "#     noise = Input(shape=input_shape)\n",
        "#     label = Input(shape=(1,), dtype=tf.int32)\n",
        "#     label_embedding = Embedding(num_classes, 50)(label)\n",
        "#     label_embedding = Flatten()(label_embedding)\n",
        "#     merged_input = Concatenate()([noise, label_embedding])\n",
        "\n",
        "#     x = Dense(7 * 7 * 128, activation=\"relu\")(merged_input)\n",
        "#     x = Reshape((7, 7, 128))(x)\n",
        "#     x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "#     x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "#     x = Conv2DTranspose(1, (7, 7), padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "#     model = Model(inputs=[noise, label], outputs=x)\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v8w5imFk5FH",
        "outputId": "3f19b6b9-0b03-4482-bc07-0af65cb06386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 313/937, Discriminator Loss: 0.7011221945285797, Generator Loss: 0.7119876742362976\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 314/937, Discriminator Loss: 0.6858304142951965, Generator Loss: 0.7214379906654358\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 315/937, Discriminator Loss: 0.6683746576309204, Generator Loss: 0.7223520278930664\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 316/937, Discriminator Loss: 0.6938644051551819, Generator Loss: 0.7410084009170532\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 317/937, Discriminator Loss: 0.6790716052055359, Generator Loss: 0.7204815745353699\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 318/937, Discriminator Loss: 0.6852698922157288, Generator Loss: 0.7091518044471741\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 319/937, Discriminator Loss: 0.6836069524288177, Generator Loss: 0.6947343349456787\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 320/937, Discriminator Loss: 0.6610363721847534, Generator Loss: 0.7297067642211914\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 321/937, Discriminator Loss: 0.6757398247718811, Generator Loss: 0.7058969140052795\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 322/937, Discriminator Loss: 0.6694666147232056, Generator Loss: 0.7255174517631531\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 323/937, Discriminator Loss: 0.6910434365272522, Generator Loss: 0.7115761041641235\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 324/937, Discriminator Loss: 0.6998320519924164, Generator Loss: 0.705274760723114\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 325/937, Discriminator Loss: 0.6833505034446716, Generator Loss: 0.7494891881942749\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 326/937, Discriminator Loss: 0.6769863069057465, Generator Loss: 0.7453250885009766\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 327/937, Discriminator Loss: 0.6695194840431213, Generator Loss: 0.7327247858047485\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 328/937, Discriminator Loss: 0.6944983601570129, Generator Loss: 0.738384485244751\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 329/937, Discriminator Loss: 0.6960360407829285, Generator Loss: 0.7570105791091919\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 330/937, Discriminator Loss: 0.701082319021225, Generator Loss: 0.7340638041496277\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 331/937, Discriminator Loss: 0.6832749545574188, Generator Loss: 0.7206417322158813\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 332/937, Discriminator Loss: 0.6904930174350739, Generator Loss: 0.7313133478164673\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 333/937, Discriminator Loss: 0.6960878968238831, Generator Loss: 0.7463034987449646\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 334/937, Discriminator Loss: 0.6930739283561707, Generator Loss: 0.7295366525650024\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 335/937, Discriminator Loss: 0.6855539679527283, Generator Loss: 0.7510746717453003\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 336/937, Discriminator Loss: 0.6990087926387787, Generator Loss: 0.7434005737304688\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 337/937, Discriminator Loss: 0.6896640658378601, Generator Loss: 0.7232149243354797\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 338/937, Discriminator Loss: 0.6844758093357086, Generator Loss: 0.7077271938323975\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 339/937, Discriminator Loss: 0.6720728576183319, Generator Loss: 0.756683349609375\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 340/937, Discriminator Loss: 0.6844920217990875, Generator Loss: 0.7436498403549194\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 341/937, Discriminator Loss: 0.679645299911499, Generator Loss: 0.7524101734161377\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 342/937, Discriminator Loss: 0.6899501383304596, Generator Loss: 0.7764469385147095\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 343/937, Discriminator Loss: 0.7260771691799164, Generator Loss: 0.7033713459968567\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 344/937, Discriminator Loss: 0.6938780844211578, Generator Loss: 0.7437955141067505\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 345/937, Discriminator Loss: 0.6851684153079987, Generator Loss: 0.7064864635467529\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 346/937, Discriminator Loss: 0.7033102214336395, Generator Loss: 0.7064625024795532\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 347/937, Discriminator Loss: 0.694429337978363, Generator Loss: 0.7045104503631592\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 348/937, Discriminator Loss: 0.6839858293533325, Generator Loss: 0.7239804267883301\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 349/937, Discriminator Loss: 0.7224130928516388, Generator Loss: 0.7075604796409607\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 350/937, Discriminator Loss: 0.693292111158371, Generator Loss: 0.7140638828277588\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 351/937, Discriminator Loss: 0.6868347525596619, Generator Loss: 0.7232940196990967\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 352/937, Discriminator Loss: 0.6820644736289978, Generator Loss: 0.7145189046859741\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 353/937, Discriminator Loss: 0.6711186468601227, Generator Loss: 0.7124859094619751\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 354/937, Discriminator Loss: 0.6630529165267944, Generator Loss: 0.728998064994812\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 355/937, Discriminator Loss: 0.6902289688587189, Generator Loss: 0.7170169353485107\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 356/937, Discriminator Loss: 0.6863110363483429, Generator Loss: 0.732008695602417\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 357/937, Discriminator Loss: 0.6955731511116028, Generator Loss: 0.7433140277862549\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 358/937, Discriminator Loss: 0.6798727810382843, Generator Loss: 0.7407299280166626\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 359/937, Discriminator Loss: 0.6896929144859314, Generator Loss: 0.739374577999115\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 8/10, Batch: 360/937, Discriminator Loss: 0.6792634725570679, Generator Loss: 0.6946924924850464\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 361/937, Discriminator Loss: 0.678634762763977, Generator Loss: 0.7468725442886353\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 362/937, Discriminator Loss: 0.6790472269058228, Generator Loss: 0.738993227481842\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 363/937, Discriminator Loss: 0.6997340023517609, Generator Loss: 0.7136185169219971\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 364/937, Discriminator Loss: 0.7047991752624512, Generator Loss: 0.7120803594589233\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 365/937, Discriminator Loss: 0.7082764804363251, Generator Loss: 0.7127323150634766\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 366/937, Discriminator Loss: 0.6813825964927673, Generator Loss: 0.7067116498947144\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 367/937, Discriminator Loss: 0.6736316978931427, Generator Loss: 0.7120521068572998\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 368/937, Discriminator Loss: 0.6905077397823334, Generator Loss: 0.7095422744750977\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 369/937, Discriminator Loss: 0.701200008392334, Generator Loss: 0.6987570524215698\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 370/937, Discriminator Loss: 0.6841731369495392, Generator Loss: 0.696560800075531\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 371/937, Discriminator Loss: 0.6983833014965057, Generator Loss: 0.7013065814971924\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 372/937, Discriminator Loss: 0.6790472269058228, Generator Loss: 0.70539790391922\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 373/937, Discriminator Loss: 0.6912524700164795, Generator Loss: 0.7262147068977356\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 374/937, Discriminator Loss: 0.6984801292419434, Generator Loss: 0.712115466594696\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 375/937, Discriminator Loss: 0.6843534708023071, Generator Loss: 0.7248908877372742\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 376/937, Discriminator Loss: 0.6890543699264526, Generator Loss: 0.7144566178321838\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "Epoch: 8/10, Batch: 377/937, Discriminator Loss: 0.696126788854599, Generator Loss: 0.7338897585868835\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 378/937, Discriminator Loss: 0.6935760378837585, Generator Loss: 0.7017995119094849\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 379/937, Discriminator Loss: 0.6824138760566711, Generator Loss: 0.7270057201385498\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 8/10, Batch: 380/937, Discriminator Loss: 0.6864533424377441, Generator Loss: 0.7146034836769104\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 381/937, Discriminator Loss: 0.6950746178627014, Generator Loss: 0.7245013117790222\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 382/937, Discriminator Loss: 0.6981561481952667, Generator Loss: 0.7320942878723145\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 383/937, Discriminator Loss: 0.6872841715812683, Generator Loss: 0.7228764891624451\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 384/937, Discriminator Loss: 0.6770489811897278, Generator Loss: 0.74461829662323\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 385/937, Discriminator Loss: 0.6815157830715179, Generator Loss: 0.7287430763244629\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 386/937, Discriminator Loss: 0.6706578433513641, Generator Loss: 0.7080739140510559\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 387/937, Discriminator Loss: 0.6916806697845459, Generator Loss: 0.740676760673523\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 388/937, Discriminator Loss: 0.6984339654445648, Generator Loss: 0.7462900876998901\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 389/937, Discriminator Loss: 0.6844983398914337, Generator Loss: 0.7322685718536377\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 390/937, Discriminator Loss: 0.672242134809494, Generator Loss: 0.720048725605011\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 391/937, Discriminator Loss: 0.6833478808403015, Generator Loss: 0.7272286415100098\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 392/937, Discriminator Loss: 0.6946290731430054, Generator Loss: 0.7144246101379395\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 393/937, Discriminator Loss: 0.6842221617698669, Generator Loss: 0.7083144187927246\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 394/937, Discriminator Loss: 0.7162334620952606, Generator Loss: 0.7071388363838196\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 395/937, Discriminator Loss: 0.6876102089881897, Generator Loss: 0.7148409485816956\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 396/937, Discriminator Loss: 0.6833171248435974, Generator Loss: 0.7217246890068054\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Epoch: 8/10, Batch: 397/937, Discriminator Loss: 0.6817377209663391, Generator Loss: 0.7389553189277649\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 398/937, Discriminator Loss: 0.7057663202285767, Generator Loss: 0.7236109375953674\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 399/937, Discriminator Loss: 0.6844610869884491, Generator Loss: 0.7329409122467041\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 400/937, Discriminator Loss: 0.6920502781867981, Generator Loss: 0.7253203392028809\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 401/937, Discriminator Loss: 0.6942009925842285, Generator Loss: 0.7287726402282715\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 402/937, Discriminator Loss: 0.6758947968482971, Generator Loss: 0.741291344165802\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 403/937, Discriminator Loss: 0.7007561326026917, Generator Loss: 0.7315717935562134\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 404/937, Discriminator Loss: 0.7043946385383606, Generator Loss: 0.746630847454071\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 405/937, Discriminator Loss: 0.6854728758335114, Generator Loss: 0.7178755402565002\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 406/937, Discriminator Loss: 0.6911352574825287, Generator Loss: 0.7212241888046265\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 407/937, Discriminator Loss: 0.6983096599578857, Generator Loss: 0.7184340953826904\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 408/937, Discriminator Loss: 0.680365800857544, Generator Loss: 0.7264939546585083\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 409/937, Discriminator Loss: 0.6760917007923126, Generator Loss: 0.730105459690094\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 410/937, Discriminator Loss: 0.6810599565505981, Generator Loss: 0.715678334236145\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 411/937, Discriminator Loss: 0.675444096326828, Generator Loss: 0.7280596494674683\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 412/937, Discriminator Loss: 0.6730232834815979, Generator Loss: 0.7119389176368713\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 413/937, Discriminator Loss: 0.6751660704612732, Generator Loss: 0.7077823877334595\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 414/937, Discriminator Loss: 0.6821964681148529, Generator Loss: 0.7089062929153442\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 415/937, Discriminator Loss: 0.6832723021507263, Generator Loss: 0.7081449031829834\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 416/937, Discriminator Loss: 0.683030754327774, Generator Loss: 0.702623724937439\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 417/937, Discriminator Loss: 0.6698926687240601, Generator Loss: 0.725459635257721\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 418/937, Discriminator Loss: 0.6866478025913239, Generator Loss: 0.7204649448394775\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 419/937, Discriminator Loss: 0.6668796539306641, Generator Loss: 0.7347695827484131\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 420/937, Discriminator Loss: 0.6833170056343079, Generator Loss: 0.718808114528656\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 421/937, Discriminator Loss: 0.6741082072257996, Generator Loss: 0.7281030416488647\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 422/937, Discriminator Loss: 0.6999046802520752, Generator Loss: 0.7504944801330566\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 423/937, Discriminator Loss: 0.6849475502967834, Generator Loss: 0.7402662038803101\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 424/937, Discriminator Loss: 0.6841728389263153, Generator Loss: 0.7288060784339905\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 425/937, Discriminator Loss: 0.690369039773941, Generator Loss: 0.742805540561676\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 426/937, Discriminator Loss: 0.6763022840023041, Generator Loss: 0.718508243560791\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 427/937, Discriminator Loss: 0.6651702523231506, Generator Loss: 0.7493355870246887\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 428/937, Discriminator Loss: 0.71483513712883, Generator Loss: 0.7308454513549805\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 429/937, Discriminator Loss: 0.6908020973205566, Generator Loss: 0.7270952463150024\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 430/937, Discriminator Loss: 0.680524080991745, Generator Loss: 0.7249587774276733\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 431/937, Discriminator Loss: 0.6736079752445221, Generator Loss: 0.7274030447006226\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 432/937, Discriminator Loss: 0.696930468082428, Generator Loss: 0.7437795996665955\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 433/937, Discriminator Loss: 0.6683367788791656, Generator Loss: 0.7271194458007812\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 434/937, Discriminator Loss: 0.6853586733341217, Generator Loss: 0.7186068892478943\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 435/937, Discriminator Loss: 0.7001748979091644, Generator Loss: 0.7221026420593262\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 436/937, Discriminator Loss: 0.6775223314762115, Generator Loss: 0.7583713531494141\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 437/937, Discriminator Loss: 0.6840405464172363, Generator Loss: 0.7571941614151001\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 438/937, Discriminator Loss: 0.6808371245861053, Generator Loss: 0.7264500856399536\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 439/937, Discriminator Loss: 0.6874299943447113, Generator Loss: 0.7705285549163818\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 440/937, Discriminator Loss: 0.6963783800601959, Generator Loss: 0.7430801391601562\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 441/937, Discriminator Loss: 0.7051590085029602, Generator Loss: 0.7088783979415894\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 442/937, Discriminator Loss: 0.699441522359848, Generator Loss: 0.7365405559539795\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 443/937, Discriminator Loss: 0.6852931380271912, Generator Loss: 0.7214049696922302\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 444/937, Discriminator Loss: 0.690335750579834, Generator Loss: 0.7170767784118652\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 445/937, Discriminator Loss: 0.6838297843933105, Generator Loss: 0.7093536853790283\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 446/937, Discriminator Loss: 0.6862321496009827, Generator Loss: 0.7040020823478699\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 447/937, Discriminator Loss: 0.6901282966136932, Generator Loss: 0.7088803052902222\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 448/937, Discriminator Loss: 0.6827661693096161, Generator Loss: 0.7226930260658264\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 449/937, Discriminator Loss: 0.698388546705246, Generator Loss: 0.7067501544952393\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 450/937, Discriminator Loss: 0.6794089376926422, Generator Loss: 0.7219760417938232\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 451/937, Discriminator Loss: 0.7179962694644928, Generator Loss: 0.7108718752861023\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 452/937, Discriminator Loss: 0.6799673140048981, Generator Loss: 0.7167741060256958\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 8/10, Batch: 453/937, Discriminator Loss: 0.7001710534095764, Generator Loss: 0.7298014760017395\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 454/937, Discriminator Loss: 0.7005668580532074, Generator Loss: 0.7276333570480347\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 455/937, Discriminator Loss: 0.6756913661956787, Generator Loss: 0.7168021202087402\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 456/937, Discriminator Loss: 0.6931407451629639, Generator Loss: 0.7069973349571228\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 457/937, Discriminator Loss: 0.6810945272445679, Generator Loss: 0.6904947757720947\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 458/937, Discriminator Loss: 0.695844292640686, Generator Loss: 0.7122403979301453\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 459/937, Discriminator Loss: 0.691918134689331, Generator Loss: 0.6846758127212524\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 460/937, Discriminator Loss: 0.7054028809070587, Generator Loss: 0.7204548120498657\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 461/937, Discriminator Loss: 0.67568039894104, Generator Loss: 0.7080258131027222\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 462/937, Discriminator Loss: 0.6894287467002869, Generator Loss: 0.7177528738975525\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 463/937, Discriminator Loss: 0.6798180639743805, Generator Loss: 0.7298787832260132\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 464/937, Discriminator Loss: 0.6895470321178436, Generator Loss: 0.7233353853225708\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 465/937, Discriminator Loss: 0.6940792202949524, Generator Loss: 0.716668426990509\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 466/937, Discriminator Loss: 0.6941021978855133, Generator Loss: 0.7079719305038452\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 467/937, Discriminator Loss: 0.6821049153804779, Generator Loss: 0.7311557531356812\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 468/937, Discriminator Loss: 0.6856046617031097, Generator Loss: 0.7056838870048523\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 469/937, Discriminator Loss: 0.6842360496520996, Generator Loss: 0.7017886638641357\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 470/937, Discriminator Loss: 0.6653468012809753, Generator Loss: 0.7004173994064331\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 471/937, Discriminator Loss: 0.6849099397659302, Generator Loss: 0.7124223113059998\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 472/937, Discriminator Loss: 0.6848487555980682, Generator Loss: 0.7192889451980591\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 473/937, Discriminator Loss: 0.6863519251346588, Generator Loss: 0.7223410606384277\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 474/937, Discriminator Loss: 0.6978186666965485, Generator Loss: 0.7048771977424622\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 475/937, Discriminator Loss: 0.6914922893047333, Generator Loss: 0.733859121799469\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 476/937, Discriminator Loss: 0.6952718496322632, Generator Loss: 0.7139906883239746\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 477/937, Discriminator Loss: 0.6998284757137299, Generator Loss: 0.7089270949363708\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 478/937, Discriminator Loss: 0.6800577342510223, Generator Loss: 0.7209737300872803\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 479/937, Discriminator Loss: 0.7164530158042908, Generator Loss: 0.7123332619667053\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 480/937, Discriminator Loss: 0.6913810074329376, Generator Loss: 0.7159653902053833\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 481/937, Discriminator Loss: 0.6836599707603455, Generator Loss: 0.724132776260376\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 482/937, Discriminator Loss: 0.6860256195068359, Generator Loss: 0.7248201370239258\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 483/937, Discriminator Loss: 0.674787163734436, Generator Loss: 0.7428100109100342\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 484/937, Discriminator Loss: 0.6913066208362579, Generator Loss: 0.7255241870880127\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 485/937, Discriminator Loss: 0.6868422031402588, Generator Loss: 0.7107160091400146\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 486/937, Discriminator Loss: 0.6759206652641296, Generator Loss: 0.7590413093566895\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 487/937, Discriminator Loss: 0.6937938630580902, Generator Loss: 0.7648731470108032\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 488/937, Discriminator Loss: 0.6816615164279938, Generator Loss: 0.7316620349884033\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 489/937, Discriminator Loss: 0.7034728825092316, Generator Loss: 0.7132299542427063\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 490/937, Discriminator Loss: 0.678706705570221, Generator Loss: 0.728807806968689\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 491/937, Discriminator Loss: 0.705645352602005, Generator Loss: 0.7244771122932434\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 492/937, Discriminator Loss: 0.6866081058979034, Generator Loss: 0.7726714611053467\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 493/937, Discriminator Loss: 0.6881581544876099, Generator Loss: 0.7612547278404236\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 494/937, Discriminator Loss: 0.7092211544513702, Generator Loss: 0.7333229184150696\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 495/937, Discriminator Loss: 0.6934031844139099, Generator Loss: 0.7166833281517029\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 496/937, Discriminator Loss: 0.689061850309372, Generator Loss: 0.7280135154724121\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 497/937, Discriminator Loss: 0.6741741597652435, Generator Loss: 0.7191134095191956\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 498/937, Discriminator Loss: 0.6843697130680084, Generator Loss: 0.7195076942443848\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 499/937, Discriminator Loss: 0.686022162437439, Generator Loss: 0.6988425254821777\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 500/937, Discriminator Loss: 0.6917905807495117, Generator Loss: 0.6993732452392578\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 501/937, Discriminator Loss: 0.6710790395736694, Generator Loss: 0.7067063450813293\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 502/937, Discriminator Loss: 0.7059272527694702, Generator Loss: 0.7003519535064697\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 503/937, Discriminator Loss: 0.7087080776691437, Generator Loss: 0.7139655351638794\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 504/937, Discriminator Loss: 0.6871216297149658, Generator Loss: 0.7291082143783569\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 505/937, Discriminator Loss: 0.6874542236328125, Generator Loss: 0.7187520265579224\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 506/937, Discriminator Loss: 0.6832283735275269, Generator Loss: 0.7267181873321533\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 507/937, Discriminator Loss: 0.6874251663684845, Generator Loss: 0.7211198210716248\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 508/937, Discriminator Loss: 0.6892488598823547, Generator Loss: 0.7266120314598083\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 509/937, Discriminator Loss: 0.6844546496868134, Generator Loss: 0.7263895869255066\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Epoch: 8/10, Batch: 510/937, Discriminator Loss: 0.6998919248580933, Generator Loss: 0.7168771028518677\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 511/937, Discriminator Loss: 0.6765175461769104, Generator Loss: 0.7337067723274231\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 512/937, Discriminator Loss: 0.7101019620895386, Generator Loss: 0.7336663603782654\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 513/937, Discriminator Loss: 0.6846341490745544, Generator Loss: 0.7212725877761841\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 514/937, Discriminator Loss: 0.6931097209453583, Generator Loss: 0.7274863123893738\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 8/10, Batch: 515/937, Discriminator Loss: 0.6991301774978638, Generator Loss: 0.7191659212112427\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 516/937, Discriminator Loss: 0.6922585666179657, Generator Loss: 0.723560094833374\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 517/937, Discriminator Loss: 0.6903598010540009, Generator Loss: 0.7163138389587402\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Epoch: 8/10, Batch: 518/937, Discriminator Loss: 0.6969058513641357, Generator Loss: 0.7318484783172607\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 519/937, Discriminator Loss: 0.6768832802772522, Generator Loss: 0.7345960736274719\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 520/937, Discriminator Loss: 0.6691927313804626, Generator Loss: 0.7295215129852295\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 521/937, Discriminator Loss: 0.7011127471923828, Generator Loss: 0.7562190890312195\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 522/937, Discriminator Loss: 0.7023825943470001, Generator Loss: 0.7383472919464111\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 523/937, Discriminator Loss: 0.6753422021865845, Generator Loss: 0.7321503758430481\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 524/937, Discriminator Loss: 0.675212174654007, Generator Loss: 0.7369024753570557\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 525/937, Discriminator Loss: 0.6947112381458282, Generator Loss: 0.7312459945678711\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 526/937, Discriminator Loss: 0.6932710707187653, Generator Loss: 0.7486513257026672\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 527/937, Discriminator Loss: 0.6881826221942902, Generator Loss: 0.7646825313568115\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 528/937, Discriminator Loss: 0.6859088838100433, Generator Loss: 0.762991189956665\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 529/937, Discriminator Loss: 0.7242408096790314, Generator Loss: 0.7376513481140137\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 530/937, Discriminator Loss: 0.6757445633411407, Generator Loss: 0.7405030727386475\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 531/937, Discriminator Loss: 0.6977525353431702, Generator Loss: 0.7548384070396423\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 532/937, Discriminator Loss: 0.6939484775066376, Generator Loss: 0.7756315469741821\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 533/937, Discriminator Loss: 0.6995834112167358, Generator Loss: 0.7630069851875305\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 534/937, Discriminator Loss: 0.6980265080928802, Generator Loss: 0.7508683204650879\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 535/937, Discriminator Loss: 0.6936314702033997, Generator Loss: 0.751118004322052\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 536/937, Discriminator Loss: 0.687063992023468, Generator Loss: 0.7303210496902466\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 537/937, Discriminator Loss: 0.6798251569271088, Generator Loss: 0.7414066791534424\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 538/937, Discriminator Loss: 0.6815233826637268, Generator Loss: 0.7702325582504272\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 539/937, Discriminator Loss: 0.6858200132846832, Generator Loss: 0.7423455715179443\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 540/937, Discriminator Loss: 0.6783288419246674, Generator Loss: 0.7536716461181641\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 541/937, Discriminator Loss: 0.6801140010356903, Generator Loss: 0.7425872087478638\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 542/937, Discriminator Loss: 0.6959595680236816, Generator Loss: 0.7675238847732544\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 543/937, Discriminator Loss: 0.6975442171096802, Generator Loss: 0.762469470500946\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 544/937, Discriminator Loss: 0.6840463876724243, Generator Loss: 0.7472675442695618\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 545/937, Discriminator Loss: 0.6718453466892242, Generator Loss: 0.7377562522888184\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 546/937, Discriminator Loss: 0.6959556341171265, Generator Loss: 0.7060197591781616\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 547/937, Discriminator Loss: 0.6995091438293457, Generator Loss: 0.7377172112464905\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 548/937, Discriminator Loss: 0.6817981600761414, Generator Loss: 0.7377909421920776\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 549/937, Discriminator Loss: 0.6904920935630798, Generator Loss: 0.7735505104064941\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 550/937, Discriminator Loss: 0.6831140518188477, Generator Loss: 0.7553200721740723\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 551/937, Discriminator Loss: 0.6873063445091248, Generator Loss: 0.720741868019104\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 552/937, Discriminator Loss: 0.6907325983047485, Generator Loss: 0.7239130735397339\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 553/937, Discriminator Loss: 0.6793153584003448, Generator Loss: 0.7295313477516174\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 554/937, Discriminator Loss: 0.7037193179130554, Generator Loss: 0.7086687088012695\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 555/937, Discriminator Loss: 0.6944355666637421, Generator Loss: 0.7169868350028992\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 556/937, Discriminator Loss: 0.6841177344322205, Generator Loss: 0.7218750715255737\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 557/937, Discriminator Loss: 0.6741701066493988, Generator Loss: 0.7096004486083984\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 558/937, Discriminator Loss: 0.6831087470054626, Generator Loss: 0.7269439697265625\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 559/937, Discriminator Loss: 0.6926319003105164, Generator Loss: 0.7075939178466797\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 560/937, Discriminator Loss: 0.6916443407535553, Generator Loss: 0.7074279189109802\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 561/937, Discriminator Loss: 0.6726004183292389, Generator Loss: 0.7151881456375122\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 562/937, Discriminator Loss: 0.6702162623405457, Generator Loss: 0.7002676725387573\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 563/937, Discriminator Loss: 0.6911830902099609, Generator Loss: 0.7138491272926331\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 564/937, Discriminator Loss: 0.6911411285400391, Generator Loss: 0.7395399808883667\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 565/937, Discriminator Loss: 0.6791945099830627, Generator Loss: 0.7076156139373779\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 566/937, Discriminator Loss: 0.6814673244953156, Generator Loss: 0.7115415334701538\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 567/937, Discriminator Loss: 0.6678834855556488, Generator Loss: 0.6929410696029663\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 568/937, Discriminator Loss: 0.717750072479248, Generator Loss: 0.7169911861419678\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 569/937, Discriminator Loss: 0.6629097759723663, Generator Loss: 0.7253588438034058\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 570/937, Discriminator Loss: 0.6677677929401398, Generator Loss: 0.7350656986236572\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 571/937, Discriminator Loss: 0.6832956969738007, Generator Loss: 0.7653588056564331\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Epoch: 8/10, Batch: 572/937, Discriminator Loss: 0.6880881190299988, Generator Loss: 0.7287086844444275\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 573/937, Discriminator Loss: 0.6861567795276642, Generator Loss: 0.6953097581863403\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 574/937, Discriminator Loss: 0.6889709532260895, Generator Loss: 0.6909710764884949\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 575/937, Discriminator Loss: 0.6832748353481293, Generator Loss: 0.7222368717193604\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 576/937, Discriminator Loss: 0.6818217635154724, Generator Loss: 0.7160351872444153\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 577/937, Discriminator Loss: 0.7007848620414734, Generator Loss: 0.6920630931854248\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 578/937, Discriminator Loss: 0.6899963319301605, Generator Loss: 0.6954894065856934\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 579/937, Discriminator Loss: 0.6873602867126465, Generator Loss: 0.6914479732513428\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 580/937, Discriminator Loss: 0.6919451951980591, Generator Loss: 0.7010612487792969\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 581/937, Discriminator Loss: 0.6757367253303528, Generator Loss: 0.7236145734786987\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 582/937, Discriminator Loss: 0.7007483839988708, Generator Loss: 0.7133809328079224\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 583/937, Discriminator Loss: 0.6972916722297668, Generator Loss: 0.7346441149711609\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 584/937, Discriminator Loss: 0.6886970102787018, Generator Loss: 0.7306919693946838\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 585/937, Discriminator Loss: 0.6863790154457092, Generator Loss: 0.7258366346359253\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 586/937, Discriminator Loss: 0.6921645402908325, Generator Loss: 0.7371843457221985\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 587/937, Discriminator Loss: 0.6912532150745392, Generator Loss: 0.728013277053833\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 588/937, Discriminator Loss: 0.6850108504295349, Generator Loss: 0.7683870792388916\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 589/937, Discriminator Loss: 0.6936858296394348, Generator Loss: 0.8333890438079834\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 590/937, Discriminator Loss: 0.6976931691169739, Generator Loss: 0.7831995487213135\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 591/937, Discriminator Loss: 0.6706658601760864, Generator Loss: 0.7408002614974976\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 592/937, Discriminator Loss: 0.6926646828651428, Generator Loss: 0.7195323705673218\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 593/937, Discriminator Loss: 0.6900060772895813, Generator Loss: 0.7148813605308533\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 594/937, Discriminator Loss: 0.6913102865219116, Generator Loss: 0.7104872465133667\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 595/937, Discriminator Loss: 0.6871812343597412, Generator Loss: 0.7192885279655457\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 596/937, Discriminator Loss: 0.6903043687343597, Generator Loss: 0.7118657827377319\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 597/937, Discriminator Loss: 0.6851101815700531, Generator Loss: 0.7086880803108215\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 598/937, Discriminator Loss: 0.6639243364334106, Generator Loss: 0.7009950280189514\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 599/937, Discriminator Loss: 0.685320645570755, Generator Loss: 0.7260242104530334\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 600/937, Discriminator Loss: 0.6896508634090424, Generator Loss: 0.7222887277603149\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 601/937, Discriminator Loss: 0.6638133823871613, Generator Loss: 0.727310061454773\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 602/937, Discriminator Loss: 0.700984388589859, Generator Loss: 0.7038240432739258\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 603/937, Discriminator Loss: 0.7010201811790466, Generator Loss: 0.7149409651756287\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 604/937, Discriminator Loss: 0.6984001994132996, Generator Loss: 0.7174946069717407\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 605/937, Discriminator Loss: 0.6627901196479797, Generator Loss: 0.7206329107284546\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 606/937, Discriminator Loss: 0.7044155597686768, Generator Loss: 0.7170653343200684\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 607/937, Discriminator Loss: 0.6630795001983643, Generator Loss: 0.7112362384796143\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 608/937, Discriminator Loss: 0.6860187351703644, Generator Loss: 0.7101571559906006\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 609/937, Discriminator Loss: 0.6691550016403198, Generator Loss: 0.7225388884544373\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 610/937, Discriminator Loss: 0.6843646466732025, Generator Loss: 0.7132555246353149\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 611/937, Discriminator Loss: 0.6892972588539124, Generator Loss: 0.7252947092056274\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 612/937, Discriminator Loss: 0.6866394579410553, Generator Loss: 0.7095372080802917\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 613/937, Discriminator Loss: 0.6830998659133911, Generator Loss: 0.7349480390548706\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 614/937, Discriminator Loss: 0.6987219452857971, Generator Loss: 0.7143185138702393\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 615/937, Discriminator Loss: 0.691180944442749, Generator Loss: 0.7230733036994934\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 616/937, Discriminator Loss: 0.6976024210453033, Generator Loss: 0.7271122932434082\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 617/937, Discriminator Loss: 0.6745732426643372, Generator Loss: 0.7242833971977234\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 618/937, Discriminator Loss: 0.6833643019199371, Generator Loss: 0.7200062274932861\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 619/937, Discriminator Loss: 0.702802449464798, Generator Loss: 0.6979446411132812\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 620/937, Discriminator Loss: 0.687565416097641, Generator Loss: 0.6985034346580505\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 621/937, Discriminator Loss: 0.6770411133766174, Generator Loss: 0.7055646181106567\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 622/937, Discriminator Loss: 0.691041499376297, Generator Loss: 0.6998572945594788\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 623/937, Discriminator Loss: 0.6800983548164368, Generator Loss: 0.7067570686340332\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 624/937, Discriminator Loss: 0.6811712384223938, Generator Loss: 0.7232983112335205\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 625/937, Discriminator Loss: 0.6671139299869537, Generator Loss: 0.7339400053024292\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 626/937, Discriminator Loss: 0.6744746565818787, Generator Loss: 0.734184980392456\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 627/937, Discriminator Loss: 0.6737076640129089, Generator Loss: 0.6939617991447449\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 628/937, Discriminator Loss: 0.6818989217281342, Generator Loss: 0.7026048302650452\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 629/937, Discriminator Loss: 0.6794096231460571, Generator Loss: 0.7115694880485535\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 630/937, Discriminator Loss: 0.6654257476329803, Generator Loss: 0.7063649892807007\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 631/937, Discriminator Loss: 0.6999897062778473, Generator Loss: 0.6999020576477051\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 632/937, Discriminator Loss: 0.6783204674720764, Generator Loss: 0.7339340448379517\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 633/937, Discriminator Loss: 0.6722545623779297, Generator Loss: 0.7360284924507141\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 634/937, Discriminator Loss: 0.7047010064125061, Generator Loss: 0.7416226863861084\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 635/937, Discriminator Loss: 0.6922662854194641, Generator Loss: 0.721664547920227\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 636/937, Discriminator Loss: 0.6810311079025269, Generator Loss: 0.71568763256073\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 637/937, Discriminator Loss: 0.694585382938385, Generator Loss: 0.708214521408081\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 638/937, Discriminator Loss: 0.670529693365097, Generator Loss: 0.7098208665847778\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 639/937, Discriminator Loss: 0.6663837730884552, Generator Loss: 0.7090139389038086\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 640/937, Discriminator Loss: 0.6833393573760986, Generator Loss: 0.7139999866485596\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 641/937, Discriminator Loss: 0.6927453577518463, Generator Loss: 0.7033770680427551\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 642/937, Discriminator Loss: 0.676480621099472, Generator Loss: 0.7154392004013062\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 643/937, Discriminator Loss: 0.6734059154987335, Generator Loss: 0.7140130996704102\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 644/937, Discriminator Loss: 0.6919875144958496, Generator Loss: 0.7055195569992065\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 645/937, Discriminator Loss: 0.6862809360027313, Generator Loss: 0.720268964767456\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 646/937, Discriminator Loss: 0.6836157441139221, Generator Loss: 0.7339475154876709\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 647/937, Discriminator Loss: 0.6633569896221161, Generator Loss: 0.7221797704696655\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 648/937, Discriminator Loss: 0.6924407482147217, Generator Loss: 0.727669358253479\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 649/937, Discriminator Loss: 0.675104558467865, Generator Loss: 0.723778247833252\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 650/937, Discriminator Loss: 0.7122105658054352, Generator Loss: 0.7251379489898682\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 651/937, Discriminator Loss: 0.7054732441902161, Generator Loss: 0.7146180868148804\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 652/937, Discriminator Loss: 0.68241748213768, Generator Loss: 0.723976731300354\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 653/937, Discriminator Loss: 0.680016964673996, Generator Loss: 0.7084395289421082\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 654/937, Discriminator Loss: 0.6798119843006134, Generator Loss: 0.7228114008903503\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 655/937, Discriminator Loss: 0.6752873063087463, Generator Loss: 0.7222007513046265\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 656/937, Discriminator Loss: 0.6914668679237366, Generator Loss: 0.721481442451477\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 657/937, Discriminator Loss: 0.6850501298904419, Generator Loss: 0.7239515781402588\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 658/937, Discriminator Loss: 0.6884337365627289, Generator Loss: 0.7385841012001038\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 659/937, Discriminator Loss: 0.6971511542797089, Generator Loss: 0.7150899767875671\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 660/937, Discriminator Loss: 0.681164562702179, Generator Loss: 0.7225807905197144\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 661/937, Discriminator Loss: 0.6890900135040283, Generator Loss: 0.716606616973877\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 662/937, Discriminator Loss: 0.6806595027446747, Generator Loss: 0.7277230620384216\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 663/937, Discriminator Loss: 0.7013717293739319, Generator Loss: 0.7196621894836426\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 664/937, Discriminator Loss: 0.6826388537883759, Generator Loss: 0.7216540575027466\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 665/937, Discriminator Loss: 0.6893939077854156, Generator Loss: 0.7239673733711243\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 666/937, Discriminator Loss: 0.7049087882041931, Generator Loss: 0.7648253440856934\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 667/937, Discriminator Loss: 0.6809942126274109, Generator Loss: 0.7246339321136475\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 668/937, Discriminator Loss: 0.6847347021102905, Generator Loss: 0.7157487869262695\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 669/937, Discriminator Loss: 0.68660968542099, Generator Loss: 0.7071877717971802\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 670/937, Discriminator Loss: 0.6742559969425201, Generator Loss: 0.728437066078186\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 671/937, Discriminator Loss: 0.6788695156574249, Generator Loss: 0.715375542640686\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 672/937, Discriminator Loss: 0.6892150044441223, Generator Loss: 0.7265384197235107\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 673/937, Discriminator Loss: 0.6770147085189819, Generator Loss: 0.7299757599830627\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 674/937, Discriminator Loss: 0.6952179372310638, Generator Loss: 0.7463452816009521\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 675/937, Discriminator Loss: 0.6869434714317322, Generator Loss: 0.7277902364730835\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 676/937, Discriminator Loss: 0.7144349813461304, Generator Loss: 0.7314786911010742\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 677/937, Discriminator Loss: 0.6897013187408447, Generator Loss: 0.7199295163154602\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 678/937, Discriminator Loss: 0.6913743019104004, Generator Loss: 0.7169008255004883\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 679/937, Discriminator Loss: 0.6789252758026123, Generator Loss: 0.7439665198326111\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 680/937, Discriminator Loss: 0.6960296034812927, Generator Loss: 0.7412334084510803\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 681/937, Discriminator Loss: 0.686842143535614, Generator Loss: 0.7344565391540527\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 682/937, Discriminator Loss: 0.6884320080280304, Generator Loss: 0.7195233702659607\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 683/937, Discriminator Loss: 0.6868391633033752, Generator Loss: 0.705400824546814\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 684/937, Discriminator Loss: 0.7014947533607483, Generator Loss: 0.7045644521713257\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 685/937, Discriminator Loss: 0.695862203836441, Generator Loss: 0.7149299383163452\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 686/937, Discriminator Loss: 0.67342808842659, Generator Loss: 0.704668402671814\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 687/937, Discriminator Loss: 0.695222944021225, Generator Loss: 0.703865110874176\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 688/937, Discriminator Loss: 0.6969444453716278, Generator Loss: 0.7163774967193604\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 689/937, Discriminator Loss: 0.6930384933948517, Generator Loss: 0.7137228846549988\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 690/937, Discriminator Loss: 0.6920526325702667, Generator Loss: 0.7343012690544128\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 691/937, Discriminator Loss: 0.6877560615539551, Generator Loss: 0.7146260738372803\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 692/937, Discriminator Loss: 0.7004171013832092, Generator Loss: 0.7097738981246948\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 693/937, Discriminator Loss: 0.6899476945400238, Generator Loss: 0.7072550058364868\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 694/937, Discriminator Loss: 0.6877481341362, Generator Loss: 0.7112666368484497\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 695/937, Discriminator Loss: 0.6993419826030731, Generator Loss: 0.7216718196868896\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 696/937, Discriminator Loss: 0.6918110847473145, Generator Loss: 0.7062498331069946\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 697/937, Discriminator Loss: 0.677719920873642, Generator Loss: 0.7186275720596313\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 698/937, Discriminator Loss: 0.6796605288982391, Generator Loss: 0.7115157842636108\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 699/937, Discriminator Loss: 0.7081142961978912, Generator Loss: 0.7074780464172363\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 700/937, Discriminator Loss: 0.6798088550567627, Generator Loss: 0.7137999534606934\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 701/937, Discriminator Loss: 0.6845465302467346, Generator Loss: 0.7273660898208618\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 702/937, Discriminator Loss: 0.700457751750946, Generator Loss: 0.7021881341934204\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 703/937, Discriminator Loss: 0.6910532116889954, Generator Loss: 0.69868004322052\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 704/937, Discriminator Loss: 0.6990117132663727, Generator Loss: 0.7027254104614258\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 705/937, Discriminator Loss: 0.6983268558979034, Generator Loss: 0.7134582996368408\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 706/937, Discriminator Loss: 0.688111275434494, Generator Loss: 0.7182275056838989\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 707/937, Discriminator Loss: 0.6820715665817261, Generator Loss: 0.7073672413825989\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 708/937, Discriminator Loss: 0.6944197118282318, Generator Loss: 0.7102940678596497\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 709/937, Discriminator Loss: 0.6877400279045105, Generator Loss: 0.7012090682983398\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 710/937, Discriminator Loss: 0.68522909283638, Generator Loss: 0.7053804397583008\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 711/937, Discriminator Loss: 0.6785487234592438, Generator Loss: 0.7067539691925049\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 712/937, Discriminator Loss: 0.6932545304298401, Generator Loss: 0.7125793695449829\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 713/937, Discriminator Loss: 0.6807491481304169, Generator Loss: 0.7026177644729614\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 714/937, Discriminator Loss: 0.692440539598465, Generator Loss: 0.7174485325813293\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 715/937, Discriminator Loss: 0.6939550638198853, Generator Loss: 0.7370388507843018\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 716/937, Discriminator Loss: 0.6697172820568085, Generator Loss: 0.7177119255065918\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 717/937, Discriminator Loss: 0.7008150219917297, Generator Loss: 0.7074410319328308\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 718/937, Discriminator Loss: 0.7041456997394562, Generator Loss: 0.7146150469779968\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 719/937, Discriminator Loss: 0.7006931304931641, Generator Loss: 0.7145089507102966\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 720/937, Discriminator Loss: 0.7070026397705078, Generator Loss: 0.7166818380355835\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 721/937, Discriminator Loss: 0.6991659998893738, Generator Loss: 0.7378346920013428\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 722/937, Discriminator Loss: 0.6954276859760284, Generator Loss: 0.7327332496643066\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 723/937, Discriminator Loss: 0.6882413923740387, Generator Loss: 0.733022153377533\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 724/937, Discriminator Loss: 0.6906495988368988, Generator Loss: 0.7473838329315186\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 8/10, Batch: 725/937, Discriminator Loss: 0.6910942792892456, Generator Loss: 0.7461807131767273\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 726/937, Discriminator Loss: 0.6881614625453949, Generator Loss: 0.7203887104988098\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 727/937, Discriminator Loss: 0.6910724937915802, Generator Loss: 0.7371249794960022\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 728/937, Discriminator Loss: 0.6895507574081421, Generator Loss: 0.740217924118042\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 729/937, Discriminator Loss: 0.6899130344390869, Generator Loss: 0.7510676383972168\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 730/937, Discriminator Loss: 0.6918072402477264, Generator Loss: 0.742057740688324\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 731/937, Discriminator Loss: 0.6878280639648438, Generator Loss: 0.740557849407196\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 732/937, Discriminator Loss: 0.6792533993721008, Generator Loss: 0.7299641370773315\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 733/937, Discriminator Loss: 0.6722684502601624, Generator Loss: 0.7499262690544128\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 734/937, Discriminator Loss: 0.6919103860855103, Generator Loss: 0.7083225250244141\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 735/937, Discriminator Loss: 0.698196530342102, Generator Loss: 0.7093262076377869\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 736/937, Discriminator Loss: 0.6894813179969788, Generator Loss: 0.7196424007415771\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 737/937, Discriminator Loss: 0.6824162900447845, Generator Loss: 0.7143574357032776\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 738/937, Discriminator Loss: 0.6927070617675781, Generator Loss: 0.7026601433753967\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 739/937, Discriminator Loss: 0.6807442009449005, Generator Loss: 0.7080389261245728\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 740/937, Discriminator Loss: 0.6893554031848907, Generator Loss: 0.7629828453063965\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 741/937, Discriminator Loss: 0.7124558687210083, Generator Loss: 0.7236752510070801\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 742/937, Discriminator Loss: 0.6973167657852173, Generator Loss: 0.7186007499694824\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 743/937, Discriminator Loss: 0.7001011967658997, Generator Loss: 0.7144970893859863\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 744/937, Discriminator Loss: 0.6818850338459015, Generator Loss: 0.7131801247596741\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 745/937, Discriminator Loss: 0.6757709085941315, Generator Loss: 0.7196570634841919\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 746/937, Discriminator Loss: 0.6984151005744934, Generator Loss: 0.7022235989570618\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 747/937, Discriminator Loss: 0.6890605688095093, Generator Loss: 0.7128757238388062\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 748/937, Discriminator Loss: 0.6967689692974091, Generator Loss: 0.7097080945968628\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 749/937, Discriminator Loss: 0.7016738057136536, Generator Loss: 0.7296061515808105\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 750/937, Discriminator Loss: 0.6799873113632202, Generator Loss: 0.7099795937538147\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 751/937, Discriminator Loss: 0.699898362159729, Generator Loss: 0.7076754570007324\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 752/937, Discriminator Loss: 0.6896699368953705, Generator Loss: 0.717620313167572\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 753/937, Discriminator Loss: 0.6769067943096161, Generator Loss: 0.7201837301254272\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 754/937, Discriminator Loss: 0.6900784969329834, Generator Loss: 0.7432396411895752\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 755/937, Discriminator Loss: 0.6766228675842285, Generator Loss: 0.7239519357681274\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "Epoch: 8/10, Batch: 756/937, Discriminator Loss: 0.6959785223007202, Generator Loss: 0.7229257822036743\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 757/937, Discriminator Loss: 0.7046698331832886, Generator Loss: 0.6966034173965454\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 758/937, Discriminator Loss: 0.6846659779548645, Generator Loss: 0.7133661508560181\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 759/937, Discriminator Loss: 0.6809004843235016, Generator Loss: 0.7043834924697876\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 760/937, Discriminator Loss: 0.6793281435966492, Generator Loss: 0.697816789150238\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 761/937, Discriminator Loss: 0.7063172459602356, Generator Loss: 0.7089272737503052\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 8/10, Batch: 762/937, Discriminator Loss: 0.6864040493965149, Generator Loss: 0.7178460955619812\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 763/937, Discriminator Loss: 0.7030445337295532, Generator Loss: 0.7247661352157593\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 764/937, Discriminator Loss: 0.6887568831443787, Generator Loss: 0.7298485040664673\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 8/10, Batch: 765/937, Discriminator Loss: 0.6821320652961731, Generator Loss: 0.7228871583938599\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 766/937, Discriminator Loss: 0.6870532631874084, Generator Loss: 0.695215106010437\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 767/937, Discriminator Loss: 0.6914724707603455, Generator Loss: 0.7146362066268921\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 768/937, Discriminator Loss: 0.7014754414558411, Generator Loss: 0.7097670435905457\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 769/937, Discriminator Loss: 0.6897419095039368, Generator Loss: 0.7110110521316528\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 770/937, Discriminator Loss: 0.6829094290733337, Generator Loss: 0.7007728815078735\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 771/937, Discriminator Loss: 0.6933810710906982, Generator Loss: 0.7121037840843201\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 772/937, Discriminator Loss: 0.691554069519043, Generator Loss: 0.7126908302307129\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 773/937, Discriminator Loss: 0.6858807504177094, Generator Loss: 0.7106332778930664\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 774/937, Discriminator Loss: 0.6873504817485809, Generator Loss: 0.7149011492729187\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 775/937, Discriminator Loss: 0.6945806443691254, Generator Loss: 0.7046222686767578\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 776/937, Discriminator Loss: 0.6878467202186584, Generator Loss: 0.7104114294052124\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 777/937, Discriminator Loss: 0.6748909950256348, Generator Loss: 0.7237602472305298\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 778/937, Discriminator Loss: 0.6947515606880188, Generator Loss: 0.7147830724716187\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 779/937, Discriminator Loss: 0.6799246072769165, Generator Loss: 0.7198989391326904\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 780/937, Discriminator Loss: 0.6833398938179016, Generator Loss: 0.7200462818145752\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 781/937, Discriminator Loss: 0.688698410987854, Generator Loss: 0.7173997163772583\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 782/937, Discriminator Loss: 0.6659251153469086, Generator Loss: 0.7516452074050903\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 783/937, Discriminator Loss: 0.7043405473232269, Generator Loss: 0.7424625158309937\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 784/937, Discriminator Loss: 0.6889289617538452, Generator Loss: 0.7223936319351196\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 785/937, Discriminator Loss: 0.7027467489242554, Generator Loss: 0.7098963856697083\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 786/937, Discriminator Loss: 0.6910796165466309, Generator Loss: 0.7216461896896362\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 787/937, Discriminator Loss: 0.6950465440750122, Generator Loss: 0.7164676189422607\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 788/937, Discriminator Loss: 0.6808424293994904, Generator Loss: 0.7152963280677795\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 789/937, Discriminator Loss: 0.6956000328063965, Generator Loss: 0.7081191539764404\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 790/937, Discriminator Loss: 0.7064312696456909, Generator Loss: 0.7276571989059448\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 791/937, Discriminator Loss: 0.6928821802139282, Generator Loss: 0.7117412090301514\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 792/937, Discriminator Loss: 0.6910396218299866, Generator Loss: 0.7118777632713318\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 793/937, Discriminator Loss: 0.6758849620819092, Generator Loss: 0.6979318261146545\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 794/937, Discriminator Loss: 0.6793099045753479, Generator Loss: 0.6976659297943115\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 795/937, Discriminator Loss: 0.7046265006065369, Generator Loss: 0.6966084241867065\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 796/937, Discriminator Loss: 0.6975575089454651, Generator Loss: 0.7121109962463379\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 797/937, Discriminator Loss: 0.6855629980564117, Generator Loss: 0.6889435052871704\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 798/937, Discriminator Loss: 0.691255122423172, Generator Loss: 0.7189943194389343\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 799/937, Discriminator Loss: 0.6867839097976685, Generator Loss: 0.7278059720993042\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 800/937, Discriminator Loss: 0.6865337789058685, Generator Loss: 0.7258602380752563\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 801/937, Discriminator Loss: 0.687606930732727, Generator Loss: 0.7144791483879089\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 802/937, Discriminator Loss: 0.6719623804092407, Generator Loss: 0.7609849572181702\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 803/937, Discriminator Loss: 0.6856149435043335, Generator Loss: 0.7106332778930664\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 804/937, Discriminator Loss: 0.7146326899528503, Generator Loss: 0.7266261577606201\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 805/937, Discriminator Loss: 0.6803727447986603, Generator Loss: 0.708571195602417\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 806/937, Discriminator Loss: 0.6857516765594482, Generator Loss: 0.7167396545410156\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 807/937, Discriminator Loss: 0.6785435080528259, Generator Loss: 0.7108931541442871\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 808/937, Discriminator Loss: 0.6925696134567261, Generator Loss: 0.7110403776168823\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 809/937, Discriminator Loss: 0.6980836391448975, Generator Loss: 0.7252583503723145\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 810/937, Discriminator Loss: 0.680623322725296, Generator Loss: 0.7054013013839722\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 811/937, Discriminator Loss: 0.6826818883419037, Generator Loss: 0.7121655941009521\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 812/937, Discriminator Loss: 0.6842802166938782, Generator Loss: 0.7123857736587524\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 813/937, Discriminator Loss: 0.6754921674728394, Generator Loss: 0.7256233096122742\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 814/937, Discriminator Loss: 0.698974609375, Generator Loss: 0.7035192847251892\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 815/937, Discriminator Loss: 0.7007271647453308, Generator Loss: 0.7130944728851318\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 816/937, Discriminator Loss: 0.6789172291755676, Generator Loss: 0.7220687866210938\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 817/937, Discriminator Loss: 0.6964767575263977, Generator Loss: 0.7022582292556763\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 818/937, Discriminator Loss: 0.6928741931915283, Generator Loss: 0.7120354175567627\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 819/937, Discriminator Loss: 0.6862435042858124, Generator Loss: 0.7242296934127808\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 820/937, Discriminator Loss: 0.6958972811698914, Generator Loss: 0.7249394059181213\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 821/937, Discriminator Loss: 0.7067748308181763, Generator Loss: 0.7228990793228149\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 822/937, Discriminator Loss: 0.6858251392841339, Generator Loss: 0.6849778890609741\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 823/937, Discriminator Loss: 0.7130930423736572, Generator Loss: 0.7157152891159058\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 824/937, Discriminator Loss: 0.7115635573863983, Generator Loss: 0.7202610969543457\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 825/937, Discriminator Loss: 0.6906208693981171, Generator Loss: 0.7116177678108215\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 826/937, Discriminator Loss: 0.6888667643070221, Generator Loss: 0.7155952453613281\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 827/937, Discriminator Loss: 0.7046582102775574, Generator Loss: 0.7171756029129028\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 828/937, Discriminator Loss: 0.7108076810836792, Generator Loss: 0.7135450839996338\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 829/937, Discriminator Loss: 0.6845122277736664, Generator Loss: 0.7286236882209778\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 830/937, Discriminator Loss: 0.6770310997962952, Generator Loss: 0.7052903175354004\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 831/937, Discriminator Loss: 0.6838570833206177, Generator Loss: 0.6945128440856934\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 832/937, Discriminator Loss: 0.7036995887756348, Generator Loss: 0.6897709965705872\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 833/937, Discriminator Loss: 0.6909420490264893, Generator Loss: 0.7195186614990234\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 834/937, Discriminator Loss: 0.7008413374423981, Generator Loss: 0.7185410261154175\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 835/937, Discriminator Loss: 0.6851191222667694, Generator Loss: 0.7241115570068359\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 836/937, Discriminator Loss: 0.6934019327163696, Generator Loss: 0.7201621532440186\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 837/937, Discriminator Loss: 0.6821896433830261, Generator Loss: 0.7143722176551819\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 838/937, Discriminator Loss: 0.6898249983787537, Generator Loss: 0.725165605545044\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 839/937, Discriminator Loss: 0.6863682568073273, Generator Loss: 0.7254971861839294\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 840/937, Discriminator Loss: 0.6886094212532043, Generator Loss: 0.7236013412475586\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 841/937, Discriminator Loss: 0.6980703771114349, Generator Loss: 0.7014115452766418\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 842/937, Discriminator Loss: 0.701566755771637, Generator Loss: 0.7123496532440186\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 843/937, Discriminator Loss: 0.6922978460788727, Generator Loss: 0.7288966178894043\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 844/937, Discriminator Loss: 0.6868100464344025, Generator Loss: 0.7216131091117859\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 845/937, Discriminator Loss: 0.6984130442142487, Generator Loss: 0.696605920791626\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 846/937, Discriminator Loss: 0.6969601213932037, Generator Loss: 0.7186711430549622\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 847/937, Discriminator Loss: 0.6748538911342621, Generator Loss: 0.7221827507019043\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 848/937, Discriminator Loss: 0.6922715306282043, Generator Loss: 0.7296252250671387\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 849/937, Discriminator Loss: 0.6916276812553406, Generator Loss: 0.7363434433937073\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 850/937, Discriminator Loss: 0.6896121203899384, Generator Loss: 0.7453524470329285\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 851/937, Discriminator Loss: 0.6971310377120972, Generator Loss: 0.7288024425506592\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 852/937, Discriminator Loss: 0.6923472881317139, Generator Loss: 0.7249964475631714\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 853/937, Discriminator Loss: 0.6853163540363312, Generator Loss: 0.7398473024368286\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 854/937, Discriminator Loss: 0.7023027837276459, Generator Loss: 0.7890172004699707\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 855/937, Discriminator Loss: 0.6822982430458069, Generator Loss: 0.7509223222732544\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 856/937, Discriminator Loss: 0.6931441724300385, Generator Loss: 0.7261834144592285\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 857/937, Discriminator Loss: 0.7085160613059998, Generator Loss: 0.7382622957229614\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 858/937, Discriminator Loss: 0.693874716758728, Generator Loss: 0.7225263118743896\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 859/937, Discriminator Loss: 0.6845670938491821, Generator Loss: 0.7395293712615967\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 860/937, Discriminator Loss: 0.7105341255664825, Generator Loss: 0.7470736503601074\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 861/937, Discriminator Loss: 0.7073823809623718, Generator Loss: 0.7317206859588623\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 862/937, Discriminator Loss: 0.69252809882164, Generator Loss: 0.7392867803573608\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 863/937, Discriminator Loss: 0.6902157962322235, Generator Loss: 0.7224448323249817\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 864/937, Discriminator Loss: 0.6948692500591278, Generator Loss: 0.7178572416305542\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 865/937, Discriminator Loss: 0.6988892555236816, Generator Loss: 0.7274662256240845\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 866/937, Discriminator Loss: 0.6974707841873169, Generator Loss: 0.7494011521339417\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 8/10, Batch: 867/937, Discriminator Loss: 0.6872450411319733, Generator Loss: 0.7398631572723389\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 868/937, Discriminator Loss: 0.6858510375022888, Generator Loss: 0.7693032622337341\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 869/937, Discriminator Loss: 0.6905976831912994, Generator Loss: 0.7486070394515991\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 870/937, Discriminator Loss: 0.6898901164531708, Generator Loss: 0.737686276435852\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 871/937, Discriminator Loss: 0.6927614212036133, Generator Loss: 0.7290263175964355\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 872/937, Discriminator Loss: 0.6913342475891113, Generator Loss: 0.7444353103637695\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 873/937, Discriminator Loss: 0.687967449426651, Generator Loss: 0.7575898170471191\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 874/937, Discriminator Loss: 0.6969590485095978, Generator Loss: 0.7254877090454102\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 875/937, Discriminator Loss: 0.7058353424072266, Generator Loss: 0.7328943014144897\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 876/937, Discriminator Loss: 0.6878696978092194, Generator Loss: 0.7357579469680786\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 8/10, Batch: 877/937, Discriminator Loss: 0.6909516155719757, Generator Loss: 0.7221900224685669\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 878/937, Discriminator Loss: 0.6920560002326965, Generator Loss: 0.7348124980926514\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 879/937, Discriminator Loss: 0.6921757757663727, Generator Loss: 0.7423039078712463\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 880/937, Discriminator Loss: 0.7023800909519196, Generator Loss: 0.7260465621948242\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 881/937, Discriminator Loss: 0.7010048627853394, Generator Loss: 0.7315629720687866\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 882/937, Discriminator Loss: 0.6881348192691803, Generator Loss: 0.738006591796875\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 883/937, Discriminator Loss: 0.6894858479499817, Generator Loss: 0.7444296479225159\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 884/937, Discriminator Loss: 0.7014890015125275, Generator Loss: 0.7277753353118896\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 885/937, Discriminator Loss: 0.7003158032894135, Generator Loss: 0.714091420173645\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 886/937, Discriminator Loss: 0.6884672343730927, Generator Loss: 0.7126113176345825\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 887/937, Discriminator Loss: 0.6832124590873718, Generator Loss: 0.7208672761917114\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 888/937, Discriminator Loss: 0.6923626959323883, Generator Loss: 0.7112264633178711\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 889/937, Discriminator Loss: 0.6982490122318268, Generator Loss: 0.7053171396255493\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 890/937, Discriminator Loss: 0.699508786201477, Generator Loss: 0.7229875326156616\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 891/937, Discriminator Loss: 0.6926402151584625, Generator Loss: 0.7252420783042908\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 892/937, Discriminator Loss: 0.6872159838676453, Generator Loss: 0.7113031148910522\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 893/937, Discriminator Loss: 0.6899777054786682, Generator Loss: 0.7011809349060059\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 894/937, Discriminator Loss: 0.6830763220787048, Generator Loss: 0.6952434778213501\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 895/937, Discriminator Loss: 0.6872155666351318, Generator Loss: 0.7043195962905884\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 896/937, Discriminator Loss: 0.6953522264957428, Generator Loss: 0.7100247144699097\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 8/10, Batch: 897/937, Discriminator Loss: 0.6929218173027039, Generator Loss: 0.7350171804428101\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 898/937, Discriminator Loss: 0.6961636245250702, Generator Loss: 0.7497918605804443\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 899/937, Discriminator Loss: 0.691436380147934, Generator Loss: 0.7202587127685547\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 900/937, Discriminator Loss: 0.684619665145874, Generator Loss: 0.7425589561462402\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 901/937, Discriminator Loss: 0.7006413638591766, Generator Loss: 0.7111630439758301\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 902/937, Discriminator Loss: 0.6766690015792847, Generator Loss: 0.6980307698249817\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 903/937, Discriminator Loss: 0.7189918160438538, Generator Loss: 0.7075836658477783\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 904/937, Discriminator Loss: 0.692738950252533, Generator Loss: 0.7214183807373047\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 8/10, Batch: 905/937, Discriminator Loss: 0.6851733922958374, Generator Loss: 0.7260735630989075\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 906/937, Discriminator Loss: 0.6947328150272369, Generator Loss: 0.7212665677070618\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 907/937, Discriminator Loss: 0.6844847798347473, Generator Loss: 0.7289865612983704\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 908/937, Discriminator Loss: 0.6939992606639862, Generator Loss: 0.7208240628242493\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 8/10, Batch: 909/937, Discriminator Loss: 0.6960062086582184, Generator Loss: 0.7051799297332764\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 910/937, Discriminator Loss: 0.6800483465194702, Generator Loss: 0.6917564868927002\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 911/937, Discriminator Loss: 0.6857844293117523, Generator Loss: 0.7092537879943848\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 912/937, Discriminator Loss: 0.6786761283874512, Generator Loss: 0.6946066617965698\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 913/937, Discriminator Loss: 0.6788329482078552, Generator Loss: 0.7273891568183899\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 914/937, Discriminator Loss: 0.69764643907547, Generator Loss: 0.7136263847351074\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 915/937, Discriminator Loss: 0.6995779275894165, Generator Loss: 0.7422162294387817\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 916/937, Discriminator Loss: 0.6885888278484344, Generator Loss: 0.7574766278266907\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 917/937, Discriminator Loss: 0.7002343535423279, Generator Loss: 0.7313904762268066\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 918/937, Discriminator Loss: 0.6925402879714966, Generator Loss: 0.7534164190292358\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 919/937, Discriminator Loss: 0.6926925480365753, Generator Loss: 0.8053370714187622\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 920/937, Discriminator Loss: 0.688002198934555, Generator Loss: 0.7330365777015686\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 921/937, Discriminator Loss: 0.6963277161121368, Generator Loss: 0.740810751914978\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 922/937, Discriminator Loss: 0.6954256594181061, Generator Loss: 0.7337374687194824\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 923/937, Discriminator Loss: 0.7033933401107788, Generator Loss: 0.734371542930603\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 8/10, Batch: 924/937, Discriminator Loss: 0.6751269400119781, Generator Loss: 0.7447946071624756\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 925/937, Discriminator Loss: 0.6846873164176941, Generator Loss: 0.7560151815414429\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 8/10, Batch: 926/937, Discriminator Loss: 0.6723106801509857, Generator Loss: 0.7423793077468872\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 927/937, Discriminator Loss: 0.6973726749420166, Generator Loss: 0.7350374460220337\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 928/937, Discriminator Loss: 0.6936688125133514, Generator Loss: 0.7319134473800659\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 929/937, Discriminator Loss: 0.6833283305168152, Generator Loss: 0.7150236368179321\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 930/937, Discriminator Loss: 0.6911779940128326, Generator Loss: 0.7323831915855408\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 8/10, Batch: 931/937, Discriminator Loss: 0.7274825572967529, Generator Loss: 0.7182255983352661\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 8/10, Batch: 932/937, Discriminator Loss: 0.674358069896698, Generator Loss: 0.7037068605422974\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 933/937, Discriminator Loss: 0.6950604915618896, Generator Loss: 0.7263244986534119\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 934/937, Discriminator Loss: 0.6933204829692841, Generator Loss: 0.7153867483139038\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 8/10, Batch: 935/937, Discriminator Loss: 0.69603231549263, Generator Loss: 0.7426860332489014\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 8/10, Batch: 936/937, Discriminator Loss: 0.6832028031349182, Generator Loss: 0.7291067838668823\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 8/10, Batch: 937/937, Discriminator Loss: 0.6900900602340698, Generator Loss: 0.7341489195823669\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 1/937, Discriminator Loss: 0.6891264617443085, Generator Loss: 0.7560703754425049\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 2/937, Discriminator Loss: 0.7006899118423462, Generator Loss: 0.7505084872245789\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 3/937, Discriminator Loss: 0.7017453908920288, Generator Loss: 0.7198340892791748\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 4/937, Discriminator Loss: 0.7164615392684937, Generator Loss: 0.7441474199295044\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 5/937, Discriminator Loss: 0.6812066435813904, Generator Loss: 0.7051236629486084\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 6/937, Discriminator Loss: 0.6869655251502991, Generator Loss: 0.7204902172088623\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 7/937, Discriminator Loss: 0.6729506552219391, Generator Loss: 0.7211538553237915\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 8/937, Discriminator Loss: 0.6741765737533569, Generator Loss: 0.7191131114959717\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 9/937, Discriminator Loss: 0.6909900903701782, Generator Loss: 0.7344104051589966\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 10/937, Discriminator Loss: 0.6820909678936005, Generator Loss: 0.7382534146308899\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 11/937, Discriminator Loss: 0.6809961795806885, Generator Loss: 0.7360106706619263\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 12/937, Discriminator Loss: 0.7011502981185913, Generator Loss: 0.745465874671936\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 13/937, Discriminator Loss: 0.7053397297859192, Generator Loss: 0.7991507053375244\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 14/937, Discriminator Loss: 0.7010548710823059, Generator Loss: 0.742438554763794\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 15/937, Discriminator Loss: 0.6838610768318176, Generator Loss: 0.7475748062133789\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 16/937, Discriminator Loss: 0.6621687710285187, Generator Loss: 0.7380817532539368\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 17/937, Discriminator Loss: 0.7053398489952087, Generator Loss: 0.7223316431045532\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 18/937, Discriminator Loss: 0.6927185356616974, Generator Loss: 0.7505745887756348\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 19/937, Discriminator Loss: 0.6732971668243408, Generator Loss: 0.7371941804885864\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 20/937, Discriminator Loss: 0.6769596040248871, Generator Loss: 0.6993488669395447\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 21/937, Discriminator Loss: 0.7019440233707428, Generator Loss: 0.754613995552063\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 22/937, Discriminator Loss: 0.6774227023124695, Generator Loss: 0.7388815879821777\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 23/937, Discriminator Loss: 0.6631706357002258, Generator Loss: 0.7350873351097107\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 24/937, Discriminator Loss: 0.6794079840183258, Generator Loss: 0.7262052893638611\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 25/937, Discriminator Loss: 0.70927494764328, Generator Loss: 0.7139058113098145\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 26/937, Discriminator Loss: 0.6763152778148651, Generator Loss: 0.7346540093421936\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 27/937, Discriminator Loss: 0.6900354623794556, Generator Loss: 0.7392594814300537\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 28/937, Discriminator Loss: 0.7631868720054626, Generator Loss: 0.7200723886489868\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 29/937, Discriminator Loss: 0.6958503425121307, Generator Loss: 0.7542169690132141\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 30/937, Discriminator Loss: 0.6927719712257385, Generator Loss: 0.8108642101287842\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 9/10, Batch: 31/937, Discriminator Loss: 0.6918982863426208, Generator Loss: 0.7704108953475952\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 32/937, Discriminator Loss: 0.7258189916610718, Generator Loss: 0.747022271156311\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 33/937, Discriminator Loss: 0.6976402699947357, Generator Loss: 0.7300601005554199\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 34/937, Discriminator Loss: 0.6701557636260986, Generator Loss: 0.727499783039093\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 35/937, Discriminator Loss: 0.6929588317871094, Generator Loss: 0.7296976447105408\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 36/937, Discriminator Loss: 0.6769067645072937, Generator Loss: 0.7385020852088928\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 37/937, Discriminator Loss: 0.7012030780315399, Generator Loss: 0.7177424430847168\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 38/937, Discriminator Loss: 0.6740739345550537, Generator Loss: 0.7289296984672546\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 39/937, Discriminator Loss: 0.7044090926647186, Generator Loss: 0.7185723185539246\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 40/937, Discriminator Loss: 0.6701633036136627, Generator Loss: 0.7357684969902039\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 41/937, Discriminator Loss: 0.6912647783756256, Generator Loss: 0.7266272902488708\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 42/937, Discriminator Loss: 0.718878448009491, Generator Loss: 0.6982337236404419\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 43/937, Discriminator Loss: 0.6671761274337769, Generator Loss: 0.7042765617370605\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 44/937, Discriminator Loss: 0.6759787797927856, Generator Loss: 0.705191969871521\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 45/937, Discriminator Loss: 0.6973154842853546, Generator Loss: 0.7250890135765076\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 46/937, Discriminator Loss: 0.6917403042316437, Generator Loss: 0.7531833052635193\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 47/937, Discriminator Loss: 0.6742205619812012, Generator Loss: 0.7414529323577881\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 48/937, Discriminator Loss: 0.6871435344219208, Generator Loss: 0.7612994909286499\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 49/937, Discriminator Loss: 0.678390234708786, Generator Loss: 0.740983247756958\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 50/937, Discriminator Loss: 0.6916028261184692, Generator Loss: 0.7530801296234131\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 51/937, Discriminator Loss: 0.6941823363304138, Generator Loss: 0.748635470867157\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 52/937, Discriminator Loss: 0.696978896856308, Generator Loss: 0.7432118654251099\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 53/937, Discriminator Loss: 0.6835199594497681, Generator Loss: 0.7886114120483398\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 54/937, Discriminator Loss: 0.6927393674850464, Generator Loss: 0.7461930513381958\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 55/937, Discriminator Loss: 0.6779228150844574, Generator Loss: 0.7494063377380371\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 56/937, Discriminator Loss: 0.6870015263557434, Generator Loss: 0.7334408164024353\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 57/937, Discriminator Loss: 0.6932472884654999, Generator Loss: 0.726967990398407\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 58/937, Discriminator Loss: 0.6958298087120056, Generator Loss: 0.7528687715530396\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 59/937, Discriminator Loss: 0.6828662753105164, Generator Loss: 0.7165287733078003\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 60/937, Discriminator Loss: 0.6804556548595428, Generator Loss: 0.7386388182640076\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 61/937, Discriminator Loss: 0.7041835486888885, Generator Loss: 0.7125910520553589\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 62/937, Discriminator Loss: 0.6820527017116547, Generator Loss: 0.7451179027557373\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 63/937, Discriminator Loss: 0.6907827854156494, Generator Loss: 0.7303906679153442\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 64/937, Discriminator Loss: 0.6986257135868073, Generator Loss: 0.7444076538085938\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 65/937, Discriminator Loss: 0.6904629468917847, Generator Loss: 0.7366228103637695\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 66/937, Discriminator Loss: 0.6879982650279999, Generator Loss: 0.749866247177124\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 67/937, Discriminator Loss: 0.6847043931484222, Generator Loss: 0.7352270483970642\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 68/937, Discriminator Loss: 0.6933380365371704, Generator Loss: 0.7260657548904419\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 69/937, Discriminator Loss: 0.6912124752998352, Generator Loss: 0.731349527835846\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 70/937, Discriminator Loss: 0.6972328126430511, Generator Loss: 0.7285616397857666\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 71/937, Discriminator Loss: 0.6870285868644714, Generator Loss: 0.7389340996742249\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 72/937, Discriminator Loss: 0.6758315563201904, Generator Loss: 0.7252542972564697\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 73/937, Discriminator Loss: 0.6903024315834045, Generator Loss: 0.7232404947280884\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 74/937, Discriminator Loss: 0.6774920225143433, Generator Loss: 0.7248326539993286\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 75/937, Discriminator Loss: 0.676241010427475, Generator Loss: 0.7216848134994507\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 76/937, Discriminator Loss: 0.7029523849487305, Generator Loss: 0.7168039083480835\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 77/937, Discriminator Loss: 0.70023313164711, Generator Loss: 0.723164439201355\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 78/937, Discriminator Loss: 0.7005269825458527, Generator Loss: 0.7207627296447754\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 79/937, Discriminator Loss: 0.6911510825157166, Generator Loss: 0.7318037748336792\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 80/937, Discriminator Loss: 0.7043433487415314, Generator Loss: 0.7067686319351196\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 81/937, Discriminator Loss: 0.6896443963050842, Generator Loss: 0.7114179134368896\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 82/937, Discriminator Loss: 0.6885721385478973, Generator Loss: 0.7176637649536133\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 83/937, Discriminator Loss: 0.7030089497566223, Generator Loss: 0.7456038594245911\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 84/937, Discriminator Loss: 0.6911969184875488, Generator Loss: 0.7262715101242065\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 85/937, Discriminator Loss: 0.6964649856090546, Generator Loss: 0.7298540472984314\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 86/937, Discriminator Loss: 0.6909120678901672, Generator Loss: 0.7409675717353821\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 87/937, Discriminator Loss: 0.6845865845680237, Generator Loss: 0.7477787137031555\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 88/937, Discriminator Loss: 0.6762375235557556, Generator Loss: 0.7415708303451538\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 89/937, Discriminator Loss: 0.673262894153595, Generator Loss: 0.7531853914260864\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 90/937, Discriminator Loss: 0.6891396045684814, Generator Loss: 0.7484832406044006\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 91/937, Discriminator Loss: 0.691991537809372, Generator Loss: 0.743598461151123\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 92/937, Discriminator Loss: 0.6873759627342224, Generator Loss: 0.7541561722755432\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 93/937, Discriminator Loss: 0.683058112859726, Generator Loss: 0.7427858114242554\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 94/937, Discriminator Loss: 0.7084127068519592, Generator Loss: 0.7516767978668213\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 95/937, Discriminator Loss: 0.6715970933437347, Generator Loss: 0.7369213104248047\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 96/937, Discriminator Loss: 0.7106704115867615, Generator Loss: 0.748377799987793\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 97/937, Discriminator Loss: 0.6851476430892944, Generator Loss: 0.7477883100509644\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 98/937, Discriminator Loss: 0.6948666572570801, Generator Loss: 0.73917156457901\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 99/937, Discriminator Loss: 0.6907064914703369, Generator Loss: 0.741562008857727\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 100/937, Discriminator Loss: 0.695696085691452, Generator Loss: 0.7426328063011169\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 101/937, Discriminator Loss: 0.6884782612323761, Generator Loss: 0.7340869307518005\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 102/937, Discriminator Loss: 0.6923755407333374, Generator Loss: 0.7454999089241028\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 103/937, Discriminator Loss: 0.6871858537197113, Generator Loss: 0.7457414269447327\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 104/937, Discriminator Loss: 0.6779942214488983, Generator Loss: 0.7658072710037231\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 105/937, Discriminator Loss: 0.6922339797019958, Generator Loss: 0.7421036958694458\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 106/937, Discriminator Loss: 0.6844115555286407, Generator Loss: 0.7525776624679565\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 107/937, Discriminator Loss: 0.6785559356212616, Generator Loss: 0.7417807579040527\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 108/937, Discriminator Loss: 0.6734055578708649, Generator Loss: 0.7368389964103699\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 109/937, Discriminator Loss: 0.6942685842514038, Generator Loss: 0.7286800146102905\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 110/937, Discriminator Loss: 0.6796658039093018, Generator Loss: 0.7241009473800659\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 111/937, Discriminator Loss: 0.6881798803806305, Generator Loss: 0.7387914061546326\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 112/937, Discriminator Loss: 0.6954351663589478, Generator Loss: 0.7472934722900391\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 113/937, Discriminator Loss: 0.6784356832504272, Generator Loss: 0.7357012033462524\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 114/937, Discriminator Loss: 0.6834125220775604, Generator Loss: 0.7043100595474243\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 115/937, Discriminator Loss: 0.6677495241165161, Generator Loss: 0.7088570594787598\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 116/937, Discriminator Loss: 0.6707057356834412, Generator Loss: 0.710364580154419\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 117/937, Discriminator Loss: 0.7122597992420197, Generator Loss: 0.723230242729187\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 118/937, Discriminator Loss: 0.6655654907226562, Generator Loss: 0.7451667189598083\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 119/937, Discriminator Loss: 0.7000871896743774, Generator Loss: 0.7185331583023071\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 120/937, Discriminator Loss: 0.6976140141487122, Generator Loss: 0.7087801098823547\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 121/937, Discriminator Loss: 0.67691370844841, Generator Loss: 0.7099354267120361\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 122/937, Discriminator Loss: 0.6734443604946136, Generator Loss: 0.7076375484466553\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 123/937, Discriminator Loss: 0.6922502219676971, Generator Loss: 0.721933126449585\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 124/937, Discriminator Loss: 0.6908316612243652, Generator Loss: 0.7215795516967773\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 125/937, Discriminator Loss: 0.6740309000015259, Generator Loss: 0.7267042994499207\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 126/937, Discriminator Loss: 0.6863314211368561, Generator Loss: 0.7395479083061218\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 127/937, Discriminator Loss: 0.674401044845581, Generator Loss: 0.713076114654541\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 128/937, Discriminator Loss: 0.6978433728218079, Generator Loss: 0.7327536344528198\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 129/937, Discriminator Loss: 0.6765789091587067, Generator Loss: 0.7154061794281006\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 130/937, Discriminator Loss: 0.6951507329940796, Generator Loss: 0.7260698080062866\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 131/937, Discriminator Loss: 0.6834474205970764, Generator Loss: 0.7359554767608643\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 132/937, Discriminator Loss: 0.6835485100746155, Generator Loss: 0.7259608507156372\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 133/937, Discriminator Loss: 0.678527295589447, Generator Loss: 0.7106852531433105\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 134/937, Discriminator Loss: 0.6822528541088104, Generator Loss: 0.7175239324569702\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 135/937, Discriminator Loss: 0.6841729283332825, Generator Loss: 0.7322869300842285\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 136/937, Discriminator Loss: 0.6912393271923065, Generator Loss: 0.7141956090927124\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 137/937, Discriminator Loss: 0.6779896318912506, Generator Loss: 0.7383252382278442\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 138/937, Discriminator Loss: 0.7078066468238831, Generator Loss: 0.7512302994728088\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 139/937, Discriminator Loss: 0.6756466329097748, Generator Loss: 0.7440106868743896\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 140/937, Discriminator Loss: 0.7020452916622162, Generator Loss: 0.728118896484375\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 141/937, Discriminator Loss: 0.6777837872505188, Generator Loss: 0.7166262865066528\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 142/937, Discriminator Loss: 0.7112859785556793, Generator Loss: 0.7200350761413574\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 143/937, Discriminator Loss: 0.677284836769104, Generator Loss: 0.7368232011795044\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 144/937, Discriminator Loss: 0.6899365186691284, Generator Loss: 0.7235198616981506\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 145/937, Discriminator Loss: 0.6967814862728119, Generator Loss: 0.7394303679466248\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 146/937, Discriminator Loss: 0.6839829683303833, Generator Loss: 0.7449150681495667\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 147/937, Discriminator Loss: 0.6860345005989075, Generator Loss: 0.7394776940345764\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 148/937, Discriminator Loss: 0.6801044642925262, Generator Loss: 0.7575302720069885\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 149/937, Discriminator Loss: 0.6843014657497406, Generator Loss: 0.7311558723449707\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 150/937, Discriminator Loss: 0.6969679594039917, Generator Loss: 0.7355346083641052\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 151/937, Discriminator Loss: 0.6817910373210907, Generator Loss: 0.7300711870193481\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 152/937, Discriminator Loss: 0.6673236787319183, Generator Loss: 0.7122770547866821\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 153/937, Discriminator Loss: 0.7146385312080383, Generator Loss: 0.7232444286346436\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 154/937, Discriminator Loss: 0.6641245782375336, Generator Loss: 0.7336528301239014\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 155/937, Discriminator Loss: 0.669305145740509, Generator Loss: 0.7302131652832031\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 156/937, Discriminator Loss: 0.6777632236480713, Generator Loss: 0.7458456754684448\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 157/937, Discriminator Loss: 0.7147577702999115, Generator Loss: 0.7273982167243958\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 158/937, Discriminator Loss: 0.6990272104740143, Generator Loss: 0.7574335336685181\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 159/937, Discriminator Loss: 0.6746453046798706, Generator Loss: 0.7480102777481079\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 160/937, Discriminator Loss: 0.6914856433868408, Generator Loss: 0.7656436562538147\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 161/937, Discriminator Loss: 0.6881020963191986, Generator Loss: 0.7499319314956665\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 162/937, Discriminator Loss: 0.7043220400810242, Generator Loss: 0.733755886554718\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 163/937, Discriminator Loss: 0.6717975735664368, Generator Loss: 0.7237601280212402\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 164/937, Discriminator Loss: 0.708651065826416, Generator Loss: 0.7575887441635132\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 165/937, Discriminator Loss: 0.683553159236908, Generator Loss: 0.7201582789421082\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 166/937, Discriminator Loss: 0.6856949329376221, Generator Loss: 0.7129574418067932\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 167/937, Discriminator Loss: 0.6927398145198822, Generator Loss: 0.7359297275543213\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 168/937, Discriminator Loss: 0.6953217387199402, Generator Loss: 0.7286668419837952\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 169/937, Discriminator Loss: 0.7316740453243256, Generator Loss: 0.7144942283630371\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 170/937, Discriminator Loss: 0.699690192937851, Generator Loss: 0.757907509803772\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 171/937, Discriminator Loss: 0.6877502202987671, Generator Loss: 0.752770185470581\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 172/937, Discriminator Loss: 0.6826165914535522, Generator Loss: 0.7038549184799194\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 173/937, Discriminator Loss: 0.6768817007541656, Generator Loss: 0.696007490158081\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 174/937, Discriminator Loss: 0.696229875087738, Generator Loss: 0.7195745706558228\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 175/937, Discriminator Loss: 0.6652370691299438, Generator Loss: 0.7350947856903076\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 176/937, Discriminator Loss: 0.6738460063934326, Generator Loss: 0.7325716018676758\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 177/937, Discriminator Loss: 0.6959923505783081, Generator Loss: 0.7105833888053894\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 178/937, Discriminator Loss: 0.7031625211238861, Generator Loss: 0.7112176418304443\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 179/937, Discriminator Loss: 0.6812075674533844, Generator Loss: 0.7309839725494385\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 180/937, Discriminator Loss: 0.6871189773082733, Generator Loss: 0.7323223948478699\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 181/937, Discriminator Loss: 0.6907370686531067, Generator Loss: 0.734831690788269\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 182/937, Discriminator Loss: 0.6813270449638367, Generator Loss: 0.7273104190826416\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 183/937, Discriminator Loss: 0.6950234174728394, Generator Loss: 0.7165778875350952\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 184/937, Discriminator Loss: 0.6964646577835083, Generator Loss: 0.7438886165618896\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 185/937, Discriminator Loss: 0.6964304447174072, Generator Loss: 0.726385772228241\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 186/937, Discriminator Loss: 0.6929903626441956, Generator Loss: 0.7301432490348816\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 187/937, Discriminator Loss: 0.6853915154933929, Generator Loss: 0.7411373853683472\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 188/937, Discriminator Loss: 0.6948260068893433, Generator Loss: 0.7300040125846863\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 189/937, Discriminator Loss: 0.6816815733909607, Generator Loss: 0.7232354879379272\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 190/937, Discriminator Loss: 0.6855855584144592, Generator Loss: 0.7469210028648376\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 191/937, Discriminator Loss: 0.6907945573329926, Generator Loss: 0.7249833345413208\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 192/937, Discriminator Loss: 0.6994259059429169, Generator Loss: 0.7168045043945312\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 193/937, Discriminator Loss: 0.6866483390331268, Generator Loss: 0.7247785925865173\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 194/937, Discriminator Loss: 0.6884321570396423, Generator Loss: 0.7249927520751953\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 195/937, Discriminator Loss: 0.687606543302536, Generator Loss: 0.7194331288337708\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 196/937, Discriminator Loss: 0.6901777982711792, Generator Loss: 0.7281534671783447\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 197/937, Discriminator Loss: 0.6857036054134369, Generator Loss: 0.7269638776779175\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 198/937, Discriminator Loss: 0.6956222653388977, Generator Loss: 0.7318269610404968\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 199/937, Discriminator Loss: 0.6867808997631073, Generator Loss: 0.744008481502533\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 200/937, Discriminator Loss: 0.6973179876804352, Generator Loss: 0.7347849607467651\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 201/937, Discriminator Loss: 0.6765978932380676, Generator Loss: 0.721189022064209\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 202/937, Discriminator Loss: 0.6844057738780975, Generator Loss: 0.7302159667015076\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 203/937, Discriminator Loss: 0.681063711643219, Generator Loss: 0.7434918284416199\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 204/937, Discriminator Loss: 0.6963901519775391, Generator Loss: 0.7220697999000549\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 205/937, Discriminator Loss: 0.7209413647651672, Generator Loss: 0.7016541957855225\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 206/937, Discriminator Loss: 0.6868329048156738, Generator Loss: 0.7193165421485901\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 207/937, Discriminator Loss: 0.6857601404190063, Generator Loss: 0.7191168069839478\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 208/937, Discriminator Loss: 0.6834172606468201, Generator Loss: 0.7216811180114746\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 209/937, Discriminator Loss: 0.6874567270278931, Generator Loss: 0.7147172689437866\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 210/937, Discriminator Loss: 0.6889991462230682, Generator Loss: 0.7097404599189758\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 211/937, Discriminator Loss: 0.6786239445209503, Generator Loss: 0.7041155099868774\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 212/937, Discriminator Loss: 0.7003186643123627, Generator Loss: 0.7150664329528809\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 213/937, Discriminator Loss: 0.6846268773078918, Generator Loss: 0.7199781537055969\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 214/937, Discriminator Loss: 0.6860964000225067, Generator Loss: 0.7073476314544678\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 215/937, Discriminator Loss: 0.6916034817695618, Generator Loss: 0.7497475147247314\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 216/937, Discriminator Loss: 0.6841023564338684, Generator Loss: 0.726665198802948\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 217/937, Discriminator Loss: 0.6959861814975739, Generator Loss: 0.7218509912490845\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 218/937, Discriminator Loss: 0.6837301552295685, Generator Loss: 0.7441356778144836\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 219/937, Discriminator Loss: 0.6694633662700653, Generator Loss: 0.7277076244354248\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 220/937, Discriminator Loss: 0.6892252862453461, Generator Loss: 0.7213357090950012\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 221/937, Discriminator Loss: 0.6966855525970459, Generator Loss: 0.7213211059570312\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 222/937, Discriminator Loss: 0.6949757933616638, Generator Loss: 0.7209821939468384\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 223/937, Discriminator Loss: 0.6808705925941467, Generator Loss: 0.7330455780029297\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 224/937, Discriminator Loss: 0.6842171549797058, Generator Loss: 0.7604244351387024\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 225/937, Discriminator Loss: 0.6748610138893127, Generator Loss: 0.7517914772033691\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 226/937, Discriminator Loss: 0.7058488726615906, Generator Loss: 0.7332702875137329\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 227/937, Discriminator Loss: 0.6810358166694641, Generator Loss: 0.7197743654251099\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 228/937, Discriminator Loss: 0.7092515826225281, Generator Loss: 0.7361411452293396\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 229/937, Discriminator Loss: 0.6861540079116821, Generator Loss: 0.7312410473823547\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 230/937, Discriminator Loss: 0.6968722939491272, Generator Loss: 0.7129997611045837\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 231/937, Discriminator Loss: 0.6924745142459869, Generator Loss: 0.7178947329521179\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 232/937, Discriminator Loss: 0.6887634694576263, Generator Loss: 0.7077682614326477\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 233/937, Discriminator Loss: 0.7133512496948242, Generator Loss: 0.7038037776947021\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 234/937, Discriminator Loss: 0.69162717461586, Generator Loss: 0.7340632677078247\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 235/937, Discriminator Loss: 0.6837944388389587, Generator Loss: 0.7409178018569946\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 236/937, Discriminator Loss: 0.686019629240036, Generator Loss: 0.7262506484985352\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 237/937, Discriminator Loss: 0.694462388753891, Generator Loss: 0.7291883230209351\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 238/937, Discriminator Loss: 0.6938415765762329, Generator Loss: 0.7300845384597778\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 239/937, Discriminator Loss: 0.6782103776931763, Generator Loss: 0.7316354513168335\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 240/937, Discriminator Loss: 0.6706225275993347, Generator Loss: 0.7613598108291626\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 241/937, Discriminator Loss: 0.6815108954906464, Generator Loss: 0.7844461798667908\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 242/937, Discriminator Loss: 0.6911746859550476, Generator Loss: 0.7556842565536499\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 243/937, Discriminator Loss: 0.6932369470596313, Generator Loss: 0.735369861125946\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 244/937, Discriminator Loss: 0.720894068479538, Generator Loss: 0.7193425893783569\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 245/937, Discriminator Loss: 0.6878170669078827, Generator Loss: 0.729944109916687\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 246/937, Discriminator Loss: 0.6807269752025604, Generator Loss: 0.7535797357559204\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 247/937, Discriminator Loss: 0.6939725875854492, Generator Loss: 0.7418299913406372\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 248/937, Discriminator Loss: 0.6794239282608032, Generator Loss: 0.7182772159576416\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 249/937, Discriminator Loss: 0.6909463703632355, Generator Loss: 0.743260383605957\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 250/937, Discriminator Loss: 0.6949581205844879, Generator Loss: 0.7377709150314331\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 251/937, Discriminator Loss: 0.6943328976631165, Generator Loss: 0.738703191280365\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 252/937, Discriminator Loss: 0.7117363214492798, Generator Loss: 0.7423675656318665\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 253/937, Discriminator Loss: 0.6923556923866272, Generator Loss: 0.7359456419944763\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 254/937, Discriminator Loss: 0.6730345487594604, Generator Loss: 0.750436544418335\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 255/937, Discriminator Loss: 0.6760905086994171, Generator Loss: 0.7577705383300781\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 256/937, Discriminator Loss: 0.6842761337757111, Generator Loss: 0.7267880439758301\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 257/937, Discriminator Loss: 0.6932097673416138, Generator Loss: 0.7429298758506775\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 258/937, Discriminator Loss: 0.6960791647434235, Generator Loss: 0.7417221069335938\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 259/937, Discriminator Loss: 0.6835966110229492, Generator Loss: 0.741192638874054\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 260/937, Discriminator Loss: 0.6764069199562073, Generator Loss: 0.728132963180542\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 261/937, Discriminator Loss: 0.6782431602478027, Generator Loss: 0.7252377867698669\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 262/937, Discriminator Loss: 0.7066856324672699, Generator Loss: 0.7168982028961182\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 263/937, Discriminator Loss: 0.6855672895908356, Generator Loss: 0.7333513498306274\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 264/937, Discriminator Loss: 0.6968668103218079, Generator Loss: 0.7272850275039673\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 265/937, Discriminator Loss: 0.6672787964344025, Generator Loss: 0.740394115447998\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 266/937, Discriminator Loss: 0.6858568787574768, Generator Loss: 0.7388883233070374\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 267/937, Discriminator Loss: 0.6888602375984192, Generator Loss: 0.7209388613700867\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 268/937, Discriminator Loss: 0.6928111910820007, Generator Loss: 0.7403531670570374\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 269/937, Discriminator Loss: 0.6754229962825775, Generator Loss: 0.7140339016914368\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 270/937, Discriminator Loss: 0.6703354716300964, Generator Loss: 0.7410169839859009\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 271/937, Discriminator Loss: 0.6859374940395355, Generator Loss: 0.7432112693786621\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 272/937, Discriminator Loss: 0.690981388092041, Generator Loss: 0.727617084980011\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 273/937, Discriminator Loss: 0.706728458404541, Generator Loss: 0.7252258062362671\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 274/937, Discriminator Loss: 0.6902400851249695, Generator Loss: 0.7224739789962769\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 275/937, Discriminator Loss: 0.69059157371521, Generator Loss: 0.7252041101455688\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 276/937, Discriminator Loss: 0.6734804511070251, Generator Loss: 0.7208625078201294\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 277/937, Discriminator Loss: 0.6791622936725616, Generator Loss: 0.72606360912323\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 278/937, Discriminator Loss: 0.6515456140041351, Generator Loss: 0.7099196910858154\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 279/937, Discriminator Loss: 0.6821678578853607, Generator Loss: 0.7146469950675964\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 280/937, Discriminator Loss: 0.6957061886787415, Generator Loss: 0.7151662111282349\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 281/937, Discriminator Loss: 0.6894681453704834, Generator Loss: 0.7347656488418579\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 282/937, Discriminator Loss: 0.6996232569217682, Generator Loss: 0.7399200797080994\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 283/937, Discriminator Loss: 0.6881119608879089, Generator Loss: 0.748860776424408\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 284/937, Discriminator Loss: 0.6962362229824066, Generator Loss: 0.7520399689674377\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 285/937, Discriminator Loss: 0.6947611570358276, Generator Loss: 0.7468037009239197\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 286/937, Discriminator Loss: 0.6963027715682983, Generator Loss: 0.7528159618377686\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 287/937, Discriminator Loss: 0.6885095834732056, Generator Loss: 0.7230031490325928\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 288/937, Discriminator Loss: 0.6823744773864746, Generator Loss: 0.7381066083908081\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 289/937, Discriminator Loss: 0.6719987392425537, Generator Loss: 0.7250378727912903\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 290/937, Discriminator Loss: 0.6844275593757629, Generator Loss: 0.7072920799255371\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 291/937, Discriminator Loss: 0.6673977971076965, Generator Loss: 0.7256284952163696\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 292/937, Discriminator Loss: 0.7278056740760803, Generator Loss: 0.726585865020752\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 293/937, Discriminator Loss: 0.6747803390026093, Generator Loss: 0.716859757900238\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 294/937, Discriminator Loss: 0.7309470176696777, Generator Loss: 0.7303394079208374\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 295/937, Discriminator Loss: 0.6906679570674896, Generator Loss: 0.7136915326118469\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 296/937, Discriminator Loss: 0.6879121363162994, Generator Loss: 0.7024801969528198\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 297/937, Discriminator Loss: 0.6832587420940399, Generator Loss: 0.7208813428878784\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 298/937, Discriminator Loss: 0.685255229473114, Generator Loss: 0.7119793891906738\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 299/937, Discriminator Loss: 0.6967289447784424, Generator Loss: 0.6939643621444702\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 300/937, Discriminator Loss: 0.7111931443214417, Generator Loss: 0.7532567977905273\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 301/937, Discriminator Loss: 0.7132666409015656, Generator Loss: 0.7288346290588379\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 302/937, Discriminator Loss: 0.6823599934577942, Generator Loss: 0.746505618095398\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 303/937, Discriminator Loss: 0.6884637773036957, Generator Loss: 0.7599055767059326\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 304/937, Discriminator Loss: 0.6894347965717316, Generator Loss: 0.7635536193847656\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 305/937, Discriminator Loss: 0.6909081339836121, Generator Loss: 0.745641827583313\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 306/937, Discriminator Loss: 0.6987034678459167, Generator Loss: 0.7425900101661682\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 307/937, Discriminator Loss: 0.6876119375228882, Generator Loss: 0.7644006013870239\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 308/937, Discriminator Loss: 0.6943295896053314, Generator Loss: 0.7428271174430847\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 309/937, Discriminator Loss: 0.6842171847820282, Generator Loss: 0.7708919048309326\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 310/937, Discriminator Loss: 0.7073157429695129, Generator Loss: 0.7155777215957642\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 311/937, Discriminator Loss: 0.6951651573181152, Generator Loss: 0.7338306903839111\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 312/937, Discriminator Loss: 0.6871612071990967, Generator Loss: 0.7317641973495483\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 313/937, Discriminator Loss: 0.671759307384491, Generator Loss: 0.7185729742050171\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 314/937, Discriminator Loss: 0.7073067426681519, Generator Loss: 0.708821177482605\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 315/937, Discriminator Loss: 0.7045941352844238, Generator Loss: 0.7339377403259277\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 316/937, Discriminator Loss: 0.7115199565887451, Generator Loss: 0.7449997663497925\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 317/937, Discriminator Loss: 0.682337760925293, Generator Loss: 0.7599600553512573\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 318/937, Discriminator Loss: 0.6814888119697571, Generator Loss: 0.7370642423629761\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 319/937, Discriminator Loss: 0.6968086957931519, Generator Loss: 0.7680832147598267\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 320/937, Discriminator Loss: 0.6847858130931854, Generator Loss: 0.7409133911132812\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Epoch: 9/10, Batch: 321/937, Discriminator Loss: 0.6766089200973511, Generator Loss: 0.7523477077484131\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 322/937, Discriminator Loss: 0.7073437571525574, Generator Loss: 0.7365720272064209\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 323/937, Discriminator Loss: 0.6902549266815186, Generator Loss: 0.7198869585990906\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 324/937, Discriminator Loss: 0.6969168186187744, Generator Loss: 0.7288351058959961\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 325/937, Discriminator Loss: 0.6880395710468292, Generator Loss: 0.7373121976852417\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 326/937, Discriminator Loss: 0.6745086312294006, Generator Loss: 0.7497179508209229\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 327/937, Discriminator Loss: 0.6874091029167175, Generator Loss: 0.7493529319763184\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 328/937, Discriminator Loss: 0.7042188942432404, Generator Loss: 0.7260034084320068\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 329/937, Discriminator Loss: 0.6936762034893036, Generator Loss: 0.7532910108566284\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 330/937, Discriminator Loss: 0.7107149958610535, Generator Loss: 0.7347438931465149\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 331/937, Discriminator Loss: 0.6813239455223083, Generator Loss: 0.7361155152320862\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 332/937, Discriminator Loss: 0.6827337741851807, Generator Loss: 0.7364771962165833\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 333/937, Discriminator Loss: 0.6904347538948059, Generator Loss: 0.7124351263046265\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 334/937, Discriminator Loss: 0.6798975467681885, Generator Loss: 0.7077914476394653\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 335/937, Discriminator Loss: 0.6709703207015991, Generator Loss: 0.7398477792739868\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 336/937, Discriminator Loss: 0.7083095014095306, Generator Loss: 0.7453925013542175\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 337/937, Discriminator Loss: 0.6720786392688751, Generator Loss: 0.7357112169265747\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 338/937, Discriminator Loss: 0.7065244615077972, Generator Loss: 0.7776800394058228\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 339/937, Discriminator Loss: 0.6973785758018494, Generator Loss: 0.7285690307617188\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 340/937, Discriminator Loss: 0.6900814473628998, Generator Loss: 0.7282557487487793\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 341/937, Discriminator Loss: 0.6837024390697479, Generator Loss: 0.7391692399978638\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 342/937, Discriminator Loss: 0.6865316033363342, Generator Loss: 0.7197617292404175\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 343/937, Discriminator Loss: 0.6850637495517731, Generator Loss: 0.7308293581008911\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 344/937, Discriminator Loss: 0.6873923242092133, Generator Loss: 0.7218910455703735\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 345/937, Discriminator Loss: 0.702670156955719, Generator Loss: 0.7816658020019531\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 346/937, Discriminator Loss: 0.6898802816867828, Generator Loss: 0.732617974281311\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 347/937, Discriminator Loss: 0.6796158254146576, Generator Loss: 0.7502777576446533\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 348/937, Discriminator Loss: 0.6857627332210541, Generator Loss: 0.7188212871551514\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 349/937, Discriminator Loss: 0.7025532722473145, Generator Loss: 0.7319348454475403\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 350/937, Discriminator Loss: 0.6942974030971527, Generator Loss: 0.7208977937698364\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 351/937, Discriminator Loss: 0.7049644291400909, Generator Loss: 0.7082587480545044\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 352/937, Discriminator Loss: 0.7048898339271545, Generator Loss: 0.7028094530105591\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 353/937, Discriminator Loss: 0.7292232513427734, Generator Loss: 0.7368022203445435\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 354/937, Discriminator Loss: 0.6956761181354523, Generator Loss: 0.7179126739501953\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 355/937, Discriminator Loss: 0.6755786240100861, Generator Loss: 0.7205316424369812\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 356/937, Discriminator Loss: 0.6952993869781494, Generator Loss: 0.7192949056625366\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 357/937, Discriminator Loss: 0.7093777060508728, Generator Loss: 0.710427463054657\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 358/937, Discriminator Loss: 0.6938462257385254, Generator Loss: 0.7365375757217407\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 359/937, Discriminator Loss: 0.6970482468605042, Generator Loss: 0.715877890586853\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 360/937, Discriminator Loss: 0.6924380958080292, Generator Loss: 0.7519860863685608\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 361/937, Discriminator Loss: 0.6832470893859863, Generator Loss: 0.7657372951507568\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 362/937, Discriminator Loss: 0.683461457490921, Generator Loss: 0.757639467716217\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 363/937, Discriminator Loss: 0.6841468811035156, Generator Loss: 0.7675397396087646\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 364/937, Discriminator Loss: 0.7105740308761597, Generator Loss: 0.7210439443588257\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 365/937, Discriminator Loss: 0.6711733639240265, Generator Loss: 0.7211558818817139\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 366/937, Discriminator Loss: 0.7044082880020142, Generator Loss: 0.7212713956832886\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 367/937, Discriminator Loss: 0.6904730498790741, Generator Loss: 0.7190190553665161\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 368/937, Discriminator Loss: 0.6885701417922974, Generator Loss: 0.7335546612739563\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 369/937, Discriminator Loss: 0.6962922215461731, Generator Loss: 0.7021074295043945\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 370/937, Discriminator Loss: 0.6796759068965912, Generator Loss: 0.7342089414596558\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 371/937, Discriminator Loss: 0.7018641233444214, Generator Loss: 0.7199783325195312\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 372/937, Discriminator Loss: 0.6706252098083496, Generator Loss: 0.726361095905304\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 373/937, Discriminator Loss: 0.6806620359420776, Generator Loss: 0.7087447643280029\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 374/937, Discriminator Loss: 0.6978718638420105, Generator Loss: 0.7193690538406372\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 375/937, Discriminator Loss: 0.7065333724021912, Generator Loss: 0.7199468016624451\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 376/937, Discriminator Loss: 0.6940991580486298, Generator Loss: 0.7224160432815552\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 377/937, Discriminator Loss: 0.6916555464267731, Generator Loss: 0.8449696898460388\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 378/937, Discriminator Loss: 0.743856281042099, Generator Loss: 0.7231458425521851\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 379/937, Discriminator Loss: 0.6824410259723663, Generator Loss: 0.7181179523468018\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 380/937, Discriminator Loss: 0.6791471540927887, Generator Loss: 0.7092630863189697\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 381/937, Discriminator Loss: 0.7076995670795441, Generator Loss: 0.7316592335700989\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 382/937, Discriminator Loss: 0.684570699930191, Generator Loss: 0.732068657875061\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 383/937, Discriminator Loss: 0.6830471754074097, Generator Loss: 0.7260781526565552\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 384/937, Discriminator Loss: 0.6918022930622101, Generator Loss: 0.7218162417411804\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 385/937, Discriminator Loss: 0.6879214346408844, Generator Loss: 0.7403320074081421\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 386/937, Discriminator Loss: 0.6875732243061066, Generator Loss: 0.7369177341461182\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 387/937, Discriminator Loss: 0.6966975629329681, Generator Loss: 0.7434617280960083\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 388/937, Discriminator Loss: 0.6823299527168274, Generator Loss: 0.7520987391471863\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 389/937, Discriminator Loss: 0.6817501485347748, Generator Loss: 0.7567423582077026\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "Epoch: 9/10, Batch: 390/937, Discriminator Loss: 0.6732165217399597, Generator Loss: 0.7523016929626465\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 391/937, Discriminator Loss: 0.67835533618927, Generator Loss: 0.7728240489959717\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 392/937, Discriminator Loss: 0.6973848342895508, Generator Loss: 0.7367216348648071\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 393/937, Discriminator Loss: 0.6841410994529724, Generator Loss: 0.7152806520462036\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 394/937, Discriminator Loss: 0.6684279143810272, Generator Loss: 0.7284988164901733\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 395/937, Discriminator Loss: 0.6961394250392914, Generator Loss: 0.7582448720932007\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 396/937, Discriminator Loss: 0.6879555881023407, Generator Loss: 0.8060978651046753\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 397/937, Discriminator Loss: 0.7262804508209229, Generator Loss: 0.7124418616294861\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 398/937, Discriminator Loss: 0.6857273578643799, Generator Loss: 0.7168304920196533\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 399/937, Discriminator Loss: 0.6942707598209381, Generator Loss: 0.7024861574172974\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 400/937, Discriminator Loss: 0.6808408498764038, Generator Loss: 0.7216894626617432\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 401/937, Discriminator Loss: 0.6684923470020294, Generator Loss: 0.7191476821899414\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 402/937, Discriminator Loss: 0.6898992955684662, Generator Loss: 0.7050999402999878\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 403/937, Discriminator Loss: 0.6990647614002228, Generator Loss: 0.7308483123779297\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 404/937, Discriminator Loss: 0.6845248639583588, Generator Loss: 0.7474079132080078\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 405/937, Discriminator Loss: 0.6859928965568542, Generator Loss: 0.7516512870788574\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 406/937, Discriminator Loss: 0.7032915949821472, Generator Loss: 0.7045432925224304\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 407/937, Discriminator Loss: 0.6911098957061768, Generator Loss: 0.7184269428253174\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 408/937, Discriminator Loss: 0.6702890992164612, Generator Loss: 0.7169110774993896\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 409/937, Discriminator Loss: 0.6945778727531433, Generator Loss: 0.7124576568603516\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 410/937, Discriminator Loss: 0.673235684633255, Generator Loss: 0.7463732361793518\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 411/937, Discriminator Loss: 0.6755703985691071, Generator Loss: 0.7231476902961731\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 412/937, Discriminator Loss: 0.6986669898033142, Generator Loss: 0.7280986309051514\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 413/937, Discriminator Loss: 0.6863356530666351, Generator Loss: 0.765476405620575\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 414/937, Discriminator Loss: 0.6818028688430786, Generator Loss: 0.7300053238868713\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 415/937, Discriminator Loss: 0.6827354431152344, Generator Loss: 0.7030439376831055\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 416/937, Discriminator Loss: 0.6796179115772247, Generator Loss: 0.7350982427597046\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 417/937, Discriminator Loss: 0.6721127927303314, Generator Loss: 0.7128704786300659\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 418/937, Discriminator Loss: 0.6794115006923676, Generator Loss: 0.7196826934814453\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 419/937, Discriminator Loss: 0.6619303822517395, Generator Loss: 0.742734432220459\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 420/937, Discriminator Loss: 0.6687209904193878, Generator Loss: 0.730965256690979\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 421/937, Discriminator Loss: 0.6887069642543793, Generator Loss: 0.762645959854126\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 422/937, Discriminator Loss: 0.690572202205658, Generator Loss: 0.7586674094200134\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 423/937, Discriminator Loss: 0.6793853640556335, Generator Loss: 0.7549120187759399\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 424/937, Discriminator Loss: 0.7019832730293274, Generator Loss: 0.7395461797714233\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 425/937, Discriminator Loss: 0.7143475413322449, Generator Loss: 0.7343801259994507\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 426/937, Discriminator Loss: 0.6712894141674042, Generator Loss: 0.7324252128601074\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 427/937, Discriminator Loss: 0.6696892082691193, Generator Loss: 0.7441420555114746\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 428/937, Discriminator Loss: 0.6838366687297821, Generator Loss: 0.731868326663971\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 429/937, Discriminator Loss: 0.6831739842891693, Generator Loss: 0.7240926623344421\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 430/937, Discriminator Loss: 0.6873745024204254, Generator Loss: 0.7269302606582642\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 431/937, Discriminator Loss: 0.68236243724823, Generator Loss: 0.7169795632362366\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 432/937, Discriminator Loss: 0.6866345107555389, Generator Loss: 0.7266653776168823\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 433/937, Discriminator Loss: 0.6843659281730652, Generator Loss: 0.7159314155578613\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 434/937, Discriminator Loss: 0.6836128234863281, Generator Loss: 0.7255246043205261\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 435/937, Discriminator Loss: 0.6848982274532318, Generator Loss: 0.721546471118927\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 436/937, Discriminator Loss: 0.6696548163890839, Generator Loss: 0.7003316879272461\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 437/937, Discriminator Loss: 0.6921567022800446, Generator Loss: 0.7323663234710693\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 438/937, Discriminator Loss: 0.7009237408638, Generator Loss: 0.714059591293335\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 439/937, Discriminator Loss: 0.668350487947464, Generator Loss: 0.7288083434104919\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 440/937, Discriminator Loss: 0.6813200414180756, Generator Loss: 0.7438876628875732\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 441/937, Discriminator Loss: 0.6960518956184387, Generator Loss: 0.7423580884933472\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 442/937, Discriminator Loss: 0.6822711825370789, Generator Loss: 0.7247306108474731\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 443/937, Discriminator Loss: 0.6934194564819336, Generator Loss: 0.7453136444091797\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 444/937, Discriminator Loss: 0.6842231154441833, Generator Loss: 0.7295926213264465\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 445/937, Discriminator Loss: 0.7078296542167664, Generator Loss: 0.743812620639801\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 446/937, Discriminator Loss: 0.7048507928848267, Generator Loss: 0.747158944606781\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 447/937, Discriminator Loss: 0.6814563870429993, Generator Loss: 0.7477234601974487\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 448/937, Discriminator Loss: 0.6987471580505371, Generator Loss: 0.7433415651321411\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 449/937, Discriminator Loss: 0.7103033363819122, Generator Loss: 0.716428816318512\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 450/937, Discriminator Loss: 0.6905683279037476, Generator Loss: 0.7359045743942261\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 451/937, Discriminator Loss: 0.6941871643066406, Generator Loss: 0.722569465637207\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 452/937, Discriminator Loss: 0.7009086906909943, Generator Loss: 0.6990859508514404\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 453/937, Discriminator Loss: 0.6836581528186798, Generator Loss: 0.7157570123672485\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 454/937, Discriminator Loss: 0.6885857880115509, Generator Loss: 0.7265030145645142\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 455/937, Discriminator Loss: 0.6808395981788635, Generator Loss: 0.7217574119567871\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 456/937, Discriminator Loss: 0.6865764856338501, Generator Loss: 0.7300843596458435\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 457/937, Discriminator Loss: 0.6887814104557037, Generator Loss: 0.7341635823249817\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 458/937, Discriminator Loss: 0.6801309287548065, Generator Loss: 0.7310574054718018\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 459/937, Discriminator Loss: 0.6826796531677246, Generator Loss: 0.7290726900100708\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 460/937, Discriminator Loss: 0.6889941394329071, Generator Loss: 0.7398110628128052\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 461/937, Discriminator Loss: 0.6744681298732758, Generator Loss: 0.7451303005218506\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 462/937, Discriminator Loss: 0.6917318403720856, Generator Loss: 0.7386471629142761\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 463/937, Discriminator Loss: 0.6968751549720764, Generator Loss: 0.7134860157966614\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 464/937, Discriminator Loss: 0.6873713433742523, Generator Loss: 0.7268611192703247\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 465/937, Discriminator Loss: 0.6944381892681122, Generator Loss: 0.7304301857948303\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 466/937, Discriminator Loss: 0.6883476972579956, Generator Loss: 0.7280993461608887\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 467/937, Discriminator Loss: 0.7035433948040009, Generator Loss: 0.7335271239280701\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 468/937, Discriminator Loss: 0.6785905361175537, Generator Loss: 0.7432191967964172\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 469/937, Discriminator Loss: 0.6791962087154388, Generator Loss: 0.7237709760665894\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 470/937, Discriminator Loss: 0.6692447066307068, Generator Loss: 0.7351484894752502\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 471/937, Discriminator Loss: 0.702669620513916, Generator Loss: 0.7272438406944275\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 472/937, Discriminator Loss: 0.6777134537696838, Generator Loss: 0.7252318859100342\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 473/937, Discriminator Loss: 0.6936373710632324, Generator Loss: 0.720621645450592\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 474/937, Discriminator Loss: 0.6917503774166107, Generator Loss: 0.7257959246635437\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 475/937, Discriminator Loss: 0.6803224086761475, Generator Loss: 0.7441573143005371\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 476/937, Discriminator Loss: 0.681933581829071, Generator Loss: 0.7429327964782715\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 477/937, Discriminator Loss: 0.6775920987129211, Generator Loss: 0.7443333864212036\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 478/937, Discriminator Loss: 0.6978472471237183, Generator Loss: 0.7354589700698853\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 479/937, Discriminator Loss: 0.6917014718055725, Generator Loss: 0.7242173552513123\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 480/937, Discriminator Loss: 0.6900028884410858, Generator Loss: 0.7402344942092896\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 481/937, Discriminator Loss: 0.6734324395656586, Generator Loss: 0.6999598145484924\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 482/937, Discriminator Loss: 0.6653141677379608, Generator Loss: 0.7293450832366943\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 483/937, Discriminator Loss: 0.6952646672725677, Generator Loss: 0.7436168193817139\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 484/937, Discriminator Loss: 0.6954013109207153, Generator Loss: 0.7309523224830627\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 485/937, Discriminator Loss: 0.7100754380226135, Generator Loss: 0.7671769857406616\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 486/937, Discriminator Loss: 0.696074366569519, Generator Loss: 0.7463386058807373\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 487/937, Discriminator Loss: 0.6898052096366882, Generator Loss: 0.7626194357872009\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 488/937, Discriminator Loss: 0.6872850060462952, Generator Loss: 0.7584169507026672\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 489/937, Discriminator Loss: 0.6915759742259979, Generator Loss: 0.732663094997406\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 490/937, Discriminator Loss: 0.6795385181903839, Generator Loss: 0.7222530841827393\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 491/937, Discriminator Loss: 0.6989538371562958, Generator Loss: 0.7270219326019287\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 492/937, Discriminator Loss: 0.7009135782718658, Generator Loss: 0.7275557518005371\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 493/937, Discriminator Loss: 0.6853808760643005, Generator Loss: 0.7233583927154541\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 494/937, Discriminator Loss: 0.6757569313049316, Generator Loss: 0.7079238891601562\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 495/937, Discriminator Loss: 0.6832927167415619, Generator Loss: 0.7248200178146362\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 496/937, Discriminator Loss: 0.6969162821769714, Generator Loss: 0.7180131673812866\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 497/937, Discriminator Loss: 0.697865754365921, Generator Loss: 0.7158985137939453\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 498/937, Discriminator Loss: 0.7073558270931244, Generator Loss: 0.7103103399276733\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 499/937, Discriminator Loss: 0.6928292512893677, Generator Loss: 0.7271625995635986\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 500/937, Discriminator Loss: 0.6853736042976379, Generator Loss: 0.7213653922080994\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 501/937, Discriminator Loss: 0.7049535810947418, Generator Loss: 0.7277432680130005\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 502/937, Discriminator Loss: 0.6936002373695374, Generator Loss: 0.716161847114563\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 503/937, Discriminator Loss: 0.6835730969905853, Generator Loss: 0.7234625220298767\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 504/937, Discriminator Loss: 0.6968019306659698, Generator Loss: 0.7237950563430786\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 505/937, Discriminator Loss: 0.6933997571468353, Generator Loss: 0.7156288027763367\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 506/937, Discriminator Loss: 0.7005329728126526, Generator Loss: 0.7358830571174622\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 507/937, Discriminator Loss: 0.6886783242225647, Generator Loss: 0.7208651304244995\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 508/937, Discriminator Loss: 0.6753785908222198, Generator Loss: 0.7142634391784668\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 509/937, Discriminator Loss: 0.6759729087352753, Generator Loss: 0.7248576879501343\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 510/937, Discriminator Loss: 0.6907580494880676, Generator Loss: 0.7141404151916504\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 511/937, Discriminator Loss: 0.6785855293273926, Generator Loss: 0.7269384860992432\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 512/937, Discriminator Loss: 0.6788805425167084, Generator Loss: 0.7225233316421509\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "Epoch: 9/10, Batch: 513/937, Discriminator Loss: 0.683217316865921, Generator Loss: 0.7118514776229858\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 514/937, Discriminator Loss: 0.6932346820831299, Generator Loss: 0.7266844511032104\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 515/937, Discriminator Loss: 0.6989993751049042, Generator Loss: 0.7211040258407593\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 516/937, Discriminator Loss: 0.7025415897369385, Generator Loss: 0.6908015012741089\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 517/937, Discriminator Loss: 0.673425018787384, Generator Loss: 0.7279943227767944\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 518/937, Discriminator Loss: 0.6763075888156891, Generator Loss: 0.728115975856781\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 519/937, Discriminator Loss: 0.6871124505996704, Generator Loss: 0.724791407585144\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 520/937, Discriminator Loss: 0.6793117821216583, Generator Loss: 0.7242243885993958\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 521/937, Discriminator Loss: 0.6898990869522095, Generator Loss: 0.7350636720657349\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 522/937, Discriminator Loss: 0.673929363489151, Generator Loss: 0.7249205112457275\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 523/937, Discriminator Loss: 0.6646774113178253, Generator Loss: 0.7436568140983582\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 524/937, Discriminator Loss: 0.6779331266880035, Generator Loss: 0.7613840103149414\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 525/937, Discriminator Loss: 0.7093336284160614, Generator Loss: 0.724040687084198\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 526/937, Discriminator Loss: 0.6952147483825684, Generator Loss: 0.7337002158164978\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 527/937, Discriminator Loss: 0.6878940463066101, Generator Loss: 0.7098702192306519\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 528/937, Discriminator Loss: 0.6940179467201233, Generator Loss: 0.745917558670044\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 529/937, Discriminator Loss: 0.6802336871623993, Generator Loss: 0.7329641580581665\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 530/937, Discriminator Loss: 0.7078436613082886, Generator Loss: 0.7249670624732971\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 531/937, Discriminator Loss: 0.7043329179286957, Generator Loss: 0.7032634019851685\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 532/937, Discriminator Loss: 0.6804560720920563, Generator Loss: 0.7248055934906006\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 533/937, Discriminator Loss: 0.6862693130970001, Generator Loss: 0.7124391794204712\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 534/937, Discriminator Loss: 0.6823883056640625, Generator Loss: 0.7300894260406494\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 535/937, Discriminator Loss: 0.6828063130378723, Generator Loss: 0.7295895218849182\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 536/937, Discriminator Loss: 0.6857771873474121, Generator Loss: 0.7430871725082397\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 537/937, Discriminator Loss: 0.6718735694885254, Generator Loss: 0.7406737804412842\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 538/937, Discriminator Loss: 0.7016213536262512, Generator Loss: 0.7006158828735352\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 539/937, Discriminator Loss: 0.6892783641815186, Generator Loss: 0.7509160041809082\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 540/937, Discriminator Loss: 0.6842792928218842, Generator Loss: 0.7411442399024963\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 541/937, Discriminator Loss: 0.6849337518215179, Generator Loss: 0.716248631477356\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 542/937, Discriminator Loss: 0.6837392449378967, Generator Loss: 0.7285473346710205\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 543/937, Discriminator Loss: 0.6748396456241608, Generator Loss: 0.7174649238586426\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 544/937, Discriminator Loss: 0.6765865087509155, Generator Loss: 0.7105365991592407\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 545/937, Discriminator Loss: 0.674641340970993, Generator Loss: 0.7248391509056091\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 546/937, Discriminator Loss: 0.6871030926704407, Generator Loss: 0.7171670198440552\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 547/937, Discriminator Loss: 0.6766163110733032, Generator Loss: 0.7299140691757202\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 548/937, Discriminator Loss: 0.6956694722175598, Generator Loss: 0.734904408454895\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 549/937, Discriminator Loss: 0.6642802655696869, Generator Loss: 0.7387129068374634\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 550/937, Discriminator Loss: 0.7016115188598633, Generator Loss: 0.7366880178451538\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 551/937, Discriminator Loss: 0.6713819801807404, Generator Loss: 0.743068277835846\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 552/937, Discriminator Loss: 0.6738667190074921, Generator Loss: 0.7350912690162659\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 553/937, Discriminator Loss: 0.6871500313282013, Generator Loss: 0.7312421202659607\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 554/937, Discriminator Loss: 0.688648521900177, Generator Loss: 0.7221120595932007\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 555/937, Discriminator Loss: 0.6999483704566956, Generator Loss: 0.7330746054649353\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 556/937, Discriminator Loss: 0.6786177456378937, Generator Loss: 0.7280868291854858\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 557/937, Discriminator Loss: 0.6687289476394653, Generator Loss: 0.719835102558136\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 558/937, Discriminator Loss: 0.701329916715622, Generator Loss: 0.7335059642791748\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 559/937, Discriminator Loss: 0.6938661336898804, Generator Loss: 0.711524486541748\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 560/937, Discriminator Loss: 0.6944345533847809, Generator Loss: 0.7234752178192139\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 561/937, Discriminator Loss: 0.6839415729045868, Generator Loss: 0.7232275009155273\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 562/937, Discriminator Loss: 0.6948822736740112, Generator Loss: 0.7104499340057373\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 563/937, Discriminator Loss: 0.6899377107620239, Generator Loss: 0.7198729515075684\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 564/937, Discriminator Loss: 0.6842970252037048, Generator Loss: 0.7219771146774292\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 565/937, Discriminator Loss: 0.6957182884216309, Generator Loss: 0.7146344184875488\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 566/937, Discriminator Loss: 0.6974551975727081, Generator Loss: 0.7156848311424255\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 567/937, Discriminator Loss: 0.6945122480392456, Generator Loss: 0.7384079694747925\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 568/937, Discriminator Loss: 0.6890583336353302, Generator Loss: 0.7101852893829346\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 569/937, Discriminator Loss: 0.6998069584369659, Generator Loss: 0.718285322189331\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 570/937, Discriminator Loss: 0.6814674735069275, Generator Loss: 0.7623621225357056\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 571/937, Discriminator Loss: 0.6771160960197449, Generator Loss: 0.7508659958839417\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 572/937, Discriminator Loss: 0.6875308454036713, Generator Loss: 0.7580110430717468\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 573/937, Discriminator Loss: 0.6676410436630249, Generator Loss: 0.7641664147377014\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 574/937, Discriminator Loss: 0.6854400932788849, Generator Loss: 0.7283724546432495\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 575/937, Discriminator Loss: 0.6818555593490601, Generator Loss: 0.706303596496582\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 576/937, Discriminator Loss: 0.6851862370967865, Generator Loss: 0.7236236929893494\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 577/937, Discriminator Loss: 0.6799158453941345, Generator Loss: 0.7244459390640259\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 578/937, Discriminator Loss: 0.6976853609085083, Generator Loss: 0.7267681360244751\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 579/937, Discriminator Loss: 0.6850831508636475, Generator Loss: 0.7197995781898499\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 580/937, Discriminator Loss: 0.688005268573761, Generator Loss: 0.7260072827339172\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 581/937, Discriminator Loss: 0.6812958419322968, Generator Loss: 0.7256538271903992\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 582/937, Discriminator Loss: 0.6880922317504883, Generator Loss: 0.722459614276886\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 583/937, Discriminator Loss: 0.6854832172393799, Generator Loss: 0.7105295658111572\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 584/937, Discriminator Loss: 0.6708991229534149, Generator Loss: 0.7259361743927002\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 585/937, Discriminator Loss: 0.6915768682956696, Generator Loss: 0.7133480906486511\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 586/937, Discriminator Loss: 0.6572222411632538, Generator Loss: 0.7103303670883179\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 587/937, Discriminator Loss: 0.7143223285675049, Generator Loss: 0.7352308034896851\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 588/937, Discriminator Loss: 0.6875373125076294, Generator Loss: 0.6941937208175659\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 589/937, Discriminator Loss: 0.684010237455368, Generator Loss: 0.7122071981430054\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 590/937, Discriminator Loss: 0.688691258430481, Generator Loss: 0.7188044786453247\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 591/937, Discriminator Loss: 0.6807310581207275, Generator Loss: 0.7264830470085144\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 592/937, Discriminator Loss: 0.687228262424469, Generator Loss: 0.717645525932312\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 593/937, Discriminator Loss: 0.6858245432376862, Generator Loss: 0.7502284646034241\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 594/937, Discriminator Loss: 0.6873958706855774, Generator Loss: 0.745782732963562\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 595/937, Discriminator Loss: 0.7000839710235596, Generator Loss: 0.7446342706680298\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 596/937, Discriminator Loss: 0.6880609095096588, Generator Loss: 0.7409533262252808\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 597/937, Discriminator Loss: 0.6996710002422333, Generator Loss: 0.7305869460105896\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 598/937, Discriminator Loss: 0.7028720080852509, Generator Loss: 0.7134584784507751\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 599/937, Discriminator Loss: 0.6874454021453857, Generator Loss: 0.731727123260498\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 600/937, Discriminator Loss: 0.6992382407188416, Generator Loss: 0.7242026329040527\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 601/937, Discriminator Loss: 0.7103777825832367, Generator Loss: 0.7246408462524414\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 602/937, Discriminator Loss: 0.678672581911087, Generator Loss: 0.7275493144989014\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 603/937, Discriminator Loss: 0.6946815252304077, Generator Loss: 0.7045981884002686\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 604/937, Discriminator Loss: 0.6847659349441528, Generator Loss: 0.7039902210235596\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 605/937, Discriminator Loss: 0.6804280281066895, Generator Loss: 0.7300814986228943\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 606/937, Discriminator Loss: 0.6829822659492493, Generator Loss: 0.7192193865776062\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 607/937, Discriminator Loss: 0.6909132301807404, Generator Loss: 0.7471017241477966\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 608/937, Discriminator Loss: 0.6859307289123535, Generator Loss: 0.7534583806991577\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 609/937, Discriminator Loss: 0.7048112154006958, Generator Loss: 0.7502765655517578\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 610/937, Discriminator Loss: 0.6811183094978333, Generator Loss: 0.7250216007232666\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 611/937, Discriminator Loss: 0.6996708810329437, Generator Loss: 0.7433607578277588\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 612/937, Discriminator Loss: 0.696890652179718, Generator Loss: 0.7781650424003601\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 613/937, Discriminator Loss: 0.7054045498371124, Generator Loss: 0.7492451667785645\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 614/937, Discriminator Loss: 0.6764247417449951, Generator Loss: 0.7741621732711792\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 615/937, Discriminator Loss: 0.6898967027664185, Generator Loss: 0.7710355520248413\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 616/937, Discriminator Loss: 0.7063733637332916, Generator Loss: 0.7410658597946167\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 617/937, Discriminator Loss: 0.6739760339260101, Generator Loss: 0.7200695872306824\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 9/10, Batch: 618/937, Discriminator Loss: 0.6829450726509094, Generator Loss: 0.732437789440155\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 619/937, Discriminator Loss: 0.6732172071933746, Generator Loss: 0.7331316471099854\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 620/937, Discriminator Loss: 0.6907348036766052, Generator Loss: 0.7425687313079834\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 621/937, Discriminator Loss: 0.7022251486778259, Generator Loss: 0.739436686038971\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 622/937, Discriminator Loss: 0.7064793109893799, Generator Loss: 0.7319215536117554\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 623/937, Discriminator Loss: 0.6716106235980988, Generator Loss: 0.7186174392700195\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 624/937, Discriminator Loss: 0.6869445145130157, Generator Loss: 0.703048050403595\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 625/937, Discriminator Loss: 0.6905618906021118, Generator Loss: 0.7170597314834595\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 626/937, Discriminator Loss: 0.6844466924667358, Generator Loss: 0.7162030935287476\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 627/937, Discriminator Loss: 0.6864977478981018, Generator Loss: 0.7153208255767822\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 628/937, Discriminator Loss: 0.6913984715938568, Generator Loss: 0.7251582145690918\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 629/937, Discriminator Loss: 0.6838285028934479, Generator Loss: 0.71806800365448\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 630/937, Discriminator Loss: 0.6831426024436951, Generator Loss: 0.746288001537323\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 631/937, Discriminator Loss: 0.6839316189289093, Generator Loss: 0.7601069808006287\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 632/937, Discriminator Loss: 0.692354142665863, Generator Loss: 0.7734534740447998\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 633/937, Discriminator Loss: 0.6895423233509064, Generator Loss: 0.7376967072486877\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 634/937, Discriminator Loss: 0.6759595274925232, Generator Loss: 0.7417194247245789\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 635/937, Discriminator Loss: 0.6998403072357178, Generator Loss: 0.7497767210006714\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 636/937, Discriminator Loss: 0.6783729493618011, Generator Loss: 0.7436071038246155\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 637/937, Discriminator Loss: 0.6937904357910156, Generator Loss: 0.7276831865310669\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 638/937, Discriminator Loss: 0.7043140530586243, Generator Loss: 0.7287964820861816\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 639/937, Discriminator Loss: 0.6768696904182434, Generator Loss: 0.7268219590187073\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 640/937, Discriminator Loss: 0.6955085694789886, Generator Loss: 0.7486209869384766\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 641/937, Discriminator Loss: 0.687551349401474, Generator Loss: 0.721507728099823\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 642/937, Discriminator Loss: 0.691053032875061, Generator Loss: 0.7597124576568604\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 643/937, Discriminator Loss: 0.6824244856834412, Generator Loss: 0.7383836507797241\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 644/937, Discriminator Loss: 0.6753076314926147, Generator Loss: 0.7392807602882385\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 645/937, Discriminator Loss: 0.7008398473262787, Generator Loss: 0.7645452618598938\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 646/937, Discriminator Loss: 0.6866051852703094, Generator Loss: 0.7402526140213013\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 647/937, Discriminator Loss: 0.692189484834671, Generator Loss: 0.7265674471855164\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 648/937, Discriminator Loss: 0.6874613165855408, Generator Loss: 0.7539807558059692\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 649/937, Discriminator Loss: 0.6965130865573883, Generator Loss: 0.7411165237426758\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 650/937, Discriminator Loss: 0.6850403547286987, Generator Loss: 0.7409241795539856\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 651/937, Discriminator Loss: 0.6887289583683014, Generator Loss: 0.7400243282318115\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 652/937, Discriminator Loss: 0.6810857951641083, Generator Loss: 0.7546736001968384\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 653/937, Discriminator Loss: 0.6680276095867157, Generator Loss: 0.73140549659729\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 654/937, Discriminator Loss: 0.6838041245937347, Generator Loss: 0.7218402624130249\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 655/937, Discriminator Loss: 0.6910499036312103, Generator Loss: 0.7316684126853943\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 656/937, Discriminator Loss: 0.6893980205059052, Generator Loss: 0.735694169998169\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 657/937, Discriminator Loss: 0.6932668387889862, Generator Loss: 0.7447086572647095\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 658/937, Discriminator Loss: 0.6735687851905823, Generator Loss: 0.7398083209991455\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 659/937, Discriminator Loss: 0.6927890777587891, Generator Loss: 0.7378833293914795\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 660/937, Discriminator Loss: 0.668607622385025, Generator Loss: 0.7057178020477295\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 661/937, Discriminator Loss: 0.6709874272346497, Generator Loss: 0.7135584354400635\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 662/937, Discriminator Loss: 0.69862100481987, Generator Loss: 0.7230627536773682\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 663/937, Discriminator Loss: 0.6614510416984558, Generator Loss: 0.7493125200271606\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 664/937, Discriminator Loss: 0.6966814398765564, Generator Loss: 0.7498656511306763\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 665/937, Discriminator Loss: 0.6807883381843567, Generator Loss: 0.7423707842826843\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 666/937, Discriminator Loss: 0.6710193455219269, Generator Loss: 0.7720848917961121\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 667/937, Discriminator Loss: 0.6876030266284943, Generator Loss: 0.7328314781188965\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 668/937, Discriminator Loss: 0.6883513033390045, Generator Loss: 0.7715096473693848\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 669/937, Discriminator Loss: 0.6854332685470581, Generator Loss: 0.7449696063995361\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 670/937, Discriminator Loss: 0.6772283017635345, Generator Loss: 0.7437280416488647\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 671/937, Discriminator Loss: 0.7031810581684113, Generator Loss: 0.7826275825500488\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 672/937, Discriminator Loss: 0.6719387471675873, Generator Loss: 0.8004337549209595\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 673/937, Discriminator Loss: 0.688523143529892, Generator Loss: 0.7445006370544434\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 674/937, Discriminator Loss: 0.6911649405956268, Generator Loss: 0.7333416938781738\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 675/937, Discriminator Loss: 0.678318202495575, Generator Loss: 0.7608035802841187\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 676/937, Discriminator Loss: 0.6860508322715759, Generator Loss: 0.7453300356864929\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 677/937, Discriminator Loss: 0.6683551073074341, Generator Loss: 0.7117246389389038\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 678/937, Discriminator Loss: 0.6980541348457336, Generator Loss: 0.6954163312911987\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 679/937, Discriminator Loss: 0.6777808368206024, Generator Loss: 0.7078206539154053\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 680/937, Discriminator Loss: 0.6855039596557617, Generator Loss: 0.7105002403259277\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 681/937, Discriminator Loss: 0.696010172367096, Generator Loss: 0.7159534692764282\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 682/937, Discriminator Loss: 0.6927972733974457, Generator Loss: 0.7146477699279785\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 683/937, Discriminator Loss: 0.6820641458034515, Generator Loss: 0.7434954047203064\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 684/937, Discriminator Loss: 0.6771133244037628, Generator Loss: 0.7354192733764648\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 685/937, Discriminator Loss: 0.6732277274131775, Generator Loss: 0.7541930675506592\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 686/937, Discriminator Loss: 0.6884602904319763, Generator Loss: 0.7303229570388794\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 687/937, Discriminator Loss: 0.7087356746196747, Generator Loss: 0.7188950777053833\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 688/937, Discriminator Loss: 0.7073764204978943, Generator Loss: 0.7478057146072388\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 689/937, Discriminator Loss: 0.6694218814373016, Generator Loss: 0.7510204315185547\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 690/937, Discriminator Loss: 0.6750088036060333, Generator Loss: 0.7298343181610107\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 691/937, Discriminator Loss: 0.6788427829742432, Generator Loss: 0.7214999794960022\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 692/937, Discriminator Loss: 0.6794009506702423, Generator Loss: 0.7527416348457336\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 693/937, Discriminator Loss: 0.6934643685817719, Generator Loss: 0.7557858228683472\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 694/937, Discriminator Loss: 0.6905212998390198, Generator Loss: 0.7433128356933594\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 695/937, Discriminator Loss: 0.6935093104839325, Generator Loss: 0.7488947510719299\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 696/937, Discriminator Loss: 0.689890444278717, Generator Loss: 0.7378754615783691\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 697/937, Discriminator Loss: 0.6967038512229919, Generator Loss: 0.7141913175582886\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 698/937, Discriminator Loss: 0.6917320489883423, Generator Loss: 0.7306934595108032\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 699/937, Discriminator Loss: 0.6796379089355469, Generator Loss: 0.7437220811843872\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 700/937, Discriminator Loss: 0.6816906929016113, Generator Loss: 0.7467938661575317\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 701/937, Discriminator Loss: 0.6941663026809692, Generator Loss: 0.7358942627906799\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 702/937, Discriminator Loss: 0.6832376420497894, Generator Loss: 0.7439713478088379\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 703/937, Discriminator Loss: 0.6773468255996704, Generator Loss: 0.737939178943634\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 704/937, Discriminator Loss: 0.678595781326294, Generator Loss: 0.741951584815979\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 705/937, Discriminator Loss: 0.6712996661663055, Generator Loss: 0.7263751029968262\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 706/937, Discriminator Loss: 0.6893932223320007, Generator Loss: 0.720367431640625\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 707/937, Discriminator Loss: 0.6951512396335602, Generator Loss: 0.7252038717269897\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 708/937, Discriminator Loss: 0.6950263977050781, Generator Loss: 0.7551003694534302\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 709/937, Discriminator Loss: 0.682876855134964, Generator Loss: 0.7522242069244385\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 710/937, Discriminator Loss: 0.6922717094421387, Generator Loss: 0.7343465089797974\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 711/937, Discriminator Loss: 0.6845193207263947, Generator Loss: 0.734878659248352\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 712/937, Discriminator Loss: 0.6797032952308655, Generator Loss: 0.7183599472045898\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 713/937, Discriminator Loss: 0.68021559715271, Generator Loss: 0.7301228642463684\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 714/937, Discriminator Loss: 0.689182847738266, Generator Loss: 0.720909833908081\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 715/937, Discriminator Loss: 0.6870676875114441, Generator Loss: 0.7155405879020691\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 716/937, Discriminator Loss: 0.7004079520702362, Generator Loss: 0.7087064981460571\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 717/937, Discriminator Loss: 0.6771447658538818, Generator Loss: 0.717590868473053\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 718/937, Discriminator Loss: 0.6796776056289673, Generator Loss: 0.7296108603477478\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 719/937, Discriminator Loss: 0.7058429419994354, Generator Loss: 0.7209226489067078\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 720/937, Discriminator Loss: 0.6722826063632965, Generator Loss: 0.7329913377761841\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 721/937, Discriminator Loss: 0.6922235488891602, Generator Loss: 0.7406836748123169\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 722/937, Discriminator Loss: 0.6868270933628082, Generator Loss: 0.742196798324585\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 723/937, Discriminator Loss: 0.6909455955028534, Generator Loss: 0.7341610193252563\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 724/937, Discriminator Loss: 0.6841514706611633, Generator Loss: 0.728044867515564\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 725/937, Discriminator Loss: 0.6906914710998535, Generator Loss: 0.7196460962295532\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 726/937, Discriminator Loss: 0.6955516934394836, Generator Loss: 0.715562641620636\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 727/937, Discriminator Loss: 0.6720871329307556, Generator Loss: 0.7512445449829102\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 728/937, Discriminator Loss: 0.6894613206386566, Generator Loss: 0.7708367109298706\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 729/937, Discriminator Loss: 0.6925783455371857, Generator Loss: 0.7727916240692139\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 730/937, Discriminator Loss: 0.7271844148635864, Generator Loss: 0.7112359404563904\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Epoch: 9/10, Batch: 731/937, Discriminator Loss: 0.6945472061634064, Generator Loss: 0.7113867998123169\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 732/937, Discriminator Loss: 0.690121054649353, Generator Loss: 0.7277859449386597\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 733/937, Discriminator Loss: 0.694768875837326, Generator Loss: 0.7360653877258301\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 734/937, Discriminator Loss: 0.6843727231025696, Generator Loss: 0.7267452478408813\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 735/937, Discriminator Loss: 0.6920264363288879, Generator Loss: 0.7178909778594971\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 736/937, Discriminator Loss: 0.7003404200077057, Generator Loss: 0.7277398705482483\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 737/937, Discriminator Loss: 0.6862620413303375, Generator Loss: 0.7416869401931763\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 738/937, Discriminator Loss: 0.6981574892997742, Generator Loss: 0.7197892665863037\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 739/937, Discriminator Loss: 0.6716766059398651, Generator Loss: 0.7359473705291748\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 740/937, Discriminator Loss: 0.6821745038032532, Generator Loss: 0.731968104839325\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 741/937, Discriminator Loss: 0.678759753704071, Generator Loss: 0.7313810586929321\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 742/937, Discriminator Loss: 0.6735865473747253, Generator Loss: 0.727756142616272\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 743/937, Discriminator Loss: 0.6901445388793945, Generator Loss: 0.7202489376068115\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 744/937, Discriminator Loss: 0.7017632126808167, Generator Loss: 0.7548401355743408\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 745/937, Discriminator Loss: 0.6879944503307343, Generator Loss: 0.7321200370788574\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 746/937, Discriminator Loss: 0.6823444366455078, Generator Loss: 0.7368876934051514\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 747/937, Discriminator Loss: 0.6774442195892334, Generator Loss: 0.7578531503677368\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 748/937, Discriminator Loss: 0.6668364405632019, Generator Loss: 0.7503631114959717\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 749/937, Discriminator Loss: 0.6854594945907593, Generator Loss: 0.7346023321151733\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 750/937, Discriminator Loss: 0.6999501585960388, Generator Loss: 0.7211611270904541\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 751/937, Discriminator Loss: 0.6637753248214722, Generator Loss: 0.7261638641357422\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 752/937, Discriminator Loss: 0.701036125421524, Generator Loss: 0.742678165435791\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 753/937, Discriminator Loss: 0.6804707050323486, Generator Loss: 0.7243026494979858\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 754/937, Discriminator Loss: 0.6838808655738831, Generator Loss: 0.7349571585655212\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 755/937, Discriminator Loss: 0.6869645714759827, Generator Loss: 0.7054497003555298\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 756/937, Discriminator Loss: 0.682184487581253, Generator Loss: 0.7290372848510742\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 757/937, Discriminator Loss: 0.6737082898616791, Generator Loss: 0.7025517225265503\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 758/937, Discriminator Loss: 0.6922014355659485, Generator Loss: 0.7217448353767395\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 759/937, Discriminator Loss: 0.6956962645053864, Generator Loss: 0.7375147342681885\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 760/937, Discriminator Loss: 0.6948462128639221, Generator Loss: 0.7250034809112549\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 761/937, Discriminator Loss: 0.6863734722137451, Generator Loss: 0.7153537273406982\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 762/937, Discriminator Loss: 0.6854434609413147, Generator Loss: 0.722217321395874\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 763/937, Discriminator Loss: 0.6894871592521667, Generator Loss: 0.7013713121414185\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 764/937, Discriminator Loss: 0.6789953708648682, Generator Loss: 0.718876302242279\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 765/937, Discriminator Loss: 0.6840142607688904, Generator Loss: 0.7491376399993896\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 766/937, Discriminator Loss: 0.6881695091724396, Generator Loss: 0.7365922927856445\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 767/937, Discriminator Loss: 0.698553204536438, Generator Loss: 0.736259937286377\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 768/937, Discriminator Loss: 0.7031485438346863, Generator Loss: 0.7301985025405884\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 769/937, Discriminator Loss: 0.708426296710968, Generator Loss: 0.732816219329834\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 770/937, Discriminator Loss: 0.7008454203605652, Generator Loss: 0.7253648042678833\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 771/937, Discriminator Loss: 0.7122412919998169, Generator Loss: 0.7470210790634155\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 772/937, Discriminator Loss: 0.7018838226795197, Generator Loss: 0.7345448732376099\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 773/937, Discriminator Loss: 0.6863439381122589, Generator Loss: 0.736690878868103\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 774/937, Discriminator Loss: 0.6857918500900269, Generator Loss: 0.7300863862037659\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 775/937, Discriminator Loss: 0.6873528957366943, Generator Loss: 0.7206066846847534\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 776/937, Discriminator Loss: 0.6933406889438629, Generator Loss: 0.7351462244987488\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 777/937, Discriminator Loss: 0.6828985810279846, Generator Loss: 0.7284295558929443\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 778/937, Discriminator Loss: 0.6871679127216339, Generator Loss: 0.7502795457839966\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 779/937, Discriminator Loss: 0.7024761140346527, Generator Loss: 0.7327079176902771\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 780/937, Discriminator Loss: 0.6761106252670288, Generator Loss: 0.7269411087036133\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 781/937, Discriminator Loss: 0.7084803283214569, Generator Loss: 0.7525944709777832\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 782/937, Discriminator Loss: 0.6769360303878784, Generator Loss: 0.7222535610198975\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 783/937, Discriminator Loss: 0.6838194131851196, Generator Loss: 0.7305821776390076\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 784/937, Discriminator Loss: 0.685157984495163, Generator Loss: 0.714104413986206\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 785/937, Discriminator Loss: 0.6936536729335785, Generator Loss: 0.7234327793121338\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 786/937, Discriminator Loss: 0.6976363062858582, Generator Loss: 0.7180017232894897\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 787/937, Discriminator Loss: 0.6933974921703339, Generator Loss: 0.7380527257919312\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 788/937, Discriminator Loss: 0.6868065893650055, Generator Loss: 0.7250330448150635\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 789/937, Discriminator Loss: 0.6749779582023621, Generator Loss: 0.7099128365516663\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 790/937, Discriminator Loss: 0.6878289580345154, Generator Loss: 0.7335160374641418\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 791/937, Discriminator Loss: 0.6987162232398987, Generator Loss: 0.7355810403823853\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 792/937, Discriminator Loss: 0.6897954046726227, Generator Loss: 0.729449987411499\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 793/937, Discriminator Loss: 0.6886871755123138, Generator Loss: 0.7181952595710754\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 794/937, Discriminator Loss: 0.6877474784851074, Generator Loss: 0.7306704521179199\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 795/937, Discriminator Loss: 0.6764353215694427, Generator Loss: 0.733029842376709\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 796/937, Discriminator Loss: 0.6746771037578583, Generator Loss: 0.7373909950256348\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 797/937, Discriminator Loss: 0.672121524810791, Generator Loss: 0.7252974510192871\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 798/937, Discriminator Loss: 0.6951566338539124, Generator Loss: 0.7176768779754639\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 799/937, Discriminator Loss: 0.7002575397491455, Generator Loss: 0.7254786491394043\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 9/10, Batch: 800/937, Discriminator Loss: 0.6881049573421478, Generator Loss: 0.7088959217071533\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 801/937, Discriminator Loss: 0.7278445959091187, Generator Loss: 0.7301416397094727\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 802/937, Discriminator Loss: 0.6793752312660217, Generator Loss: 0.7624196410179138\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 803/937, Discriminator Loss: 0.6753220856189728, Generator Loss: 0.7318406701087952\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 804/937, Discriminator Loss: 0.6725977957248688, Generator Loss: 0.7805570363998413\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 805/937, Discriminator Loss: 0.6657245457172394, Generator Loss: 0.7202330827713013\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 806/937, Discriminator Loss: 0.6833180785179138, Generator Loss: 0.7126569747924805\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 807/937, Discriminator Loss: 0.7061593234539032, Generator Loss: 0.7279307842254639\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 808/937, Discriminator Loss: 0.6969112157821655, Generator Loss: 0.7296898365020752\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 809/937, Discriminator Loss: 0.7019229233264923, Generator Loss: 0.7226539254188538\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 810/937, Discriminator Loss: 0.696357935667038, Generator Loss: 0.7264928221702576\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 811/937, Discriminator Loss: 0.6908693909645081, Generator Loss: 0.7211990356445312\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 812/937, Discriminator Loss: 0.6921255886554718, Generator Loss: 0.7166285514831543\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 813/937, Discriminator Loss: 0.6921033561229706, Generator Loss: 0.7194836735725403\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 814/937, Discriminator Loss: 0.6882451176643372, Generator Loss: 0.7187697291374207\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 815/937, Discriminator Loss: 0.6977296471595764, Generator Loss: 0.7141691446304321\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 816/937, Discriminator Loss: 0.6934689879417419, Generator Loss: 0.7214257717132568\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 817/937, Discriminator Loss: 0.69186070561409, Generator Loss: 0.7087869644165039\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 818/937, Discriminator Loss: 0.6913469433784485, Generator Loss: 0.7300708293914795\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 819/937, Discriminator Loss: 0.7039479911327362, Generator Loss: 0.7155333757400513\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 820/937, Discriminator Loss: 0.6946906447410583, Generator Loss: 0.7226837277412415\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 821/937, Discriminator Loss: 0.6798203885555267, Generator Loss: 0.7312427759170532\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 822/937, Discriminator Loss: 0.6842980086803436, Generator Loss: 0.7386558651924133\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 823/937, Discriminator Loss: 0.6927369832992554, Generator Loss: 0.7329974174499512\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 824/937, Discriminator Loss: 0.6914925575256348, Generator Loss: 0.735739529132843\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 825/937, Discriminator Loss: 0.6903739273548126, Generator Loss: 0.7394403219223022\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 9/10, Batch: 826/937, Discriminator Loss: 0.6932264864444733, Generator Loss: 0.7363814115524292\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 827/937, Discriminator Loss: 0.6885213851928711, Generator Loss: 0.7080239653587341\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 828/937, Discriminator Loss: 0.7062889635562897, Generator Loss: 0.7338886260986328\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 829/937, Discriminator Loss: 0.6784371733665466, Generator Loss: 0.7229806184768677\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 830/937, Discriminator Loss: 0.6775314211845398, Generator Loss: 0.6912937164306641\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 831/937, Discriminator Loss: 0.7054857015609741, Generator Loss: 0.7223583459854126\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 832/937, Discriminator Loss: 0.6951359510421753, Generator Loss: 0.7182905673980713\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 833/937, Discriminator Loss: 0.6823535859584808, Generator Loss: 0.7125657796859741\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 834/937, Discriminator Loss: 0.6928281486034393, Generator Loss: 0.7239903807640076\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 835/937, Discriminator Loss: 0.6969605684280396, Generator Loss: 0.7190863490104675\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 836/937, Discriminator Loss: 0.669863373041153, Generator Loss: 0.718374490737915\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 837/937, Discriminator Loss: 0.7016648650169373, Generator Loss: 0.721271276473999\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 838/937, Discriminator Loss: 0.6661728024482727, Generator Loss: 0.694473922252655\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 839/937, Discriminator Loss: 0.6946673095226288, Generator Loss: 0.7427547574043274\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 840/937, Discriminator Loss: 0.6923723518848419, Generator Loss: 0.7416210770606995\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 841/937, Discriminator Loss: 0.7004176676273346, Generator Loss: 0.7353870868682861\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 842/937, Discriminator Loss: 0.7014838755130768, Generator Loss: 0.7284685373306274\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 843/937, Discriminator Loss: 0.6976625919342041, Generator Loss: 0.7131332755088806\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 844/937, Discriminator Loss: 0.693085253238678, Generator Loss: 0.7179242372512817\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 845/937, Discriminator Loss: 0.7007079720497131, Generator Loss: 0.7226920127868652\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 846/937, Discriminator Loss: 0.6973000764846802, Generator Loss: 0.7193127870559692\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 9/10, Batch: 847/937, Discriminator Loss: 0.7021068632602692, Generator Loss: 0.7268802523612976\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 848/937, Discriminator Loss: 0.684003084897995, Generator Loss: 0.7258620858192444\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 849/937, Discriminator Loss: 0.681465744972229, Generator Loss: 0.7232513427734375\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 850/937, Discriminator Loss: 0.6833382546901703, Generator Loss: 0.7168404459953308\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 851/937, Discriminator Loss: 0.696825236082077, Generator Loss: 0.7054083347320557\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 852/937, Discriminator Loss: 0.6874035894870758, Generator Loss: 0.7070615291595459\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 853/937, Discriminator Loss: 0.7131242156028748, Generator Loss: 0.7272557616233826\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 854/937, Discriminator Loss: 0.7028895318508148, Generator Loss: 0.7327943444252014\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 855/937, Discriminator Loss: 0.6974788010120392, Generator Loss: 0.7326781749725342\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 856/937, Discriminator Loss: 0.6870971024036407, Generator Loss: 0.7478630542755127\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 857/937, Discriminator Loss: 0.6756910979747772, Generator Loss: 0.7321557998657227\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 858/937, Discriminator Loss: 0.6974146962165833, Generator Loss: 0.7429347038269043\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 859/937, Discriminator Loss: 0.6797747910022736, Generator Loss: 0.7448303699493408\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 860/937, Discriminator Loss: 0.7040189504623413, Generator Loss: 0.7409816980361938\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 861/937, Discriminator Loss: 0.6901443600654602, Generator Loss: 0.7430351376533508\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 862/937, Discriminator Loss: 0.7049357891082764, Generator Loss: 0.7319265604019165\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 863/937, Discriminator Loss: 0.6956793665885925, Generator Loss: 0.7503594160079956\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 9/10, Batch: 864/937, Discriminator Loss: 0.6908745765686035, Generator Loss: 0.7258981466293335\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 865/937, Discriminator Loss: 0.691851794719696, Generator Loss: 0.7222669124603271\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 866/937, Discriminator Loss: 0.6877017319202423, Generator Loss: 0.7406065464019775\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 867/937, Discriminator Loss: 0.7022324800491333, Generator Loss: 0.7353242635726929\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Epoch: 9/10, Batch: 868/937, Discriminator Loss: 0.6954786777496338, Generator Loss: 0.7292895317077637\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 869/937, Discriminator Loss: 0.6975614726543427, Generator Loss: 0.7186177968978882\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 870/937, Discriminator Loss: 0.6937967836856842, Generator Loss: 0.7221900224685669\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 871/937, Discriminator Loss: 0.6908951699733734, Generator Loss: 0.7240703105926514\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 872/937, Discriminator Loss: 0.6838406920433044, Generator Loss: 0.7143019437789917\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 873/937, Discriminator Loss: 0.7017865478992462, Generator Loss: 0.7250158786773682\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 874/937, Discriminator Loss: 0.6826582252979279, Generator Loss: 0.7160345315933228\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 875/937, Discriminator Loss: 0.6935692727565765, Generator Loss: 0.7101958990097046\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 876/937, Discriminator Loss: 0.6926778852939606, Generator Loss: 0.7153424024581909\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 9/10, Batch: 877/937, Discriminator Loss: 0.7011398673057556, Generator Loss: 0.7249789237976074\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 878/937, Discriminator Loss: 0.6886954605579376, Generator Loss: 0.7260443568229675\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 879/937, Discriminator Loss: 0.6902782917022705, Generator Loss: 0.7145606279373169\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 880/937, Discriminator Loss: 0.6809589862823486, Generator Loss: 0.7043015956878662\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 881/937, Discriminator Loss: 0.6878005862236023, Generator Loss: 0.7116237878799438\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 882/937, Discriminator Loss: 0.6835035085678101, Generator Loss: 0.716693103313446\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 883/937, Discriminator Loss: 0.6952242255210876, Generator Loss: 0.7168883085250854\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 884/937, Discriminator Loss: 0.6914100348949432, Generator Loss: 0.7247098684310913\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 885/937, Discriminator Loss: 0.6943208873271942, Generator Loss: 0.7139149308204651\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 886/937, Discriminator Loss: 0.6956709325313568, Generator Loss: 0.7297157049179077\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 887/937, Discriminator Loss: 0.6946778893470764, Generator Loss: 0.7198116183280945\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 888/937, Discriminator Loss: 0.6863288581371307, Generator Loss: 0.7089606523513794\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 889/937, Discriminator Loss: 0.7089639008045197, Generator Loss: 0.7057567834854126\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 890/937, Discriminator Loss: 0.692418783903122, Generator Loss: 0.7096632719039917\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 891/937, Discriminator Loss: 0.6736279129981995, Generator Loss: 0.7384114861488342\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 892/937, Discriminator Loss: 0.6873985528945923, Generator Loss: 0.7122380137443542\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 893/937, Discriminator Loss: 0.6920251846313477, Generator Loss: 0.706439197063446\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 894/937, Discriminator Loss: 0.686724066734314, Generator Loss: 0.7167919874191284\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 895/937, Discriminator Loss: 0.6873150765895844, Generator Loss: 0.7274847030639648\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 896/937, Discriminator Loss: 0.7000071406364441, Generator Loss: 0.7149988412857056\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 9/10, Batch: 897/937, Discriminator Loss: 0.7093221545219421, Generator Loss: 0.7284024953842163\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 898/937, Discriminator Loss: 0.6951905786991119, Generator Loss: 0.7381178736686707\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 899/937, Discriminator Loss: 0.6871665716171265, Generator Loss: 0.7317559719085693\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 900/937, Discriminator Loss: 0.6836851239204407, Generator Loss: 0.7309424877166748\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 901/937, Discriminator Loss: 0.697064220905304, Generator Loss: 0.7110077142715454\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 902/937, Discriminator Loss: 0.6861562430858612, Generator Loss: 0.7219408750534058\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 903/937, Discriminator Loss: 0.690817654132843, Generator Loss: 0.7220538258552551\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 904/937, Discriminator Loss: 0.6728471517562866, Generator Loss: 0.7165736556053162\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 9/10, Batch: 905/937, Discriminator Loss: 0.6874260306358337, Generator Loss: 0.7293437719345093\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 906/937, Discriminator Loss: 0.6909931302070618, Generator Loss: 0.7250157594680786\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 9/10, Batch: 907/937, Discriminator Loss: 0.7092804908752441, Generator Loss: 0.7204874157905579\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 908/937, Discriminator Loss: 0.6892525851726532, Generator Loss: 0.7308543920516968\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 909/937, Discriminator Loss: 0.695840984582901, Generator Loss: 0.7409216165542603\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 910/937, Discriminator Loss: 0.6892004609107971, Generator Loss: 0.7168681621551514\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 911/937, Discriminator Loss: 0.6863457560539246, Generator Loss: 0.7316198945045471\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 912/937, Discriminator Loss: 0.6768150329589844, Generator Loss: 0.7223904132843018\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 913/937, Discriminator Loss: 0.6829830408096313, Generator Loss: 0.7188973426818848\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 914/937, Discriminator Loss: 0.6915421485900879, Generator Loss: 0.7154841423034668\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 915/937, Discriminator Loss: 0.6920506060123444, Generator Loss: 0.7123193144798279\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 916/937, Discriminator Loss: 0.6937453746795654, Generator Loss: 0.7146884202957153\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 917/937, Discriminator Loss: 0.6988140046596527, Generator Loss: 0.7250616550445557\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 918/937, Discriminator Loss: 0.6804321706295013, Generator Loss: 0.7274248600006104\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 919/937, Discriminator Loss: 0.6931636929512024, Generator Loss: 0.7213847637176514\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 920/937, Discriminator Loss: 0.6943699419498444, Generator Loss: 0.7440213561058044\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 921/937, Discriminator Loss: 0.6813226342201233, Generator Loss: 0.7307475209236145\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 922/937, Discriminator Loss: 0.6920992732048035, Generator Loss: 0.7283490300178528\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 923/937, Discriminator Loss: 0.6900521516799927, Generator Loss: 0.7203949689865112\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 924/937, Discriminator Loss: 0.689560055732727, Generator Loss: 0.7356424331665039\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 925/937, Discriminator Loss: 0.6877307891845703, Generator Loss: 0.7622994184494019\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 9/10, Batch: 926/937, Discriminator Loss: 0.7015213370323181, Generator Loss: 0.7276971936225891\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 927/937, Discriminator Loss: 0.6858357191085815, Generator Loss: 0.7115914225578308\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 928/937, Discriminator Loss: 0.6875857710838318, Generator Loss: 0.7097516059875488\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 929/937, Discriminator Loss: 0.6879230439662933, Generator Loss: 0.7205539345741272\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 9/10, Batch: 930/937, Discriminator Loss: 0.7050535976886749, Generator Loss: 0.7205023765563965\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 9/10, Batch: 931/937, Discriminator Loss: 0.693983793258667, Generator Loss: 0.7129294872283936\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 932/937, Discriminator Loss: 0.6861957311630249, Generator Loss: 0.7146636247634888\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 9/10, Batch: 933/937, Discriminator Loss: 0.7055484056472778, Generator Loss: 0.7268432974815369\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 934/937, Discriminator Loss: 0.6878759562969208, Generator Loss: 0.7301037907600403\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 9/10, Batch: 935/937, Discriminator Loss: 0.6814536154270172, Generator Loss: 0.7239677906036377\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 936/937, Discriminator Loss: 0.6969305872917175, Generator Loss: 0.7303948402404785\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 9/10, Batch: 937/937, Discriminator Loss: 0.6774387061595917, Generator Loss: 0.7169492244720459\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 1/937, Discriminator Loss: 0.6841645836830139, Generator Loss: 0.711117148399353\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 2/937, Discriminator Loss: 0.6921057105064392, Generator Loss: 0.7078804969787598\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 3/937, Discriminator Loss: 0.6931374967098236, Generator Loss: 0.7068580389022827\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 4/937, Discriminator Loss: 0.6786185801029205, Generator Loss: 0.7435964941978455\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 5/937, Discriminator Loss: 0.6784767806529999, Generator Loss: 0.7256293296813965\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 6/937, Discriminator Loss: 0.697416216135025, Generator Loss: 0.7099298238754272\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 7/937, Discriminator Loss: 0.7223183512687683, Generator Loss: 0.7270669937133789\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 8/937, Discriminator Loss: 0.6803072690963745, Generator Loss: 0.7128795385360718\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 9/937, Discriminator Loss: 0.6854354739189148, Generator Loss: 0.7340056896209717\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 10/937, Discriminator Loss: 0.682742565870285, Generator Loss: 0.7575076818466187\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 11/937, Discriminator Loss: 0.6944319009780884, Generator Loss: 0.7352652549743652\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 12/937, Discriminator Loss: 0.7008125185966492, Generator Loss: 0.7247890830039978\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Epoch: 10/10, Batch: 13/937, Discriminator Loss: 0.6855168342590332, Generator Loss: 0.7289880514144897\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 14/937, Discriminator Loss: 0.6967337429523468, Generator Loss: 0.7235151529312134\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 15/937, Discriminator Loss: 0.683040976524353, Generator Loss: 0.72125244140625\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 16/937, Discriminator Loss: 0.6925514340400696, Generator Loss: 0.7286409735679626\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 17/937, Discriminator Loss: 0.6944377422332764, Generator Loss: 0.7356499433517456\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 18/937, Discriminator Loss: 0.6964824199676514, Generator Loss: 0.7184654474258423\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 19/937, Discriminator Loss: 0.6866486966609955, Generator Loss: 0.7126197814941406\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 20/937, Discriminator Loss: 0.6926435828208923, Generator Loss: 0.7133693695068359\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 21/937, Discriminator Loss: 0.6875053644180298, Generator Loss: 0.7107448577880859\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 22/937, Discriminator Loss: 0.6900778412818909, Generator Loss: 0.7065317630767822\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 23/937, Discriminator Loss: 0.7036626636981964, Generator Loss: 0.7154266834259033\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 24/937, Discriminator Loss: 0.6858006417751312, Generator Loss: 0.7220171689987183\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 25/937, Discriminator Loss: 0.6847425699234009, Generator Loss: 0.7099541425704956\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 26/937, Discriminator Loss: 0.6916005909442902, Generator Loss: 0.7084515690803528\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 27/937, Discriminator Loss: 0.6931827664375305, Generator Loss: 0.7188997268676758\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 28/937, Discriminator Loss: 0.6918751299381256, Generator Loss: 0.7355204820632935\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 29/937, Discriminator Loss: 0.68633833527565, Generator Loss: 0.7189722061157227\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 30/937, Discriminator Loss: 0.6987358331680298, Generator Loss: 0.7155605554580688\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 31/937, Discriminator Loss: 0.695799857378006, Generator Loss: 0.7281428575515747\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 32/937, Discriminator Loss: 0.6853552758693695, Generator Loss: 0.7218926548957825\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 33/937, Discriminator Loss: 0.6873056888580322, Generator Loss: 0.7083317041397095\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 34/937, Discriminator Loss: 0.6766578555107117, Generator Loss: 0.7103654146194458\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 35/937, Discriminator Loss: 0.6940510272979736, Generator Loss: 0.7152435183525085\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 36/937, Discriminator Loss: 0.6957861185073853, Generator Loss: 0.71446692943573\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 37/937, Discriminator Loss: 0.680325448513031, Generator Loss: 0.7157641649246216\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 38/937, Discriminator Loss: 0.6847656071186066, Generator Loss: 0.7143684029579163\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 39/937, Discriminator Loss: 0.6972101032733917, Generator Loss: 0.7159334421157837\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 40/937, Discriminator Loss: 0.6803922653198242, Generator Loss: 0.7204462289810181\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 41/937, Discriminator Loss: 0.6988794803619385, Generator Loss: 0.7200791835784912\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 42/937, Discriminator Loss: 0.6871581077575684, Generator Loss: 0.7275963425636292\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 43/937, Discriminator Loss: 0.6855572164058685, Generator Loss: 0.7216647863388062\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 44/937, Discriminator Loss: 0.6852511167526245, Generator Loss: 0.7289380431175232\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 45/937, Discriminator Loss: 0.6888735592365265, Generator Loss: 0.721992552280426\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 46/937, Discriminator Loss: 0.6920120716094971, Generator Loss: 0.7274444103240967\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 47/937, Discriminator Loss: 0.6802566051483154, Generator Loss: 0.7343710660934448\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 48/937, Discriminator Loss: 0.6875960826873779, Generator Loss: 0.7366325855255127\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 49/937, Discriminator Loss: 0.6825456917285919, Generator Loss: 0.751497745513916\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 50/937, Discriminator Loss: 0.7000413239002228, Generator Loss: 0.7310881614685059\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 51/937, Discriminator Loss: 0.6902065277099609, Generator Loss: 0.7410204410552979\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 52/937, Discriminator Loss: 0.6834891438484192, Generator Loss: 0.7381885051727295\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 53/937, Discriminator Loss: 0.6899333596229553, Generator Loss: 0.729877233505249\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 54/937, Discriminator Loss: 0.7122837603092194, Generator Loss: 0.7449346780776978\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 55/937, Discriminator Loss: 0.6919645667076111, Generator Loss: 0.7241427898406982\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 56/937, Discriminator Loss: 0.6873612701892853, Generator Loss: 0.7333664298057556\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 57/937, Discriminator Loss: 0.7021478414535522, Generator Loss: 0.7374355792999268\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 58/937, Discriminator Loss: 0.6917330920696259, Generator Loss: 0.7231183052062988\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 59/937, Discriminator Loss: 0.6900945007801056, Generator Loss: 0.7270646691322327\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 60/937, Discriminator Loss: 0.6916104555130005, Generator Loss: 0.7266379594802856\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 61/937, Discriminator Loss: 0.691631406545639, Generator Loss: 0.7245712876319885\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 62/937, Discriminator Loss: 0.6988561153411865, Generator Loss: 0.7205227017402649\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 63/937, Discriminator Loss: 0.6799483299255371, Generator Loss: 0.7264603972434998\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 64/937, Discriminator Loss: 0.6829566359519958, Generator Loss: 0.7362242937088013\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 65/937, Discriminator Loss: 0.6881408989429474, Generator Loss: 0.7284001111984253\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 66/937, Discriminator Loss: 0.6908525824546814, Generator Loss: 0.7342251539230347\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 67/937, Discriminator Loss: 0.6941434741020203, Generator Loss: 0.714516282081604\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 68/937, Discriminator Loss: 0.6938818991184235, Generator Loss: 0.7173109650611877\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 69/937, Discriminator Loss: 0.6936743855476379, Generator Loss: 0.7050577402114868\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 70/937, Discriminator Loss: 0.6776357889175415, Generator Loss: 0.7099171876907349\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 71/937, Discriminator Loss: 0.6788238286972046, Generator Loss: 0.7002116441726685\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 72/937, Discriminator Loss: 0.6954720318317413, Generator Loss: 0.7137775421142578\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 73/937, Discriminator Loss: 0.6938519477844238, Generator Loss: 0.7058248519897461\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 74/937, Discriminator Loss: 0.6844769716262817, Generator Loss: 0.7180420160293579\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 75/937, Discriminator Loss: 0.6909424662590027, Generator Loss: 0.7108820676803589\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 76/937, Discriminator Loss: 0.6851109862327576, Generator Loss: 0.7168688178062439\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 77/937, Discriminator Loss: 0.6937907040119171, Generator Loss: 0.7056850790977478\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 78/937, Discriminator Loss: 0.6860283613204956, Generator Loss: 0.6929007768630981\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 79/937, Discriminator Loss: 0.6942457854747772, Generator Loss: 0.7046540975570679\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 80/937, Discriminator Loss: 0.68495774269104, Generator Loss: 0.699859082698822\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 81/937, Discriminator Loss: 0.7134919166564941, Generator Loss: 0.698622465133667\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 82/937, Discriminator Loss: 0.6896636486053467, Generator Loss: 0.707152783870697\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 83/937, Discriminator Loss: 0.6828937530517578, Generator Loss: 0.7051389217376709\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 84/937, Discriminator Loss: 0.6961864829063416, Generator Loss: 0.7002112865447998\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 85/937, Discriminator Loss: 0.6959221959114075, Generator Loss: 0.6893253922462463\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 86/937, Discriminator Loss: 0.7110261619091034, Generator Loss: 0.7066873908042908\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 87/937, Discriminator Loss: 0.6910196542739868, Generator Loss: 0.703973650932312\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 88/937, Discriminator Loss: 0.6895423531532288, Generator Loss: 0.7155984044075012\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 89/937, Discriminator Loss: 0.6882225871086121, Generator Loss: 0.7189552783966064\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 90/937, Discriminator Loss: 0.6860906779766083, Generator Loss: 0.7176065444946289\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 91/937, Discriminator Loss: 0.6937633156776428, Generator Loss: 0.7100566625595093\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 92/937, Discriminator Loss: 0.6893133521080017, Generator Loss: 0.7135273814201355\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Epoch: 10/10, Batch: 93/937, Discriminator Loss: 0.6950996220111847, Generator Loss: 0.7252165675163269\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 94/937, Discriminator Loss: 0.6917917132377625, Generator Loss: 0.7052779197692871\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 95/937, Discriminator Loss: 0.6906298995018005, Generator Loss: 0.7190523147583008\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 96/937, Discriminator Loss: 0.6988436579704285, Generator Loss: 0.7303856611251831\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 97/937, Discriminator Loss: 0.6891093552112579, Generator Loss: 0.7181395292282104\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 98/937, Discriminator Loss: 0.6829786002635956, Generator Loss: 0.7125896215438843\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 99/937, Discriminator Loss: 0.6981287896633148, Generator Loss: 0.7281350493431091\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 10/10, Batch: 100/937, Discriminator Loss: 0.697237104177475, Generator Loss: 0.7198469638824463\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 101/937, Discriminator Loss: 0.6917296051979065, Generator Loss: 0.7089859843254089\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 102/937, Discriminator Loss: 0.6762795448303223, Generator Loss: 0.7063978910446167\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 103/937, Discriminator Loss: 0.6876508593559265, Generator Loss: 0.6994861364364624\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 104/937, Discriminator Loss: 0.706797182559967, Generator Loss: 0.7081199884414673\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 105/937, Discriminator Loss: 0.6872934401035309, Generator Loss: 0.712368905544281\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 106/937, Discriminator Loss: 0.6906173229217529, Generator Loss: 0.717657208442688\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 107/937, Discriminator Loss: 0.6940482258796692, Generator Loss: 0.7342512011528015\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 108/937, Discriminator Loss: 0.6924155652523041, Generator Loss: 0.7120776772499084\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 109/937, Discriminator Loss: 0.6894170939922333, Generator Loss: 0.7232797145843506\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 110/937, Discriminator Loss: 0.6884245276451111, Generator Loss: 0.7562522888183594\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 111/937, Discriminator Loss: 0.700050562620163, Generator Loss: 0.7323513627052307\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 112/937, Discriminator Loss: 0.6961692273616791, Generator Loss: 0.71965491771698\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 113/937, Discriminator Loss: 0.6928980648517609, Generator Loss: 0.7136155366897583\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 114/937, Discriminator Loss: 0.6852707862854004, Generator Loss: 0.7193261384963989\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 115/937, Discriminator Loss: 0.6934190988540649, Generator Loss: 0.7198693156242371\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 116/937, Discriminator Loss: 0.6957075893878937, Generator Loss: 0.7234736680984497\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 117/937, Discriminator Loss: 0.7032173275947571, Generator Loss: 0.7305764555931091\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 118/937, Discriminator Loss: 0.683127224445343, Generator Loss: 0.7203906774520874\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 119/937, Discriminator Loss: 0.6970012187957764, Generator Loss: 0.7060083150863647\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 120/937, Discriminator Loss: 0.6936191916465759, Generator Loss: 0.7177222967147827\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 121/937, Discriminator Loss: 0.6835266053676605, Generator Loss: 0.727070689201355\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 122/937, Discriminator Loss: 0.6802681088447571, Generator Loss: 0.7179332375526428\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 123/937, Discriminator Loss: 0.6863072514533997, Generator Loss: 0.7190192341804504\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 124/937, Discriminator Loss: 0.694562554359436, Generator Loss: 0.7148557305335999\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 125/937, Discriminator Loss: 0.6872802078723907, Generator Loss: 0.7059191465377808\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 126/937, Discriminator Loss: 0.6907563805580139, Generator Loss: 0.7050127387046814\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 127/937, Discriminator Loss: 0.6935452818870544, Generator Loss: 0.707175076007843\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Epoch: 10/10, Batch: 128/937, Discriminator Loss: 0.6872217655181885, Generator Loss: 0.7154524922370911\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 129/937, Discriminator Loss: 0.6822454631328583, Generator Loss: 0.7077580094337463\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 130/937, Discriminator Loss: 0.6906329989433289, Generator Loss: 0.70265793800354\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 131/937, Discriminator Loss: 0.6882107853889465, Generator Loss: 0.7144890427589417\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 132/937, Discriminator Loss: 0.697957307100296, Generator Loss: 0.7201474905014038\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 133/937, Discriminator Loss: 0.6947035789489746, Generator Loss: 0.7159680128097534\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 134/937, Discriminator Loss: 0.6896263957023621, Generator Loss: 0.7195305824279785\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 135/937, Discriminator Loss: 0.6924767792224884, Generator Loss: 0.718101978302002\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 136/937, Discriminator Loss: 0.6971930861473083, Generator Loss: 0.7210805416107178\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 137/937, Discriminator Loss: 0.6910692453384399, Generator Loss: 0.7314918041229248\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 138/937, Discriminator Loss: 0.7010435461997986, Generator Loss: 0.7179754972457886\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 139/937, Discriminator Loss: 0.699678361415863, Generator Loss: 0.727969765663147\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 140/937, Discriminator Loss: 0.7037458419799805, Generator Loss: 0.7153631448745728\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 141/937, Discriminator Loss: 0.6876207292079926, Generator Loss: 0.7262190580368042\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 142/937, Discriminator Loss: 0.6821289956569672, Generator Loss: 0.7170999646186829\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 143/937, Discriminator Loss: 0.6907602250576019, Generator Loss: 0.7250909209251404\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 144/937, Discriminator Loss: 0.7019888460636139, Generator Loss: 0.7228648066520691\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 145/937, Discriminator Loss: 0.690479964017868, Generator Loss: 0.7189837098121643\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 146/937, Discriminator Loss: 0.6989908516407013, Generator Loss: 0.7220044136047363\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 147/937, Discriminator Loss: 0.6929036974906921, Generator Loss: 0.7340097427368164\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 148/937, Discriminator Loss: 0.6855082213878632, Generator Loss: 0.7200456857681274\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 149/937, Discriminator Loss: 0.6913465261459351, Generator Loss: 0.7209094762802124\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 150/937, Discriminator Loss: 0.6808870434761047, Generator Loss: 0.7284450531005859\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 10/10, Batch: 151/937, Discriminator Loss: 0.6905945539474487, Generator Loss: 0.7211993932723999\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 152/937, Discriminator Loss: 0.6974117159843445, Generator Loss: 0.7236971855163574\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 153/937, Discriminator Loss: 0.6906026601791382, Generator Loss: 0.7064861059188843\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 154/937, Discriminator Loss: 0.6985966861248016, Generator Loss: 0.718132495880127\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 155/937, Discriminator Loss: 0.6886906623840332, Generator Loss: 0.6908019781112671\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 156/937, Discriminator Loss: 0.681984007358551, Generator Loss: 0.7089963555335999\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 157/937, Discriminator Loss: 0.6775732338428497, Generator Loss: 0.7131313681602478\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 158/937, Discriminator Loss: 0.7023701667785645, Generator Loss: 0.7104029655456543\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 159/937, Discriminator Loss: 0.6880674362182617, Generator Loss: 0.6972613334655762\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 160/937, Discriminator Loss: 0.6950244307518005, Generator Loss: 0.7072136402130127\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 161/937, Discriminator Loss: 0.6921883821487427, Generator Loss: 0.7078067660331726\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 162/937, Discriminator Loss: 0.6808519065380096, Generator Loss: 0.7022577524185181\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 163/937, Discriminator Loss: 0.6951825618743896, Generator Loss: 0.692568302154541\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 164/937, Discriminator Loss: 0.6948913335800171, Generator Loss: 0.703002393245697\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 165/937, Discriminator Loss: 0.689895361661911, Generator Loss: 0.7180246710777283\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 166/937, Discriminator Loss: 0.6851133108139038, Generator Loss: 0.7174038290977478\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 167/937, Discriminator Loss: 0.6894811689853668, Generator Loss: 0.7078735828399658\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 168/937, Discriminator Loss: 0.6993544101715088, Generator Loss: 0.718579888343811\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 169/937, Discriminator Loss: 0.6781478822231293, Generator Loss: 0.7207031846046448\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 170/937, Discriminator Loss: 0.6911519467830658, Generator Loss: 0.72600257396698\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 171/937, Discriminator Loss: 0.6991346478462219, Generator Loss: 0.7222106456756592\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 172/937, Discriminator Loss: 0.694679319858551, Generator Loss: 0.7199229598045349\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 173/937, Discriminator Loss: 0.6762640178203583, Generator Loss: 0.7177056074142456\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 174/937, Discriminator Loss: 0.6865391135215759, Generator Loss: 0.701645016670227\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 175/937, Discriminator Loss: 0.7077702879905701, Generator Loss: 0.7242754101753235\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 176/937, Discriminator Loss: 0.6755857169628143, Generator Loss: 0.7286708354949951\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 177/937, Discriminator Loss: 0.6852940022945404, Generator Loss: 0.7503584027290344\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 178/937, Discriminator Loss: 0.7108613550662994, Generator Loss: 0.7332240343093872\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 179/937, Discriminator Loss: 0.6957031488418579, Generator Loss: 0.7158091068267822\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 180/937, Discriminator Loss: 0.6911687850952148, Generator Loss: 0.7266244888305664\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 181/937, Discriminator Loss: 0.7065237462520599, Generator Loss: 0.7030028104782104\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 182/937, Discriminator Loss: 0.6791662871837616, Generator Loss: 0.7083381414413452\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 183/937, Discriminator Loss: 0.694505512714386, Generator Loss: 0.69795823097229\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 184/937, Discriminator Loss: 0.6822380125522614, Generator Loss: 0.7355119585990906\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 185/937, Discriminator Loss: 0.7010918855667114, Generator Loss: 0.7133814096450806\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 186/937, Discriminator Loss: 0.6893914043903351, Generator Loss: 0.7215876579284668\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 187/937, Discriminator Loss: 0.7071250677108765, Generator Loss: 0.7223336696624756\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 188/937, Discriminator Loss: 0.696577399969101, Generator Loss: 0.7388648390769958\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 189/937, Discriminator Loss: 0.6770209074020386, Generator Loss: 0.7370762228965759\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "Epoch: 10/10, Batch: 190/937, Discriminator Loss: 0.6961159110069275, Generator Loss: 0.7282714247703552\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 191/937, Discriminator Loss: 0.695948600769043, Generator Loss: 0.7261019945144653\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 192/937, Discriminator Loss: 0.6856769621372223, Generator Loss: 0.7257128357887268\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 193/937, Discriminator Loss: 0.6787856221199036, Generator Loss: 0.7284166812896729\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 194/937, Discriminator Loss: 0.6854453086853027, Generator Loss: 0.7242304682731628\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 195/937, Discriminator Loss: 0.6972455084323883, Generator Loss: 0.7170886993408203\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 196/937, Discriminator Loss: 0.6978728473186493, Generator Loss: 0.7209856510162354\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 197/937, Discriminator Loss: 0.706387460231781, Generator Loss: 0.7217608690261841\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 198/937, Discriminator Loss: 0.6943795382976532, Generator Loss: 0.7158724069595337\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 199/937, Discriminator Loss: 0.6902674436569214, Generator Loss: 0.6982543468475342\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 200/937, Discriminator Loss: 0.6939460039138794, Generator Loss: 0.7124235033988953\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 201/937, Discriminator Loss: 0.6923720240592957, Generator Loss: 0.710626482963562\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 202/937, Discriminator Loss: 0.6963041126728058, Generator Loss: 0.7258530855178833\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 203/937, Discriminator Loss: 0.6922455132007599, Generator Loss: 0.7132149934768677\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 204/937, Discriminator Loss: 0.6939706802368164, Generator Loss: 0.7161476612091064\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 205/937, Discriminator Loss: 0.6963450014591217, Generator Loss: 0.7203702926635742\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 206/937, Discriminator Loss: 0.6888808012008667, Generator Loss: 0.7205785512924194\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 207/937, Discriminator Loss: 0.687977135181427, Generator Loss: 0.7238706350326538\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 208/937, Discriminator Loss: 0.6963325440883636, Generator Loss: 0.7176992893218994\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 209/937, Discriminator Loss: 0.6925677061080933, Generator Loss: 0.7187685966491699\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 210/937, Discriminator Loss: 0.6953585147857666, Generator Loss: 0.7164318561553955\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 211/937, Discriminator Loss: 0.6822671890258789, Generator Loss: 0.7343485355377197\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 212/937, Discriminator Loss: 0.6952658295631409, Generator Loss: 0.7308816909790039\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 213/937, Discriminator Loss: 0.6953003108501434, Generator Loss: 0.7239247560501099\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 214/937, Discriminator Loss: 0.6929283738136292, Generator Loss: 0.7275705337524414\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 215/937, Discriminator Loss: 0.701042503118515, Generator Loss: 0.720449686050415\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 216/937, Discriminator Loss: 0.6961094439029694, Generator Loss: 0.706092894077301\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 217/937, Discriminator Loss: 0.6968101561069489, Generator Loss: 0.7199615240097046\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 218/937, Discriminator Loss: 0.6928080916404724, Generator Loss: 0.7030600309371948\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 219/937, Discriminator Loss: 0.6941166818141937, Generator Loss: 0.7108621597290039\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 220/937, Discriminator Loss: 0.6981624960899353, Generator Loss: 0.7098407745361328\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 221/937, Discriminator Loss: 0.7007574439048767, Generator Loss: 0.6967513561248779\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 222/937, Discriminator Loss: 0.6916927099227905, Generator Loss: 0.6992800235748291\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 223/937, Discriminator Loss: 0.698718249797821, Generator Loss: 0.7002270221710205\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 224/937, Discriminator Loss: 0.6820002198219299, Generator Loss: 0.7072399258613586\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 225/937, Discriminator Loss: 0.7087473273277283, Generator Loss: 0.7117860317230225\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 226/937, Discriminator Loss: 0.6790028214454651, Generator Loss: 0.7087690830230713\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 227/937, Discriminator Loss: 0.691595047712326, Generator Loss: 0.7166918516159058\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 228/937, Discriminator Loss: 0.6922683715820312, Generator Loss: 0.7081972360610962\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 229/937, Discriminator Loss: 0.6954825222492218, Generator Loss: 0.7144023180007935\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 230/937, Discriminator Loss: 0.703509509563446, Generator Loss: 0.7181826829910278\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 231/937, Discriminator Loss: 0.6915591955184937, Generator Loss: 0.7094966173171997\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 232/937, Discriminator Loss: 0.7030395269393921, Generator Loss: 0.720386266708374\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 233/937, Discriminator Loss: 0.6954832077026367, Generator Loss: 0.7095929980278015\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 234/937, Discriminator Loss: 0.6913307309150696, Generator Loss: 0.7118872404098511\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 235/937, Discriminator Loss: 0.7077494263648987, Generator Loss: 0.7113592624664307\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 236/937, Discriminator Loss: 0.6917516589164734, Generator Loss: 0.7066608667373657\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 237/937, Discriminator Loss: 0.6942541003227234, Generator Loss: 0.7080659866333008\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 238/937, Discriminator Loss: 0.692428708076477, Generator Loss: 0.7170963883399963\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 239/937, Discriminator Loss: 0.6960410177707672, Generator Loss: 0.7148721218109131\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 240/937, Discriminator Loss: 0.6920920312404633, Generator Loss: 0.7158679366111755\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 241/937, Discriminator Loss: 0.6986223757266998, Generator Loss: 0.7171427011489868\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 242/937, Discriminator Loss: 0.6962337493896484, Generator Loss: 0.7126608490943909\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 243/937, Discriminator Loss: 0.6871523261070251, Generator Loss: 0.7289060354232788\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 244/937, Discriminator Loss: 0.6886975765228271, Generator Loss: 0.7062689065933228\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 245/937, Discriminator Loss: 0.6937165856361389, Generator Loss: 0.7131211757659912\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 246/937, Discriminator Loss: 0.6897093951702118, Generator Loss: 0.7149417996406555\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 247/937, Discriminator Loss: 0.697968989610672, Generator Loss: 0.7115660905838013\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 248/937, Discriminator Loss: 0.6858260631561279, Generator Loss: 0.7087392210960388\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 249/937, Discriminator Loss: 0.6897425949573517, Generator Loss: 0.7020989060401917\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 250/937, Discriminator Loss: 0.6920218467712402, Generator Loss: 0.7169666290283203\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 251/937, Discriminator Loss: 0.7000434398651123, Generator Loss: 0.7227308750152588\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 252/937, Discriminator Loss: 0.7002748548984528, Generator Loss: 0.727534294128418\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 253/937, Discriminator Loss: 0.6980562508106232, Generator Loss: 0.7202029228210449\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 254/937, Discriminator Loss: 0.6951637864112854, Generator Loss: 0.7031809091567993\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 255/937, Discriminator Loss: 0.6915710270404816, Generator Loss: 0.7103005051612854\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 256/937, Discriminator Loss: 0.6991077363491058, Generator Loss: 0.7121577262878418\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 257/937, Discriminator Loss: 0.6804606020450592, Generator Loss: 0.713320255279541\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 258/937, Discriminator Loss: 0.6924582421779633, Generator Loss: 0.7175456881523132\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 259/937, Discriminator Loss: 0.6944045126438141, Generator Loss: 0.6954180002212524\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 260/937, Discriminator Loss: 0.7005214095115662, Generator Loss: 0.7146040201187134\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 261/937, Discriminator Loss: 0.7006075382232666, Generator Loss: 0.7254565954208374\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 262/937, Discriminator Loss: 0.6936695873737335, Generator Loss: 0.7125564217567444\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 263/937, Discriminator Loss: 0.7046940326690674, Generator Loss: 0.716773509979248\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 264/937, Discriminator Loss: 0.691455066204071, Generator Loss: 0.7114881277084351\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 265/937, Discriminator Loss: 0.6933051645755768, Generator Loss: 0.7178854942321777\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 266/937, Discriminator Loss: 0.7001409828662872, Generator Loss: 0.711111843585968\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 267/937, Discriminator Loss: 0.6957451105117798, Generator Loss: 0.7082041501998901\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 268/937, Discriminator Loss: 0.6908761262893677, Generator Loss: 0.7110515236854553\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 269/937, Discriminator Loss: 0.6938005089759827, Generator Loss: 0.7171221375465393\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 270/937, Discriminator Loss: 0.6916047930717468, Generator Loss: 0.7144221067428589\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 271/937, Discriminator Loss: 0.6877737045288086, Generator Loss: 0.7200743556022644\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 272/937, Discriminator Loss: 0.6871351301670074, Generator Loss: 0.7104910612106323\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 273/937, Discriminator Loss: 0.6984966695308685, Generator Loss: 0.7008830904960632\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 274/937, Discriminator Loss: 0.6907382309436798, Generator Loss: 0.7108365893363953\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 275/937, Discriminator Loss: 0.6886525750160217, Generator Loss: 0.7220015525817871\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 276/937, Discriminator Loss: 0.6876446008682251, Generator Loss: 0.7112143635749817\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 277/937, Discriminator Loss: 0.6950413882732391, Generator Loss: 0.7092607021331787\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 278/937, Discriminator Loss: 0.6946858763694763, Generator Loss: 0.7185635566711426\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 279/937, Discriminator Loss: 0.6887300908565521, Generator Loss: 0.7154015898704529\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 280/937, Discriminator Loss: 0.6981795132160187, Generator Loss: 0.7134177088737488\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 281/937, Discriminator Loss: 0.6868081390857697, Generator Loss: 0.728851318359375\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 282/937, Discriminator Loss: 0.6891291439533234, Generator Loss: 0.7197926044464111\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 283/937, Discriminator Loss: 0.6849140822887421, Generator Loss: 0.7447775602340698\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 284/937, Discriminator Loss: 0.6845456063747406, Generator Loss: 0.7204331755638123\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 285/937, Discriminator Loss: 0.6976572871208191, Generator Loss: 0.7256840467453003\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 286/937, Discriminator Loss: 0.6857159435749054, Generator Loss: 0.7279777526855469\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 287/937, Discriminator Loss: 0.6924485564231873, Generator Loss: 0.7322046756744385\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 288/937, Discriminator Loss: 0.7109605073928833, Generator Loss: 0.7271678447723389\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 289/937, Discriminator Loss: 0.7008154094219208, Generator Loss: 0.7163482308387756\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 290/937, Discriminator Loss: 0.6996104419231415, Generator Loss: 0.7141231298446655\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 291/937, Discriminator Loss: 0.6877138912677765, Generator Loss: 0.7269922494888306\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 292/937, Discriminator Loss: 0.696198582649231, Generator Loss: 0.7099365592002869\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 293/937, Discriminator Loss: 0.698532223701477, Generator Loss: 0.7067925333976746\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 294/937, Discriminator Loss: 0.698084831237793, Generator Loss: 0.7079328298568726\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 295/937, Discriminator Loss: 0.6749494075775146, Generator Loss: 0.6996139287948608\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 296/937, Discriminator Loss: 0.6921647489070892, Generator Loss: 0.7098225355148315\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 297/937, Discriminator Loss: 0.6854422092437744, Generator Loss: 0.7109657526016235\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 298/937, Discriminator Loss: 0.6880762577056885, Generator Loss: 0.7124157547950745\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 299/937, Discriminator Loss: 0.6786282360553741, Generator Loss: 0.7169333696365356\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 300/937, Discriminator Loss: 0.7022232711315155, Generator Loss: 0.716650128364563\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 301/937, Discriminator Loss: 0.6906026601791382, Generator Loss: 0.6985254287719727\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 302/937, Discriminator Loss: 0.6854439079761505, Generator Loss: 0.7221019268035889\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 303/937, Discriminator Loss: 0.7019644677639008, Generator Loss: 0.7152646780014038\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 304/937, Discriminator Loss: 0.6913173496723175, Generator Loss: 0.7080484628677368\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 305/937, Discriminator Loss: 0.6871878504753113, Generator Loss: 0.7041332721710205\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 306/937, Discriminator Loss: 0.6935582160949707, Generator Loss: 0.7148506045341492\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 307/937, Discriminator Loss: 0.689007580280304, Generator Loss: 0.7061322927474976\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 308/937, Discriminator Loss: 0.6894992887973785, Generator Loss: 0.7031474113464355\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 309/937, Discriminator Loss: 0.6922138333320618, Generator Loss: 0.7132126092910767\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 310/937, Discriminator Loss: 0.691311240196228, Generator Loss: 0.70484459400177\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 311/937, Discriminator Loss: 0.6897449195384979, Generator Loss: 0.7106465101242065\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 312/937, Discriminator Loss: 0.6932871639728546, Generator Loss: 0.7048683166503906\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 313/937, Discriminator Loss: 0.7020347118377686, Generator Loss: 0.7063498497009277\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 314/937, Discriminator Loss: 0.6882339715957642, Generator Loss: 0.7008933424949646\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 315/937, Discriminator Loss: 0.6871203780174255, Generator Loss: 0.707271158695221\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 316/937, Discriminator Loss: 0.6903655529022217, Generator Loss: 0.6919362545013428\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 317/937, Discriminator Loss: 0.6968261897563934, Generator Loss: 0.6911128163337708\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 318/937, Discriminator Loss: 0.7071039974689484, Generator Loss: 0.7019264698028564\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 319/937, Discriminator Loss: 0.6874503791332245, Generator Loss: 0.6968336701393127\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 320/937, Discriminator Loss: 0.70050048828125, Generator Loss: 0.710137665271759\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 321/937, Discriminator Loss: 0.6975550055503845, Generator Loss: 0.7092866897583008\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 322/937, Discriminator Loss: 0.7018295228481293, Generator Loss: 0.7143652439117432\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 323/937, Discriminator Loss: 0.691214382648468, Generator Loss: 0.7095271944999695\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 324/937, Discriminator Loss: 0.6862528324127197, Generator Loss: 0.7049651145935059\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 325/937, Discriminator Loss: 0.6854999363422394, Generator Loss: 0.7150918245315552\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 326/937, Discriminator Loss: 0.6823728978633881, Generator Loss: 0.7186723947525024\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 327/937, Discriminator Loss: 0.700128585100174, Generator Loss: 0.709338903427124\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 328/937, Discriminator Loss: 0.7066599726676941, Generator Loss: 0.7141815423965454\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 329/937, Discriminator Loss: 0.6929323673248291, Generator Loss: 0.7001962661743164\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 330/937, Discriminator Loss: 0.681400865316391, Generator Loss: 0.7098830938339233\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 331/937, Discriminator Loss: 0.6994620561599731, Generator Loss: 0.7118844389915466\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 332/937, Discriminator Loss: 0.6849765479564667, Generator Loss: 0.7122385501861572\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 333/937, Discriminator Loss: 0.6874081194400787, Generator Loss: 0.7099961042404175\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 334/937, Discriminator Loss: 0.6829639673233032, Generator Loss: 0.7041915059089661\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 335/937, Discriminator Loss: 0.6889024078845978, Generator Loss: 0.6974886059761047\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 336/937, Discriminator Loss: 0.6862150430679321, Generator Loss: 0.7063872814178467\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 337/937, Discriminator Loss: 0.6965507864952087, Generator Loss: 0.7282930016517639\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 338/937, Discriminator Loss: 0.6952041685581207, Generator Loss: 0.7319371700286865\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 339/937, Discriminator Loss: 0.6888472139835358, Generator Loss: 0.7279216647148132\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 340/937, Discriminator Loss: 0.7009643316268921, Generator Loss: 0.7362260222434998\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 341/937, Discriminator Loss: 0.6957138776779175, Generator Loss: 0.7353482842445374\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 342/937, Discriminator Loss: 0.6942634582519531, Generator Loss: 0.740361213684082\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 343/937, Discriminator Loss: 0.6870496273040771, Generator Loss: 0.7283511757850647\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 344/937, Discriminator Loss: 0.6954374313354492, Generator Loss: 0.7429517507553101\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 345/937, Discriminator Loss: 0.6974529027938843, Generator Loss: 0.7456177473068237\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 346/937, Discriminator Loss: 0.7058039009571075, Generator Loss: 0.7136757373809814\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 347/937, Discriminator Loss: 0.6854767799377441, Generator Loss: 0.7120176553726196\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 348/937, Discriminator Loss: 0.6783574819564819, Generator Loss: 0.7019083499908447\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 349/937, Discriminator Loss: 0.6992216408252716, Generator Loss: 0.7111399173736572\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 350/937, Discriminator Loss: 0.6946223080158234, Generator Loss: 0.6987979412078857\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 351/937, Discriminator Loss: 0.6913704872131348, Generator Loss: 0.7184435129165649\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 352/937, Discriminator Loss: 0.6987535059452057, Generator Loss: 0.7197745442390442\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 353/937, Discriminator Loss: 0.6913070678710938, Generator Loss: 0.7212380170822144\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 354/937, Discriminator Loss: 0.685263991355896, Generator Loss: 0.7205513715744019\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 355/937, Discriminator Loss: 0.6842924952507019, Generator Loss: 0.7212897539138794\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 356/937, Discriminator Loss: 0.684497594833374, Generator Loss: 0.7130328416824341\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 357/937, Discriminator Loss: 0.6891868710517883, Generator Loss: 0.7255659103393555\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 358/937, Discriminator Loss: 0.6817004680633545, Generator Loss: 0.7499347925186157\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 359/937, Discriminator Loss: 0.6830617487430573, Generator Loss: 0.7428983449935913\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 360/937, Discriminator Loss: 0.6971247792243958, Generator Loss: 0.7245846390724182\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 361/937, Discriminator Loss: 0.6938316226005554, Generator Loss: 0.7282091379165649\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 362/937, Discriminator Loss: 0.6918735802173615, Generator Loss: 0.7055567502975464\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 363/937, Discriminator Loss: 0.691547155380249, Generator Loss: 0.7186095714569092\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 364/937, Discriminator Loss: 0.6899840831756592, Generator Loss: 0.715596079826355\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 365/937, Discriminator Loss: 0.6876681745052338, Generator Loss: 0.7236664295196533\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 366/937, Discriminator Loss: 0.6947922706604004, Generator Loss: 0.7046557664871216\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 367/937, Discriminator Loss: 0.7139544486999512, Generator Loss: 0.7226947546005249\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 368/937, Discriminator Loss: 0.6868287920951843, Generator Loss: 0.7125792503356934\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 369/937, Discriminator Loss: 0.6925997734069824, Generator Loss: 0.7221412658691406\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 370/937, Discriminator Loss: 0.685746967792511, Generator Loss: 0.7101524472236633\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 371/937, Discriminator Loss: 0.6925471723079681, Generator Loss: 0.7157829999923706\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 372/937, Discriminator Loss: 0.6863427460193634, Generator Loss: 0.7193949222564697\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 373/937, Discriminator Loss: 0.6851526498794556, Generator Loss: 0.7167551517486572\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 374/937, Discriminator Loss: 0.6965306401252747, Generator Loss: 0.7107673287391663\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 375/937, Discriminator Loss: 0.691009521484375, Generator Loss: 0.7036322951316833\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 376/937, Discriminator Loss: 0.6918053925037384, Generator Loss: 0.7158255577087402\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 377/937, Discriminator Loss: 0.677121639251709, Generator Loss: 0.7145279049873352\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 378/937, Discriminator Loss: 0.6800442039966583, Generator Loss: 0.7201930284500122\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 379/937, Discriminator Loss: 0.685897022485733, Generator Loss: 0.7208893299102783\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 380/937, Discriminator Loss: 0.693465381860733, Generator Loss: 0.7235399484634399\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 381/937, Discriminator Loss: 0.6981931328773499, Generator Loss: 0.7307456135749817\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 382/937, Discriminator Loss: 0.700583815574646, Generator Loss: 0.7133532762527466\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 383/937, Discriminator Loss: 0.6793943643569946, Generator Loss: 0.7257274389266968\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 384/937, Discriminator Loss: 0.6920348703861237, Generator Loss: 0.7169961929321289\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 385/937, Discriminator Loss: 0.6853293180465698, Generator Loss: 0.7569974660873413\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 386/937, Discriminator Loss: 0.6925652325153351, Generator Loss: 0.7299018502235413\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 387/937, Discriminator Loss: 0.6936101913452148, Generator Loss: 0.704879641532898\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 388/937, Discriminator Loss: 0.7054129838943481, Generator Loss: 0.7432353496551514\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 389/937, Discriminator Loss: 0.6810712814331055, Generator Loss: 0.7119845151901245\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 390/937, Discriminator Loss: 0.7005944848060608, Generator Loss: 0.7185817956924438\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 391/937, Discriminator Loss: 0.6965236961841583, Generator Loss: 0.7205528020858765\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 392/937, Discriminator Loss: 0.7012633383274078, Generator Loss: 0.7197265625\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 393/937, Discriminator Loss: 0.7002497315406799, Generator Loss: 0.7098714113235474\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 394/937, Discriminator Loss: 0.6930462121963501, Generator Loss: 0.7235113382339478\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 395/937, Discriminator Loss: 0.688611626625061, Generator Loss: 0.7208350896835327\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 396/937, Discriminator Loss: 0.6858007907867432, Generator Loss: 0.7208795547485352\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 397/937, Discriminator Loss: 0.6983807981014252, Generator Loss: 0.7487793564796448\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 398/937, Discriminator Loss: 0.7015034258365631, Generator Loss: 0.734082818031311\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 399/937, Discriminator Loss: 0.7062402963638306, Generator Loss: 0.723027229309082\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 400/937, Discriminator Loss: 0.6885619759559631, Generator Loss: 0.718386709690094\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 401/937, Discriminator Loss: 0.6975780725479126, Generator Loss: 0.7134519815444946\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 402/937, Discriminator Loss: 0.685300201177597, Generator Loss: 0.7063030004501343\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 403/937, Discriminator Loss: 0.6934146881103516, Generator Loss: 0.7150864601135254\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 404/937, Discriminator Loss: 0.6934284269809723, Generator Loss: 0.7195336818695068\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 405/937, Discriminator Loss: 0.6876920461654663, Generator Loss: 0.7122671604156494\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 406/937, Discriminator Loss: 0.6933179497718811, Generator Loss: 0.7115883231163025\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 407/937, Discriminator Loss: 0.6957746744155884, Generator Loss: 0.7293793559074402\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 408/937, Discriminator Loss: 0.690201610326767, Generator Loss: 0.722296953201294\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 409/937, Discriminator Loss: 0.6892216205596924, Generator Loss: 0.7180716395378113\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 410/937, Discriminator Loss: 0.6821569204330444, Generator Loss: 0.7145130634307861\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 411/937, Discriminator Loss: 0.6898762285709381, Generator Loss: 0.7189860343933105\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 412/937, Discriminator Loss: 0.6866022050380707, Generator Loss: 0.7535821199417114\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 413/937, Discriminator Loss: 0.7041279673576355, Generator Loss: 0.7198246717453003\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 414/937, Discriminator Loss: 0.693735659122467, Generator Loss: 0.7396397590637207\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 415/937, Discriminator Loss: 0.6892354190349579, Generator Loss: 0.7230130434036255\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 416/937, Discriminator Loss: 0.6890303492546082, Generator Loss: 0.7153466939926147\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 417/937, Discriminator Loss: 0.6986787021160126, Generator Loss: 0.7205708026885986\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 418/937, Discriminator Loss: 0.6717807650566101, Generator Loss: 0.7056323289871216\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 419/937, Discriminator Loss: 0.6716171205043793, Generator Loss: 0.7106431126594543\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 420/937, Discriminator Loss: 0.6852888762950897, Generator Loss: 0.6913531422615051\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 421/937, Discriminator Loss: 0.6896780133247375, Generator Loss: 0.7141134142875671\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 422/937, Discriminator Loss: 0.6922588348388672, Generator Loss: 0.7057920098304749\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 423/937, Discriminator Loss: 0.6881105005741119, Generator Loss: 0.7149931192398071\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 424/937, Discriminator Loss: 0.6978724300861359, Generator Loss: 0.7226729393005371\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 425/937, Discriminator Loss: 0.6993707120418549, Generator Loss: 0.719163179397583\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 426/937, Discriminator Loss: 0.6795923113822937, Generator Loss: 0.7191222906112671\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 427/937, Discriminator Loss: 0.6878677010536194, Generator Loss: 0.7159243822097778\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 428/937, Discriminator Loss: 0.6958522498607635, Generator Loss: 0.7322390079498291\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 429/937, Discriminator Loss: 0.6765830516815186, Generator Loss: 0.745659351348877\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 430/937, Discriminator Loss: 0.6777216196060181, Generator Loss: 0.7231026887893677\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 431/937, Discriminator Loss: 0.6931390464305878, Generator Loss: 0.7285882234573364\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 432/937, Discriminator Loss: 0.673643946647644, Generator Loss: 0.7178531885147095\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 433/937, Discriminator Loss: 0.6850003004074097, Generator Loss: 0.711851954460144\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 434/937, Discriminator Loss: 0.7066036760807037, Generator Loss: 0.6916210055351257\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 435/937, Discriminator Loss: 0.7078222334384918, Generator Loss: 0.730549693107605\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 436/937, Discriminator Loss: 0.6812939047813416, Generator Loss: 0.726866602897644\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 437/937, Discriminator Loss: 0.6837044656276703, Generator Loss: 0.7061240673065186\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 438/937, Discriminator Loss: 0.6920782923698425, Generator Loss: 0.7219828963279724\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 439/937, Discriminator Loss: 0.6807377338409424, Generator Loss: 0.7004863023757935\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 440/937, Discriminator Loss: 0.6843124628067017, Generator Loss: 0.7217355966567993\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 441/937, Discriminator Loss: 0.6839367151260376, Generator Loss: 0.7052376866340637\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 442/937, Discriminator Loss: 0.6860259175300598, Generator Loss: 0.7072943449020386\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 443/937, Discriminator Loss: 0.6862492859363556, Generator Loss: 0.7172474265098572\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 444/937, Discriminator Loss: 0.6707701981067657, Generator Loss: 0.69755619764328\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 445/937, Discriminator Loss: 0.6838872134685516, Generator Loss: 0.6971887946128845\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 446/937, Discriminator Loss: 0.6871461570262909, Generator Loss: 0.698884129524231\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 447/937, Discriminator Loss: 0.6865665018558502, Generator Loss: 0.6979540586471558\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 448/937, Discriminator Loss: 0.6970643997192383, Generator Loss: 0.7185400724411011\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 449/937, Discriminator Loss: 0.6802248358726501, Generator Loss: 0.7143769264221191\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 450/937, Discriminator Loss: 0.6950767636299133, Generator Loss: 0.7204059362411499\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 451/937, Discriminator Loss: 0.6968920528888702, Generator Loss: 0.7187297344207764\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 452/937, Discriminator Loss: 0.690656453371048, Generator Loss: 0.7103363275527954\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 453/937, Discriminator Loss: 0.6929052174091339, Generator Loss: 0.7172167301177979\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 454/937, Discriminator Loss: 0.6878800392150879, Generator Loss: 0.7210034132003784\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 455/937, Discriminator Loss: 0.6886547803878784, Generator Loss: 0.7120094299316406\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 456/937, Discriminator Loss: 0.6754462420940399, Generator Loss: 0.7053088545799255\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 457/937, Discriminator Loss: 0.6863729059696198, Generator Loss: 0.7196499109268188\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 458/937, Discriminator Loss: 0.6890674829483032, Generator Loss: 0.7094855308532715\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 459/937, Discriminator Loss: 0.6887446641921997, Generator Loss: 0.7145727872848511\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 460/937, Discriminator Loss: 0.6869262158870697, Generator Loss: 0.7229118347167969\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 461/937, Discriminator Loss: 0.6892225742340088, Generator Loss: 0.7067015171051025\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 462/937, Discriminator Loss: 0.7121779024600983, Generator Loss: 0.7323259115219116\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 463/937, Discriminator Loss: 0.6877744495868683, Generator Loss: 0.7234170436859131\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 464/937, Discriminator Loss: 0.7037227749824524, Generator Loss: 0.7102193832397461\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 465/937, Discriminator Loss: 0.6966274678707123, Generator Loss: 0.7190170288085938\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 466/937, Discriminator Loss: 0.6896301805973053, Generator Loss: 0.7190463542938232\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 467/937, Discriminator Loss: 0.6863259077072144, Generator Loss: 0.7243951559066772\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 468/937, Discriminator Loss: 0.694013774394989, Generator Loss: 0.713982880115509\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 469/937, Discriminator Loss: 0.6713538467884064, Generator Loss: 0.7034658789634705\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 470/937, Discriminator Loss: 0.699081540107727, Generator Loss: 0.721176028251648\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 471/937, Discriminator Loss: 0.6865599751472473, Generator Loss: 0.7225368022918701\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 472/937, Discriminator Loss: 0.7024337351322174, Generator Loss: 0.7229064702987671\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 473/937, Discriminator Loss: 0.6758449077606201, Generator Loss: 0.7243061065673828\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 474/937, Discriminator Loss: 0.6836112141609192, Generator Loss: 0.7237005233764648\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 475/937, Discriminator Loss: 0.68457892537117, Generator Loss: 0.7140127420425415\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 476/937, Discriminator Loss: 0.6849339008331299, Generator Loss: 0.7210367321968079\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 477/937, Discriminator Loss: 0.6832190454006195, Generator Loss: 0.7159217596054077\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 478/937, Discriminator Loss: 0.6860360205173492, Generator Loss: 0.7174831628799438\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 479/937, Discriminator Loss: 0.6997628211975098, Generator Loss: 0.711969256401062\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 480/937, Discriminator Loss: 0.6734118461608887, Generator Loss: 0.7335318922996521\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 481/937, Discriminator Loss: 0.6920108795166016, Generator Loss: 0.7458754181861877\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 482/937, Discriminator Loss: 0.714879721403122, Generator Loss: 0.7305416464805603\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 483/937, Discriminator Loss: 0.6875517666339874, Generator Loss: 0.722502589225769\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 484/937, Discriminator Loss: 0.669643759727478, Generator Loss: 0.694434404373169\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 485/937, Discriminator Loss: 0.7246947288513184, Generator Loss: 0.7141685485839844\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 486/937, Discriminator Loss: 0.6806711852550507, Generator Loss: 0.7297073006629944\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 487/937, Discriminator Loss: 0.6688939929008484, Generator Loss: 0.7280951738357544\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 488/937, Discriminator Loss: 0.6884580850601196, Generator Loss: 0.7153090238571167\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 489/937, Discriminator Loss: 0.7001630961894989, Generator Loss: 0.7319927215576172\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 490/937, Discriminator Loss: 0.6889907419681549, Generator Loss: 0.7363776564598083\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 491/937, Discriminator Loss: 0.6840057969093323, Generator Loss: 0.7380104064941406\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 492/937, Discriminator Loss: 0.6929206550121307, Generator Loss: 0.7353062033653259\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 493/937, Discriminator Loss: 0.6757409572601318, Generator Loss: 0.7385954260826111\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 494/937, Discriminator Loss: 0.6886031031608582, Generator Loss: 0.7322427034378052\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 495/937, Discriminator Loss: 0.6933505833148956, Generator Loss: 0.734722375869751\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 496/937, Discriminator Loss: 0.6963194906711578, Generator Loss: 0.7388668060302734\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 497/937, Discriminator Loss: 0.7008217871189117, Generator Loss: 0.7284843921661377\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 498/937, Discriminator Loss: 0.6874929964542389, Generator Loss: 0.7357630133628845\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 499/937, Discriminator Loss: 0.6710090935230255, Generator Loss: 0.7355589270591736\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 500/937, Discriminator Loss: 0.6942956447601318, Generator Loss: 0.7282828688621521\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 501/937, Discriminator Loss: 0.6818718612194061, Generator Loss: 0.7249002456665039\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 502/937, Discriminator Loss: 0.6676602065563202, Generator Loss: 0.7397058606147766\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 503/937, Discriminator Loss: 0.6936615109443665, Generator Loss: 0.7281244993209839\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 504/937, Discriminator Loss: 0.6811778247356415, Generator Loss: 0.7263866662979126\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 505/937, Discriminator Loss: 0.6954452693462372, Generator Loss: 0.750420868396759\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 506/937, Discriminator Loss: 0.6793776452541351, Generator Loss: 0.7337189316749573\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 507/937, Discriminator Loss: 0.6983121037483215, Generator Loss: 0.7507209777832031\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 508/937, Discriminator Loss: 0.7248764336109161, Generator Loss: 0.7118703126907349\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 509/937, Discriminator Loss: 0.6918826699256897, Generator Loss: 0.719667911529541\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 510/937, Discriminator Loss: 0.691004604101181, Generator Loss: 0.728746235370636\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 511/937, Discriminator Loss: 0.6958296597003937, Generator Loss: 0.7318499088287354\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 512/937, Discriminator Loss: 0.6857521235942841, Generator Loss: 0.7429245710372925\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 513/937, Discriminator Loss: 0.6821407675743103, Generator Loss: 0.7440782785415649\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 514/937, Discriminator Loss: 0.6764856576919556, Generator Loss: 0.7462877035140991\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 515/937, Discriminator Loss: 0.7071161270141602, Generator Loss: 0.7270232439041138\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 516/937, Discriminator Loss: 0.6970015466213226, Generator Loss: 0.732050895690918\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 517/937, Discriminator Loss: 0.6914229989051819, Generator Loss: 0.7408976554870605\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 518/937, Discriminator Loss: 0.6802345812320709, Generator Loss: 0.7486873865127563\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 519/937, Discriminator Loss: 0.6734272241592407, Generator Loss: 0.7426764965057373\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 520/937, Discriminator Loss: 0.684505820274353, Generator Loss: 1.2154467105865479\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 521/937, Discriminator Loss: 0.9906710088253021, Generator Loss: 0.7138568162918091\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 522/937, Discriminator Loss: 0.6918728649616241, Generator Loss: 0.7209323644638062\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 523/937, Discriminator Loss: 0.6952508389949799, Generator Loss: 0.6937873959541321\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 524/937, Discriminator Loss: 0.7007384896278381, Generator Loss: 0.719567060470581\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 525/937, Discriminator Loss: 0.6844683587551117, Generator Loss: 0.7210923433303833\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 526/937, Discriminator Loss: 0.704874724149704, Generator Loss: 0.7061197757720947\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 527/937, Discriminator Loss: 0.6989985704421997, Generator Loss: 0.713613748550415\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 528/937, Discriminator Loss: 0.6879875063896179, Generator Loss: 0.720184326171875\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 529/937, Discriminator Loss: 0.6801429986953735, Generator Loss: 0.7174875140190125\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 530/937, Discriminator Loss: 0.6810785830020905, Generator Loss: 0.7041880488395691\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 531/937, Discriminator Loss: 0.7109159529209137, Generator Loss: 0.7214279174804688\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 532/937, Discriminator Loss: 0.6890747547149658, Generator Loss: 0.7213650941848755\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 533/937, Discriminator Loss: 0.687432736158371, Generator Loss: 0.7271173596382141\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 534/937, Discriminator Loss: 0.6879265010356903, Generator Loss: 0.7176162004470825\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 535/937, Discriminator Loss: 0.6955674886703491, Generator Loss: 0.7174150943756104\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 536/937, Discriminator Loss: 0.6901016533374786, Generator Loss: 0.7155530452728271\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 537/937, Discriminator Loss: 0.6921091079711914, Generator Loss: 0.7314433455467224\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 538/937, Discriminator Loss: 0.6844888925552368, Generator Loss: 0.7278271913528442\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 539/937, Discriminator Loss: 0.6850691437721252, Generator Loss: 0.7182058691978455\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 540/937, Discriminator Loss: 0.6986467242240906, Generator Loss: 0.7109307050704956\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 541/937, Discriminator Loss: 0.6912742257118225, Generator Loss: 0.7172760367393494\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 542/937, Discriminator Loss: 0.6898832619190216, Generator Loss: 0.7203971743583679\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 543/937, Discriminator Loss: 0.688221663236618, Generator Loss: 0.717558741569519\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 544/937, Discriminator Loss: 0.6962289810180664, Generator Loss: 0.697272539138794\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 545/937, Discriminator Loss: 0.688306599855423, Generator Loss: 0.7067298889160156\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 546/937, Discriminator Loss: 0.6898425221443176, Generator Loss: 0.6964088678359985\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 547/937, Discriminator Loss: 0.6918147504329681, Generator Loss: 0.707541823387146\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 548/937, Discriminator Loss: 0.6743935346603394, Generator Loss: 0.7041054964065552\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 549/937, Discriminator Loss: 0.6952638030052185, Generator Loss: 0.7029737234115601\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 550/937, Discriminator Loss: 0.6857984960079193, Generator Loss: 0.6990087032318115\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 551/937, Discriminator Loss: 0.6975584626197815, Generator Loss: 0.7284703254699707\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 552/937, Discriminator Loss: 0.6963885426521301, Generator Loss: 0.7094815373420715\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 553/937, Discriminator Loss: 0.6830413341522217, Generator Loss: 0.715630292892456\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 554/937, Discriminator Loss: 0.6928854882717133, Generator Loss: 0.7196542620658875\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 555/937, Discriminator Loss: 0.6939075589179993, Generator Loss: 0.7217655181884766\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 556/937, Discriminator Loss: 0.6987292170524597, Generator Loss: 0.712440013885498\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 557/937, Discriminator Loss: 0.7136604487895966, Generator Loss: 0.7123445272445679\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 558/937, Discriminator Loss: 0.6859234869480133, Generator Loss: 0.734156608581543\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 559/937, Discriminator Loss: 0.684123694896698, Generator Loss: 0.7169923186302185\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 560/937, Discriminator Loss: 0.7100353837013245, Generator Loss: 0.7192996740341187\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 561/937, Discriminator Loss: 0.6873379051685333, Generator Loss: 0.71736079454422\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 562/937, Discriminator Loss: 0.6775357723236084, Generator Loss: 0.7162539958953857\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 563/937, Discriminator Loss: 0.6993906199932098, Generator Loss: 0.7061113119125366\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 564/937, Discriminator Loss: 0.6837862730026245, Generator Loss: 0.7173471450805664\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 565/937, Discriminator Loss: 0.6883071660995483, Generator Loss: 0.7201052904129028\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 566/937, Discriminator Loss: 0.6862645745277405, Generator Loss: 0.7016721963882446\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 567/937, Discriminator Loss: 0.7032300531864166, Generator Loss: 0.7153801918029785\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 568/937, Discriminator Loss: 0.6915761232376099, Generator Loss: 0.7163228392601013\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 569/937, Discriminator Loss: 0.6912085115909576, Generator Loss: 0.7201530933380127\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 570/937, Discriminator Loss: 0.6978383660316467, Generator Loss: 0.7218090891838074\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 571/937, Discriminator Loss: 0.6865952908992767, Generator Loss: 0.699799656867981\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 572/937, Discriminator Loss: 0.69669970870018, Generator Loss: 0.7111845016479492\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 573/937, Discriminator Loss: 0.6858007907867432, Generator Loss: 0.7137429714202881\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 574/937, Discriminator Loss: 0.6954385936260223, Generator Loss: 0.7070069313049316\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 575/937, Discriminator Loss: 0.6920892894268036, Generator Loss: 0.7262363433837891\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 576/937, Discriminator Loss: 0.6978440880775452, Generator Loss: 0.713245153427124\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 577/937, Discriminator Loss: 0.6920585632324219, Generator Loss: 0.7121987342834473\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 578/937, Discriminator Loss: 0.6838034689426422, Generator Loss: 0.7141703367233276\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 579/937, Discriminator Loss: 0.6979755163192749, Generator Loss: 0.714535117149353\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 580/937, Discriminator Loss: 0.69659623503685, Generator Loss: 0.7212947607040405\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 581/937, Discriminator Loss: 0.6918171346187592, Generator Loss: 0.7172189950942993\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 582/937, Discriminator Loss: 0.6895942986011505, Generator Loss: 0.7109054923057556\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 583/937, Discriminator Loss: 0.6790956556797028, Generator Loss: 0.7079887390136719\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 584/937, Discriminator Loss: 0.7000406086444855, Generator Loss: 0.7125605344772339\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 585/937, Discriminator Loss: 0.6732039749622345, Generator Loss: 0.7065457701683044\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 586/937, Discriminator Loss: 0.6804883778095245, Generator Loss: 0.7112861275672913\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 587/937, Discriminator Loss: 0.6939288973808289, Generator Loss: 0.7213320732116699\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 588/937, Discriminator Loss: 0.6799611747264862, Generator Loss: 0.7137283086776733\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 589/937, Discriminator Loss: 0.6935170292854309, Generator Loss: 0.7290288209915161\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 590/937, Discriminator Loss: 0.6921454966068268, Generator Loss: 0.7174882888793945\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 591/937, Discriminator Loss: 0.6806676387786865, Generator Loss: 0.7144187688827515\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 592/937, Discriminator Loss: 0.6963638961315155, Generator Loss: 0.7305454015731812\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 593/937, Discriminator Loss: 0.6931494176387787, Generator Loss: 0.7107577919960022\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 594/937, Discriminator Loss: 0.689122349023819, Generator Loss: 0.7342646718025208\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 595/937, Discriminator Loss: 0.6867033243179321, Generator Loss: 0.7141794562339783\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 596/937, Discriminator Loss: 0.6932980418205261, Generator Loss: 0.7285970449447632\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 597/937, Discriminator Loss: 0.694958508014679, Generator Loss: 0.7157777547836304\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 598/937, Discriminator Loss: 0.6955302357673645, Generator Loss: 0.7219089269638062\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 599/937, Discriminator Loss: 0.6979820728302002, Generator Loss: 0.7043148279190063\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 600/937, Discriminator Loss: 0.6847642958164215, Generator Loss: 0.7029561996459961\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 601/937, Discriminator Loss: 0.689272403717041, Generator Loss: 0.7246557474136353\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 602/937, Discriminator Loss: 0.6963513493537903, Generator Loss: 0.7187656164169312\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 603/937, Discriminator Loss: 0.6900722682476044, Generator Loss: 0.718898594379425\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 604/937, Discriminator Loss: 0.6863104701042175, Generator Loss: 0.7509539723396301\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 605/937, Discriminator Loss: 0.6939807534217834, Generator Loss: 0.7253705263137817\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 606/937, Discriminator Loss: 0.6942334473133087, Generator Loss: 0.6911203861236572\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 607/937, Discriminator Loss: 0.7505699992179871, Generator Loss: 0.7144707441329956\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 608/937, Discriminator Loss: 0.6742841005325317, Generator Loss: 0.7238040566444397\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 609/937, Discriminator Loss: 0.690950870513916, Generator Loss: 0.7273406386375427\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 610/937, Discriminator Loss: 0.6885719001293182, Generator Loss: 0.7435709834098816\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 611/937, Discriminator Loss: 0.6840648651123047, Generator Loss: 0.7387269735336304\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 612/937, Discriminator Loss: 0.6951625049114227, Generator Loss: 0.7341470122337341\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 613/937, Discriminator Loss: 0.6926984786987305, Generator Loss: 0.7071252465248108\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 614/937, Discriminator Loss: 0.6973070204257965, Generator Loss: 0.7114783525466919\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 615/937, Discriminator Loss: 0.6994459331035614, Generator Loss: 0.7413027882575989\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 616/937, Discriminator Loss: 0.7023826539516449, Generator Loss: 0.718205451965332\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 617/937, Discriminator Loss: 0.7041548490524292, Generator Loss: 0.7384698987007141\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 618/937, Discriminator Loss: 0.6942998766899109, Generator Loss: 0.7364949584007263\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 619/937, Discriminator Loss: 0.6992312371730804, Generator Loss: 0.7355234026908875\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 620/937, Discriminator Loss: 0.6896826028823853, Generator Loss: 0.7120794057846069\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 621/937, Discriminator Loss: 0.6892995238304138, Generator Loss: 0.7297317981719971\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 622/937, Discriminator Loss: 0.691019743680954, Generator Loss: 0.7228995561599731\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 623/937, Discriminator Loss: 0.6883176267147064, Generator Loss: 0.7308698892593384\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 624/937, Discriminator Loss: 0.6969435811042786, Generator Loss: 0.7360364198684692\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 625/937, Discriminator Loss: 0.679595559835434, Generator Loss: 0.7216667532920837\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 626/937, Discriminator Loss: 0.684710681438446, Generator Loss: 0.720618486404419\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 627/937, Discriminator Loss: 0.7135543823242188, Generator Loss: 0.7346315383911133\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 628/937, Discriminator Loss: 0.6705032885074615, Generator Loss: 0.7116290330886841\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 629/937, Discriminator Loss: 0.6850308775901794, Generator Loss: 0.7236034870147705\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 630/937, Discriminator Loss: 0.7170435786247253, Generator Loss: 0.7270082235336304\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 631/937, Discriminator Loss: 0.6926324963569641, Generator Loss: 0.7196850776672363\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 632/937, Discriminator Loss: 0.6948524713516235, Generator Loss: 0.7289086580276489\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 633/937, Discriminator Loss: 0.6881316304206848, Generator Loss: 0.7134918570518494\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 634/937, Discriminator Loss: 0.6679627001285553, Generator Loss: 0.7238977551460266\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 635/937, Discriminator Loss: 0.6867582201957703, Generator Loss: 0.7277631759643555\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 636/937, Discriminator Loss: 0.6912774443626404, Generator Loss: 0.7236064672470093\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 637/937, Discriminator Loss: 0.7038833498954773, Generator Loss: 0.7177481651306152\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 638/937, Discriminator Loss: 0.7029238939285278, Generator Loss: 0.7328511476516724\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 639/937, Discriminator Loss: 0.6763941645622253, Generator Loss: 0.7531977891921997\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 640/937, Discriminator Loss: 0.6910065412521362, Generator Loss: 0.7320406436920166\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 641/937, Discriminator Loss: 0.6952036619186401, Generator Loss: 0.7360228300094604\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 642/937, Discriminator Loss: 0.6921637058258057, Generator Loss: 0.7361793518066406\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 643/937, Discriminator Loss: 0.6800321638584137, Generator Loss: 0.7384132146835327\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 644/937, Discriminator Loss: 0.702116996049881, Generator Loss: 0.7579876780509949\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 645/937, Discriminator Loss: 0.6801169514656067, Generator Loss: 0.743433952331543\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 646/937, Discriminator Loss: 0.6878495216369629, Generator Loss: 0.7594863176345825\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 647/937, Discriminator Loss: 0.6925296783447266, Generator Loss: 0.7455506324768066\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 648/937, Discriminator Loss: 0.7005037665367126, Generator Loss: 0.7207145690917969\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 649/937, Discriminator Loss: 0.7017208635807037, Generator Loss: 0.7177783250808716\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 650/937, Discriminator Loss: 0.6904343664646149, Generator Loss: 0.7340774536132812\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 651/937, Discriminator Loss: 0.6895330548286438, Generator Loss: 0.6998862028121948\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 652/937, Discriminator Loss: 0.6840702295303345, Generator Loss: 0.7248051762580872\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 653/937, Discriminator Loss: 0.6885438859462738, Generator Loss: 0.7244189977645874\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 654/937, Discriminator Loss: 0.6937278211116791, Generator Loss: 0.7257634997367859\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 655/937, Discriminator Loss: 0.6750650405883789, Generator Loss: 0.7338985204696655\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 656/937, Discriminator Loss: 0.6965404450893402, Generator Loss: 0.7418956160545349\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 657/937, Discriminator Loss: 0.6853026449680328, Generator Loss: 0.7439966201782227\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 658/937, Discriminator Loss: 0.6930326223373413, Generator Loss: 0.7397212982177734\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 659/937, Discriminator Loss: 0.6809047162532806, Generator Loss: 0.7269602417945862\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 660/937, Discriminator Loss: 0.7013164162635803, Generator Loss: 0.7157303094863892\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 661/937, Discriminator Loss: 0.700623095035553, Generator Loss: 0.7186914682388306\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 662/937, Discriminator Loss: 0.6891543567180634, Generator Loss: 0.7227952480316162\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 663/937, Discriminator Loss: 0.6944989562034607, Generator Loss: 0.721623420715332\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 664/937, Discriminator Loss: 0.6953352987766266, Generator Loss: 0.7107568979263306\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 665/937, Discriminator Loss: 0.6889278590679169, Generator Loss: 0.7090492248535156\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 666/937, Discriminator Loss: 0.686750739812851, Generator Loss: 0.7132699489593506\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 667/937, Discriminator Loss: 0.697562038898468, Generator Loss: 0.7243545651435852\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 668/937, Discriminator Loss: 0.6902928948402405, Generator Loss: 0.7210690379142761\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 669/937, Discriminator Loss: 0.6892619729042053, Generator Loss: 0.7253026366233826\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 670/937, Discriminator Loss: 0.6892750263214111, Generator Loss: 0.7173436284065247\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 671/937, Discriminator Loss: 0.6913036704063416, Generator Loss: 0.7234716415405273\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 672/937, Discriminator Loss: 0.6903524994850159, Generator Loss: 0.7136656045913696\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 673/937, Discriminator Loss: 0.7034678757190704, Generator Loss: 0.712632417678833\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 674/937, Discriminator Loss: 0.6863941848278046, Generator Loss: 0.7137526869773865\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 675/937, Discriminator Loss: 0.6942159235477448, Generator Loss: 0.7059272527694702\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 676/937, Discriminator Loss: 0.7118341028690338, Generator Loss: 0.7212662696838379\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 677/937, Discriminator Loss: 0.6871459186077118, Generator Loss: 0.7140992879867554\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 678/937, Discriminator Loss: 0.6945409178733826, Generator Loss: 0.7208101153373718\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 679/937, Discriminator Loss: 0.6877803802490234, Generator Loss: 0.7012209892272949\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 680/937, Discriminator Loss: 0.6934258043766022, Generator Loss: 0.7291737794876099\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 681/937, Discriminator Loss: 0.6795941293239594, Generator Loss: 0.7187023162841797\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 682/937, Discriminator Loss: 0.6894586980342865, Generator Loss: 0.7105816602706909\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 683/937, Discriminator Loss: 0.6814238131046295, Generator Loss: 0.7148377895355225\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 684/937, Discriminator Loss: 0.6886997222900391, Generator Loss: 0.727257251739502\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 685/937, Discriminator Loss: 0.6964650750160217, Generator Loss: 0.7046513557434082\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 686/937, Discriminator Loss: 0.6925976574420929, Generator Loss: 0.7163063287734985\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 687/937, Discriminator Loss: 0.68588587641716, Generator Loss: 0.7127359509468079\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 688/937, Discriminator Loss: 0.6893278360366821, Generator Loss: 0.7334173917770386\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 689/937, Discriminator Loss: 0.6921328008174896, Generator Loss: 0.763558566570282\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 690/937, Discriminator Loss: 0.6870239973068237, Generator Loss: 0.7358106374740601\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 691/937, Discriminator Loss: 0.6912595331668854, Generator Loss: 0.7160249948501587\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 692/937, Discriminator Loss: 0.6888591647148132, Generator Loss: 0.7131359577178955\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 693/937, Discriminator Loss: 0.685946524143219, Generator Loss: 0.7155817151069641\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 694/937, Discriminator Loss: 0.6860783398151398, Generator Loss: 0.7128950357437134\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 695/937, Discriminator Loss: 0.6721850335597992, Generator Loss: 0.7001438140869141\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 696/937, Discriminator Loss: 0.6553125083446503, Generator Loss: 0.7225099205970764\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 697/937, Discriminator Loss: 0.6702467501163483, Generator Loss: 0.7160805463790894\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 698/937, Discriminator Loss: 0.7176768779754639, Generator Loss: 0.7271466851234436\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 699/937, Discriminator Loss: 0.6851557791233063, Generator Loss: 0.721626877784729\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 700/937, Discriminator Loss: 0.6976476907730103, Generator Loss: 0.7309118509292603\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 701/937, Discriminator Loss: 0.7000477910041809, Generator Loss: 0.7834372520446777\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 702/937, Discriminator Loss: 0.6703535914421082, Generator Loss: 0.7500015497207642\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 703/937, Discriminator Loss: 0.7163422107696533, Generator Loss: 0.7147877216339111\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 704/937, Discriminator Loss: 0.6837490797042847, Generator Loss: 0.7175174951553345\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 705/937, Discriminator Loss: 0.6828807294368744, Generator Loss: 0.7097451686859131\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 706/937, Discriminator Loss: 0.6953299641609192, Generator Loss: 0.7276740670204163\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 707/937, Discriminator Loss: 0.6916057467460632, Generator Loss: 0.7131955623626709\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 708/937, Discriminator Loss: 0.6879039704799652, Generator Loss: 0.7022128105163574\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 709/937, Discriminator Loss: 0.6651346981525421, Generator Loss: 0.7213743925094604\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 710/937, Discriminator Loss: 0.6901101469993591, Generator Loss: 0.7147545218467712\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 711/937, Discriminator Loss: 0.692608654499054, Generator Loss: 0.7240636944770813\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 712/937, Discriminator Loss: 0.6934450268745422, Generator Loss: 0.7201046347618103\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 713/937, Discriminator Loss: 0.6966898739337921, Generator Loss: 0.7405096292495728\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 714/937, Discriminator Loss: 0.6817624270915985, Generator Loss: 0.7394204139709473\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 715/937, Discriminator Loss: 0.6744387149810791, Generator Loss: 0.7444277405738831\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 716/937, Discriminator Loss: 0.6892305910587311, Generator Loss: 0.7530401945114136\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 717/937, Discriminator Loss: 0.6911071836948395, Generator Loss: 0.7252886295318604\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 718/937, Discriminator Loss: 0.6864655911922455, Generator Loss: 0.7275078296661377\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 719/937, Discriminator Loss: 0.701293408870697, Generator Loss: 0.7203346490859985\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 720/937, Discriminator Loss: 0.7010053396224976, Generator Loss: 0.7191284894943237\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 721/937, Discriminator Loss: 0.6750139594078064, Generator Loss: 0.7086847424507141\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 722/937, Discriminator Loss: 0.6722663342952728, Generator Loss: 0.7406051754951477\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 723/937, Discriminator Loss: 0.678541749715805, Generator Loss: 0.7627850770950317\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 724/937, Discriminator Loss: 0.6974341869354248, Generator Loss: 0.7358530759811401\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 725/937, Discriminator Loss: 0.6700215935707092, Generator Loss: 0.7668717503547668\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 726/937, Discriminator Loss: 0.686349481344223, Generator Loss: 0.7460126876831055\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 727/937, Discriminator Loss: 0.741669088602066, Generator Loss: 0.7198091745376587\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 728/937, Discriminator Loss: 0.6909037828445435, Generator Loss: 0.7345311641693115\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 729/937, Discriminator Loss: 0.6818522810935974, Generator Loss: 0.7361949682235718\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 730/937, Discriminator Loss: 0.6859522461891174, Generator Loss: 0.7075315713882446\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 731/937, Discriminator Loss: 0.680116206407547, Generator Loss: 0.732594907283783\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 732/937, Discriminator Loss: 0.696337878704071, Generator Loss: 0.7216662168502808\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 733/937, Discriminator Loss: 0.696587324142456, Generator Loss: 0.7244513630867004\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 734/937, Discriminator Loss: 0.6633361279964447, Generator Loss: 0.7085971236228943\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 735/937, Discriminator Loss: 0.6935799717903137, Generator Loss: 0.7210263013839722\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 736/937, Discriminator Loss: 0.6927802860736847, Generator Loss: 0.7074844837188721\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 737/937, Discriminator Loss: 0.6959837973117828, Generator Loss: 0.7230923771858215\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 738/937, Discriminator Loss: 0.6907720863819122, Generator Loss: 0.7251893281936646\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 739/937, Discriminator Loss: 0.6976466774940491, Generator Loss: 0.7199139595031738\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 740/937, Discriminator Loss: 0.688237875699997, Generator Loss: 0.7327349185943604\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 741/937, Discriminator Loss: 0.6818349957466125, Generator Loss: 0.7184396982192993\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 742/937, Discriminator Loss: 0.6827163696289062, Generator Loss: 0.7152283191680908\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 743/937, Discriminator Loss: 0.6991268694400787, Generator Loss: 0.7111649513244629\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 744/937, Discriminator Loss: 0.6874508261680603, Generator Loss: 0.713456928730011\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 745/937, Discriminator Loss: 0.7164521217346191, Generator Loss: 0.7070314288139343\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 746/937, Discriminator Loss: 0.6977914869785309, Generator Loss: 0.7039773464202881\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 747/937, Discriminator Loss: 0.693183422088623, Generator Loss: 0.7100936770439148\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 748/937, Discriminator Loss: 0.7028950154781342, Generator Loss: 0.7297258377075195\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 749/937, Discriminator Loss: 0.6935460567474365, Generator Loss: 0.729833722114563\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 750/937, Discriminator Loss: 0.6915784776210785, Generator Loss: 0.7202719449996948\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 751/937, Discriminator Loss: 0.6936486065387726, Generator Loss: 0.7210096120834351\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 752/937, Discriminator Loss: 0.6925526261329651, Generator Loss: 0.7197023630142212\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 753/937, Discriminator Loss: 0.70221808552742, Generator Loss: 0.7248023748397827\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 754/937, Discriminator Loss: 0.6995968520641327, Generator Loss: 0.7246378660202026\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 755/937, Discriminator Loss: 0.6790815889835358, Generator Loss: 0.7370922565460205\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 756/937, Discriminator Loss: 0.7056437730789185, Generator Loss: 0.7153304815292358\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 757/937, Discriminator Loss: 0.6938573122024536, Generator Loss: 0.7220891714096069\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 758/937, Discriminator Loss: 0.7050991952419281, Generator Loss: 0.716149628162384\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 759/937, Discriminator Loss: 0.7006160616874695, Generator Loss: 0.7274544835090637\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 760/937, Discriminator Loss: 0.7057449817657471, Generator Loss: 0.7260373830795288\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 761/937, Discriminator Loss: 0.6808435916900635, Generator Loss: 0.7326033115386963\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 762/937, Discriminator Loss: 0.7022127509117126, Generator Loss: 0.7410069704055786\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 763/937, Discriminator Loss: 0.7067535519599915, Generator Loss: 0.7187344431877136\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 764/937, Discriminator Loss: 0.6931818723678589, Generator Loss: 0.714482843875885\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 765/937, Discriminator Loss: 0.6869813203811646, Generator Loss: 0.7299452424049377\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 766/937, Discriminator Loss: 0.6930713653564453, Generator Loss: 0.7190356254577637\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 767/937, Discriminator Loss: 0.6959013044834137, Generator Loss: 0.729735255241394\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 768/937, Discriminator Loss: 0.697946161031723, Generator Loss: 0.724748969078064\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 769/937, Discriminator Loss: 0.6991136074066162, Generator Loss: 0.7226347923278809\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 770/937, Discriminator Loss: 0.6881255805492401, Generator Loss: 0.7160464525222778\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 771/937, Discriminator Loss: 0.6977558732032776, Generator Loss: 0.7221875786781311\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 772/937, Discriminator Loss: 0.6908991634845734, Generator Loss: 0.7153732776641846\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 773/937, Discriminator Loss: 0.6754457056522369, Generator Loss: 0.7173169851303101\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 774/937, Discriminator Loss: 0.7021284699440002, Generator Loss: 0.728585958480835\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 775/937, Discriminator Loss: 0.6948440968990326, Generator Loss: 0.7232725620269775\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 776/937, Discriminator Loss: 0.7008082568645477, Generator Loss: 0.7282296419143677\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 777/937, Discriminator Loss: 0.6906890273094177, Generator Loss: 0.717657208442688\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 778/937, Discriminator Loss: 0.6875982284545898, Generator Loss: 0.7265430688858032\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 779/937, Discriminator Loss: 0.6985733509063721, Generator Loss: 0.7166116237640381\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 780/937, Discriminator Loss: 0.6987183094024658, Generator Loss: 0.7264255881309509\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 781/937, Discriminator Loss: 0.6917456090450287, Generator Loss: 0.7253593802452087\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 782/937, Discriminator Loss: 0.6895497739315033, Generator Loss: 0.7066048979759216\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 783/937, Discriminator Loss: 0.6905480325222015, Generator Loss: 0.6990815997123718\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 784/937, Discriminator Loss: 0.6986942887306213, Generator Loss: 0.7195470333099365\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 785/937, Discriminator Loss: 0.7008571326732635, Generator Loss: 0.7123409509658813\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 786/937, Discriminator Loss: 0.6712421774864197, Generator Loss: 0.7173957824707031\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 787/937, Discriminator Loss: 0.6909438967704773, Generator Loss: 0.7245358228683472\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 788/937, Discriminator Loss: 0.7044393420219421, Generator Loss: 0.7200801372528076\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 789/937, Discriminator Loss: 0.6900401711463928, Generator Loss: 0.7260861992835999\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 790/937, Discriminator Loss: 0.705372542142868, Generator Loss: 0.723266065120697\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 791/937, Discriminator Loss: 0.70108562707901, Generator Loss: 0.7144615054130554\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 792/937, Discriminator Loss: 0.6922208070755005, Generator Loss: 0.7247459888458252\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 793/937, Discriminator Loss: 0.694155365228653, Generator Loss: 0.7106191515922546\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 794/937, Discriminator Loss: 0.6860503852367401, Generator Loss: 0.729977011680603\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 795/937, Discriminator Loss: 0.6820188760757446, Generator Loss: 0.7411814332008362\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 796/937, Discriminator Loss: 0.6908497512340546, Generator Loss: 0.7385281324386597\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 797/937, Discriminator Loss: 0.6866677701473236, Generator Loss: 0.7391619086265564\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 798/937, Discriminator Loss: 0.6957534551620483, Generator Loss: 0.7250055074691772\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 799/937, Discriminator Loss: 0.6884244680404663, Generator Loss: 0.7341969609260559\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 800/937, Discriminator Loss: 0.6937256455421448, Generator Loss: 0.7104362845420837\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 801/937, Discriminator Loss: 0.6853410601615906, Generator Loss: 0.714120626449585\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 802/937, Discriminator Loss: 0.6966568231582642, Generator Loss: 0.6907367706298828\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 803/937, Discriminator Loss: 0.6981793344020844, Generator Loss: 0.7055398225784302\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 804/937, Discriminator Loss: 0.6923885643482208, Generator Loss: 0.7029522061347961\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 805/937, Discriminator Loss: 0.6942864060401917, Generator Loss: 0.7086735963821411\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 806/937, Discriminator Loss: 0.6966730356216431, Generator Loss: 0.714595377445221\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 807/937, Discriminator Loss: 0.69328972697258, Generator Loss: 0.7118352651596069\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 808/937, Discriminator Loss: 0.6914515197277069, Generator Loss: 0.7205085754394531\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 809/937, Discriminator Loss: 0.695216566324234, Generator Loss: 0.7219289541244507\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 810/937, Discriminator Loss: 0.6780376136302948, Generator Loss: 0.7195072174072266\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 811/937, Discriminator Loss: 0.6933830976486206, Generator Loss: 0.7225699424743652\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 812/937, Discriminator Loss: 0.697972446680069, Generator Loss: 0.7138050198554993\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 813/937, Discriminator Loss: 0.6949597895145416, Generator Loss: 0.7160310745239258\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 814/937, Discriminator Loss: 0.6766769587993622, Generator Loss: 0.7190130949020386\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 815/937, Discriminator Loss: 0.693623423576355, Generator Loss: 0.7212018966674805\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 816/937, Discriminator Loss: 0.6975898742675781, Generator Loss: 0.7199022769927979\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 817/937, Discriminator Loss: 0.6830788254737854, Generator Loss: 0.720685601234436\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 818/937, Discriminator Loss: 0.699920117855072, Generator Loss: 0.7151013612747192\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 819/937, Discriminator Loss: 0.6925157010555267, Generator Loss: 0.7221899628639221\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 820/937, Discriminator Loss: 0.6865854859352112, Generator Loss: 0.7247464656829834\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 821/937, Discriminator Loss: 0.6907593607902527, Generator Loss: 0.7110376358032227\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 822/937, Discriminator Loss: 0.6907699704170227, Generator Loss: 0.721466064453125\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 823/937, Discriminator Loss: 0.6958924531936646, Generator Loss: 0.7317935228347778\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 824/937, Discriminator Loss: 0.7023361623287201, Generator Loss: 0.7138671875\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 825/937, Discriminator Loss: 0.7024855315685272, Generator Loss: 0.7122973203659058\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 826/937, Discriminator Loss: 0.6859634816646576, Generator Loss: 0.7067743539810181\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 827/937, Discriminator Loss: 0.6927123963832855, Generator Loss: 0.70430588722229\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Epoch: 10/10, Batch: 828/937, Discriminator Loss: 0.690609872341156, Generator Loss: 0.7012938261032104\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 829/937, Discriminator Loss: 0.6910300850868225, Generator Loss: 0.7023332715034485\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 830/937, Discriminator Loss: 0.6853731274604797, Generator Loss: 0.7052737474441528\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 831/937, Discriminator Loss: 0.687749832868576, Generator Loss: 0.7091401815414429\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 832/937, Discriminator Loss: 0.6876871585845947, Generator Loss: 0.7072621583938599\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 833/937, Discriminator Loss: 0.6842818558216095, Generator Loss: 0.6991437673568726\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 834/937, Discriminator Loss: 0.7076871395111084, Generator Loss: 0.7041752338409424\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 835/937, Discriminator Loss: 0.6871903538703918, Generator Loss: 0.6974197626113892\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 836/937, Discriminator Loss: 0.6846096217632294, Generator Loss: 0.6991417407989502\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 837/937, Discriminator Loss: 0.6810377240180969, Generator Loss: 0.7025434970855713\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 838/937, Discriminator Loss: 0.6782409250736237, Generator Loss: 0.7146182060241699\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 839/937, Discriminator Loss: 0.6957098543643951, Generator Loss: 0.7042656540870667\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 840/937, Discriminator Loss: 0.6868175268173218, Generator Loss: 0.704674482345581\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 841/937, Discriminator Loss: 0.6911811232566833, Generator Loss: 0.7137557864189148\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 842/937, Discriminator Loss: 0.7023980915546417, Generator Loss: 0.7130011916160583\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 843/937, Discriminator Loss: 0.6858888268470764, Generator Loss: 0.7174606323242188\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 844/937, Discriminator Loss: 0.6988077759742737, Generator Loss: 0.7148113250732422\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 845/937, Discriminator Loss: 0.6860252022743225, Generator Loss: 0.7269023060798645\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 846/937, Discriminator Loss: 0.6817743480205536, Generator Loss: 0.7137859463691711\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 847/937, Discriminator Loss: 0.6938821077346802, Generator Loss: 0.7121739387512207\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 848/937, Discriminator Loss: 0.6850016415119171, Generator Loss: 0.7164261341094971\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 849/937, Discriminator Loss: 0.6980176866054535, Generator Loss: 0.7031106948852539\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 850/937, Discriminator Loss: 0.6869133710861206, Generator Loss: 0.7213164567947388\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 851/937, Discriminator Loss: 0.6980814933776855, Generator Loss: 0.7244123220443726\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 852/937, Discriminator Loss: 0.6952215433120728, Generator Loss: 0.7180323600769043\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 853/937, Discriminator Loss: 0.6888906955718994, Generator Loss: 0.7180513143539429\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 854/937, Discriminator Loss: 0.6885069608688354, Generator Loss: 0.716780424118042\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 855/937, Discriminator Loss: 0.6855904459953308, Generator Loss: 0.7154131531715393\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 856/937, Discriminator Loss: 0.696563333272934, Generator Loss: 0.7074109315872192\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 857/937, Discriminator Loss: 0.6889906525611877, Generator Loss: 0.7231652736663818\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 858/937, Discriminator Loss: 0.6959829926490784, Generator Loss: 0.714077353477478\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 859/937, Discriminator Loss: 0.6957223117351532, Generator Loss: 0.7199546098709106\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 860/937, Discriminator Loss: 0.690678596496582, Generator Loss: 0.7177034616470337\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 861/937, Discriminator Loss: 0.6869188547134399, Generator Loss: 0.7175629138946533\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 862/937, Discriminator Loss: 0.6893855333328247, Generator Loss: 0.730284571647644\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 863/937, Discriminator Loss: 0.682393342256546, Generator Loss: 0.7418859601020813\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 864/937, Discriminator Loss: 0.7018677592277527, Generator Loss: 0.7212252616882324\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 865/937, Discriminator Loss: 0.6900909841060638, Generator Loss: 0.7198145985603333\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 866/937, Discriminator Loss: 0.6943884193897247, Generator Loss: 0.7235907316207886\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 867/937, Discriminator Loss: 0.6963559687137604, Generator Loss: 0.7181421518325806\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 868/937, Discriminator Loss: 0.6964731812477112, Generator Loss: 0.7169243693351746\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 869/937, Discriminator Loss: 0.6951730251312256, Generator Loss: 0.7129934430122375\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 870/937, Discriminator Loss: 0.6974627375602722, Generator Loss: 0.7092279195785522\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 871/937, Discriminator Loss: 0.6920253038406372, Generator Loss: 0.7139238119125366\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 872/937, Discriminator Loss: 0.685333788394928, Generator Loss: 0.7014920711517334\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 873/937, Discriminator Loss: 0.6767255067825317, Generator Loss: 0.7088671922683716\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 874/937, Discriminator Loss: 0.6947880387306213, Generator Loss: 0.6856460571289062\n",
            "2/2 [==============================] - 0s 20ms/step\n",
            "Epoch: 10/10, Batch: 875/937, Discriminator Loss: 0.6852672696113586, Generator Loss: 0.7114710807800293\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 876/937, Discriminator Loss: 0.6816352307796478, Generator Loss: 0.7007519006729126\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 877/937, Discriminator Loss: 0.6979454457759857, Generator Loss: 0.7086872458457947\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 878/937, Discriminator Loss: 0.6949176788330078, Generator Loss: 0.7078579664230347\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 879/937, Discriminator Loss: 0.6801561117172241, Generator Loss: 0.714702308177948\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 880/937, Discriminator Loss: 0.689508706331253, Generator Loss: 0.711378812789917\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 881/937, Discriminator Loss: 0.6869188547134399, Generator Loss: 0.7112609148025513\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Epoch: 10/10, Batch: 882/937, Discriminator Loss: 0.6916422843933105, Generator Loss: 0.7119635343551636\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 883/937, Discriminator Loss: 0.6875311732292175, Generator Loss: 0.7123355865478516\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 884/937, Discriminator Loss: 0.6936239302158356, Generator Loss: 0.7145524621009827\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 885/937, Discriminator Loss: 0.6995474696159363, Generator Loss: 0.7291574478149414\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 886/937, Discriminator Loss: 0.6888397634029388, Generator Loss: 0.7160516977310181\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 887/937, Discriminator Loss: 0.6851484477519989, Generator Loss: 0.7346935272216797\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 888/937, Discriminator Loss: 0.692569762468338, Generator Loss: 0.7404378652572632\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Epoch: 10/10, Batch: 889/937, Discriminator Loss: 0.6866271495819092, Generator Loss: 0.7305319309234619\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 890/937, Discriminator Loss: 0.6894028782844543, Generator Loss: 0.7268967032432556\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 891/937, Discriminator Loss: 0.6862810254096985, Generator Loss: 0.7269365787506104\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 892/937, Discriminator Loss: 0.6832555532455444, Generator Loss: 0.7212620973587036\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 893/937, Discriminator Loss: 0.6888492405414581, Generator Loss: 0.7087719440460205\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 894/937, Discriminator Loss: 0.6840488910675049, Generator Loss: 0.7047457695007324\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 895/937, Discriminator Loss: 0.7220195233821869, Generator Loss: 0.7268050909042358\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 896/937, Discriminator Loss: 0.6935876905918121, Generator Loss: 0.726938009262085\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 897/937, Discriminator Loss: 0.7034008502960205, Generator Loss: 0.7209241390228271\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 898/937, Discriminator Loss: 0.6905537843704224, Generator Loss: 0.7139682769775391\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 899/937, Discriminator Loss: 0.6954296231269836, Generator Loss: 0.7046571969985962\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 900/937, Discriminator Loss: 0.685558021068573, Generator Loss: 0.7098138332366943\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 901/937, Discriminator Loss: 0.7123261392116547, Generator Loss: 0.7200853824615479\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 902/937, Discriminator Loss: 0.6867469251155853, Generator Loss: 0.7322872877120972\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 903/937, Discriminator Loss: 0.692341148853302, Generator Loss: 0.7170411944389343\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Epoch: 10/10, Batch: 904/937, Discriminator Loss: 0.6949566900730133, Generator Loss: 0.7137874364852905\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 905/937, Discriminator Loss: 0.7035605013370514, Generator Loss: 0.7315035462379456\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 906/937, Discriminator Loss: 0.6910766959190369, Generator Loss: 0.7178045511245728\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 907/937, Discriminator Loss: 0.7004672884941101, Generator Loss: 0.7310503721237183\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 908/937, Discriminator Loss: 0.6858807504177094, Generator Loss: 0.7196942567825317\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 909/937, Discriminator Loss: 0.689405083656311, Generator Loss: 0.7137175798416138\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Epoch: 10/10, Batch: 910/937, Discriminator Loss: 0.6931309401988983, Generator Loss: 0.7188436985015869\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 911/937, Discriminator Loss: 0.6894670128822327, Generator Loss: 0.7269328832626343\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 912/937, Discriminator Loss: 0.6949567794799805, Generator Loss: 0.7149006724357605\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 913/937, Discriminator Loss: 0.6781347990036011, Generator Loss: 0.7464340925216675\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 914/937, Discriminator Loss: 0.7185106575489044, Generator Loss: 0.721756637096405\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 915/937, Discriminator Loss: 0.6998679935932159, Generator Loss: 0.7155370712280273\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Epoch: 10/10, Batch: 916/937, Discriminator Loss: 0.6742827892303467, Generator Loss: 0.7262541651725769\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 917/937, Discriminator Loss: 0.7003153264522552, Generator Loss: 0.7119449377059937\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 918/937, Discriminator Loss: 0.6872339844703674, Generator Loss: 0.7159342765808105\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 919/937, Discriminator Loss: 0.694417417049408, Generator Loss: 0.7246381044387817\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 920/937, Discriminator Loss: 0.6864210665225983, Generator Loss: 0.7060340642929077\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 921/937, Discriminator Loss: 0.6871205270290375, Generator Loss: 0.7069689035415649\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 922/937, Discriminator Loss: 0.6902654469013214, Generator Loss: 0.7101535797119141\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 923/937, Discriminator Loss: 0.6911856830120087, Generator Loss: 0.6949305534362793\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Epoch: 10/10, Batch: 924/937, Discriminator Loss: 0.7017315626144409, Generator Loss: 0.7315242886543274\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 925/937, Discriminator Loss: 0.6978941559791565, Generator Loss: 0.7177942395210266\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 926/937, Discriminator Loss: 0.6998797953128815, Generator Loss: 0.7175934314727783\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 927/937, Discriminator Loss: 0.6740851402282715, Generator Loss: 0.7158923149108887\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Epoch: 10/10, Batch: 928/937, Discriminator Loss: 0.6882912218570709, Generator Loss: 0.7235295176506042\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 929/937, Discriminator Loss: 0.7069622874259949, Generator Loss: 0.7406275272369385\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Epoch: 10/10, Batch: 930/937, Discriminator Loss: 0.6964319348335266, Generator Loss: 0.7282782196998596\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 931/937, Discriminator Loss: 0.6964055001735687, Generator Loss: 0.7269028425216675\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Epoch: 10/10, Batch: 932/937, Discriminator Loss: 0.6868322193622589, Generator Loss: 0.7202671766281128\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 933/937, Discriminator Loss: 0.6931001842021942, Generator Loss: 0.7251672744750977\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Epoch: 10/10, Batch: 934/937, Discriminator Loss: 0.6937767565250397, Generator Loss: 0.7193607091903687\n",
            "2/2 [==============================] - 0s 7ms/step\n",
            "Epoch: 10/10, Batch: 935/937, Discriminator Loss: 0.6935811042785645, Generator Loss: 0.718138575553894\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "Epoch: 10/10, Batch: 936/937, Discriminator Loss: 0.686673492193222, Generator Loss: 0.7130619883537292\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "Epoch: 10/10, Batch: 937/937, Discriminator Loss: 0.697431892156601, Generator Loss: 0.7229496240615845\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Set hyperparameters\n",
        "latent_dim = 100\n",
        "num_classes = 10\n",
        "image_shape = (28, 28, 1)\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "save_interval = 5\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\")\n",
        "x_train = (x_train - 127.5) / 127.5  # Normalize images to [-1, 1]\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "\n",
        "# Build the generator network\n",
        "def build_generator(latent_dim, num_classes):\n",
        "    noise = layers.Input(shape=(latent_dim,))\n",
        "    label = layers.Input(shape=(num_classes,))\n",
        "\n",
        "    x = layers.Concatenate()([noise, label])\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(28 * 28 * 1, activation=\"tanh\")(x)\n",
        "    generated_image = layers.Reshape(image_shape)(x)\n",
        "\n",
        "    model = models.Model([noise, label], generated_image)\n",
        "    return model\n",
        "\n",
        "# Build the discriminator network\n",
        "def build_discriminator(image_shape, num_classes):\n",
        "    image = layers.Input(shape=image_shape)\n",
        "    label = layers.Input(shape=(num_classes,))\n",
        "\n",
        "    x = layers.Flatten()(image)\n",
        "    x = layers.Concatenate()([x, label])\n",
        "    x = layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    validity = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = models.Model([image, label], validity)\n",
        "    return model\n",
        "\n",
        "# Build the CGAN model\n",
        "def build_cgan(generator, discriminator):\n",
        "    noise = layers.Input(shape=(latent_dim,))\n",
        "    label = layers.Input(shape=(num_classes,))\n",
        "    generated_image = generator([noise, label])\n",
        "    validity = discriminator([generated_image, label])\n",
        "\n",
        "    model = models.Model([noise, label], validity)\n",
        "    return model\n",
        "\n",
        "# Build the generator and discriminator networks\n",
        "generator = build_generator(latent_dim, num_classes)\n",
        "discriminator = build_discriminator(image_shape, num_classes)\n",
        "\n",
        "# Build the CGAN model\n",
        "cgan = build_cgan(generator, discriminator)\n",
        "\n",
        "# Compile the CGAN model\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "discriminator.trainable = False\n",
        "cgan.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "\n",
        "# Training loop\n",
        "num_batches = x_train.shape[0] // batch_size\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(num_batches):\n",
        "        # Train the discriminator\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        real_images = x_train[idx]\n",
        "        labels = y_train[idx]\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        generated_images = generator.predict([noise, labels])\n",
        "\n",
        "        real_labels = np.ones((batch_size, 1))\n",
        "        fake_labels = np.zeros((batch_size, 1))\n",
        "        real_loss = discriminator.train_on_batch([real_images, labels], real_labels)\n",
        "        fake_loss = discriminator.train_on_batch([generated_images, labels], fake_labels)\n",
        "        discriminator_loss = 0.5 * np.add(real_loss, fake_loss)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        generator_loss = cgan.train_on_batch([noise, labels], real_labels)\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}, Batch: {batch+1}/{num_batches}, Discriminator Loss: {discriminator_loss}, Generator Loss: {generator_loss}\")\n",
        "\n",
        "    # Generate and save sample images\n",
        "    if (epoch + 1) % save_interval == 0:\n",
        "        num_samples = 10\n",
        "        noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "        labels = np.eye(num_classes)[np.arange(num_samples) % num_classes]\n",
        "        generated_images = generator.predict([noise, labels])\n",
        "\n",
        "        # Rescale images to [0, 1]\n",
        "        generated_images = (generated_images + 1) / 2.0\n",
        "\n",
        "        # Save sample images\n",
        "        os.makedirs(\"samples\", exist_ok=True)\n",
        "        for i in range(num_samples):\n",
        "            plt.imsave(f\"samples/sample_{epoch+1}_{i}.png\", generated_images[i].reshape(28, 28), cmap=\"gray\")\n",
        "\n",
        "# Save the generator model\n",
        "# generator.save(\"generator_model.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lists to store loss values\n",
        "discriminator_losses = []\n",
        "generator_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(num_batches):\n",
        "        # ... Rest of the code ...\n",
        "\n",
        "        # Store the loss values\n",
        "        discriminator_losses.append(discriminator_loss)\n",
        "        generator_losses.append(generator_loss)\n",
        "\n",
        "    # ... Rest of the code ...\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.plot(range(len(discriminator_losses)), discriminator_losses, label='Discriminator Loss')\n",
        "plt.plot(range(len(generator_losses)), generator_losses, label='Generator Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualize generated images\n",
        "num_samples = 10\n",
        "noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "labels = np.eye(num_classes)[np.arange(num_samples) % num_classes]\n",
        "generated_images = generator.predict([noise, labels])\n",
        "\n",
        "fig, axes = plt.subplots(1, num_samples, figsize=(10, 1))\n",
        "for i in range(num_samples):\n",
        "    axes[i].imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SzGvcKSiDZal",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "9701d13d-e654-47bd-eb1f-f38d7b74875a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# Set the path to the directory containing the images\n",
        "image_dir = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# Set the input image size for InceptionV3\n",
        "input_size = (224, 224)\n",
        "\n",
        "# Load the pre-trained InceptionV3 model\n",
        "input_tensor = Input(shape=(input_size[0], input_size[1], 3))\n",
        "inception_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "\n",
        "# Create a new model with InceptionV3 as the base and outputting the activation features\n",
        "inception_features_model = Model(inputs=inception_model.input, outputs=inception_model.get_layer('avg_pool').output)\n",
        "\n",
        "# Preprocess the images for InceptionV3\n",
        "def preprocess_images(images):\n",
        "    preprocessed_images = []\n",
        "    for image in images:\n",
        "        image = tf.image.resize(image, input_size)\n",
        "        image = preprocess_input(image)\n",
        "        preprocessed_images.append(image)\n",
        "    return np.array(preprocessed_images)\n",
        "\n",
        "# Load and preprocess the images\n",
        "image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir)]\n",
        "images = [plt.imread(image_path) for image_path in image_paths]\n",
        "preprocessed_images = preprocess_images(images)\n",
        "\n",
        "# Extract features using InceptionV3\n",
        "features = inception_features_model.predict(preprocessed_images)\n",
        "\n",
        "# Print the shape of the features\n",
        "print(\"Features shape:\", features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "c64fNIaQKvWN",
        "outputId": "37d5cf08-40a4-4b6c-dca0-edf8d7c3ef52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3328a11bbcb7>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Create a new model with InceptionV3 as the base and outputting the activation features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0minception_features_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minception_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minception_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg_pool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Preprocess the images for InceptionV3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   3288\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   3291\u001b[0m                 \u001b[0;34mf\"No such layer: {name}. Existing layers are: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0;34mf\"{list(layer.name for layer in self.layers)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No such layer: avg_pool. Existing layers are: ['input_10', 'conv2d_188', 'batch_normalization_190', 'activation_188', 'conv2d_189', 'batch_normalization_191', 'activation_189', 'conv2d_190', 'batch_normalization_192', 'activation_190', 'max_pooling2d_8', 'conv2d_191', 'batch_normalization_193', 'activation_191', 'conv2d_192', 'batch_normalization_194', 'activation_192', 'max_pooling2d_9', 'conv2d_196', 'batch_normalization_198', 'activation_196', 'conv2d_194', 'conv2d_197', 'batch_normalization_196', 'batch_normalization_199', 'activation_194', 'activation_197', 'average_pooling2d_18', 'conv2d_193', 'conv2d_195', 'conv2d_198', 'conv2d_199', 'batch_normalization_195', 'batch_normalization_197', 'batch_normalization_200', 'batch_normalization_201', 'activation_193', 'activation_195', 'activation_198', 'activation_199', 'mixed0', 'conv2d_203', 'batch_normalization_205', 'activation_203', 'conv2d_201', 'conv2d_204', 'batch_normalization_203', 'batch_normalization_206', 'activation_201', 'activation_204', 'average_pooling2d_19', 'conv2d_200', 'conv2d_202', 'conv2d_205', 'conv2d_206', 'batch_normalization_202', 'batch_normalization_204', 'batch_normalization_207', 'batch_normalization_208', 'activation_200', 'activation_202', 'activation_205', 'activation_206', 'mixed1', 'conv2d_210', 'batch_normalization_212', 'activation_210', 'conv2d_208', 'conv2d_211', 'batch_normalization_210', 'batch_normalization_213', 'activation_208', 'activation_211', 'average_po..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xY2dnXQrugk",
        "outputId": "29a66432-d6cb-4468-9858-f38475b19fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_282 (Conv2D)            (None, 112, 112, 64  9472        ['input_11[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_284 (Batch  (None, 112, 112, 64  256        ['conv2d_282[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " activation_282 (Activation)    (None, 112, 112, 64  0           ['batch_normalization_284[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 56, 56, 64)  0           ['activation_282[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_283 (Conv2D)            (None, 56, 56, 64)   4160        ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_285 (Batch  (None, 56, 56, 64)  256         ['conv2d_283[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_283 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_285[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_284 (Conv2D)            (None, 56, 56, 64)   36928       ['activation_283[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_286 (Batch  (None, 56, 56, 64)  256         ['conv2d_284[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_284 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_286[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_285 (Conv2D)            (None, 56, 56, 256)  16640       ['activation_284[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_286 (Conv2D)            (None, 56, 56, 256)  16640       ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_287 (Batch  (None, 56, 56, 256)  1024       ['conv2d_285[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_288 (Batch  (None, 56, 56, 256)  1024       ['conv2d_286[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 56, 56, 256)  0           ['batch_normalization_287[0][0]',\n",
            "                                                                  'batch_normalization_288[0][0]']\n",
            "                                                                                                  \n",
            " activation_285 (Activation)    (None, 56, 56, 256)  0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_287 (Conv2D)            (None, 56, 56, 64)   16448       ['activation_285[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_289 (Batch  (None, 56, 56, 64)  256         ['conv2d_287[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_286 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_289[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_288 (Conv2D)            (None, 56, 56, 64)   36928       ['activation_286[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_290 (Batch  (None, 56, 56, 64)  256         ['conv2d_288[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_287 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_290[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_289 (Conv2D)            (None, 56, 56, 256)  16640       ['activation_287[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_291 (Batch  (None, 56, 56, 256)  1024       ['conv2d_289[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 56, 56, 256)  0           ['batch_normalization_291[0][0]',\n",
            "                                                                  'activation_285[0][0]']         \n",
            "                                                                                                  \n",
            " activation_288 (Activation)    (None, 56, 56, 256)  0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_290 (Conv2D)            (None, 56, 56, 64)   16448       ['activation_288[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_292 (Batch  (None, 56, 56, 64)  256         ['conv2d_290[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_289 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_292[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_291 (Conv2D)            (None, 56, 56, 64)   36928       ['activation_289[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_293 (Batch  (None, 56, 56, 64)  256         ['conv2d_291[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_290 (Activation)    (None, 56, 56, 64)   0           ['batch_normalization_293[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_292 (Conv2D)            (None, 56, 56, 256)  16640       ['activation_290[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_294 (Batch  (None, 56, 56, 256)  1024       ['conv2d_292[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 56, 56, 256)  0           ['batch_normalization_294[0][0]',\n",
            "                                                                  'activation_288[0][0]']         \n",
            "                                                                                                  \n",
            " activation_291 (Activation)    (None, 56, 56, 256)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_293 (Conv2D)            (None, 28, 28, 128)  32896       ['activation_291[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_295 (Batch  (None, 28, 28, 128)  512        ['conv2d_293[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_292 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_295[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_294 (Conv2D)            (None, 28, 28, 128)  147584      ['activation_292[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_296 (Batch  (None, 28, 28, 128)  512        ['conv2d_294[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_293 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_296[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_295 (Conv2D)            (None, 28, 28, 512)  66048       ['activation_293[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_296 (Conv2D)            (None, 28, 28, 512)  131584      ['activation_291[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_297 (Batch  (None, 28, 28, 512)  2048       ['conv2d_295[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_298 (Batch  (None, 28, 28, 512)  2048       ['conv2d_296[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 28, 28, 512)  0           ['batch_normalization_297[0][0]',\n",
            "                                                                  'batch_normalization_298[0][0]']\n",
            "                                                                                                  \n",
            " activation_294 (Activation)    (None, 28, 28, 512)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_297 (Conv2D)            (None, 28, 28, 128)  65664       ['activation_294[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_299 (Batch  (None, 28, 28, 128)  512        ['conv2d_297[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_295 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_299[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_298 (Conv2D)            (None, 28, 28, 128)  147584      ['activation_295[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_300 (Batch  (None, 28, 28, 128)  512        ['conv2d_298[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_296 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_300[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_299 (Conv2D)            (None, 28, 28, 512)  66048       ['activation_296[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_301 (Batch  (None, 28, 28, 512)  2048       ['conv2d_299[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 28, 28, 512)  0           ['batch_normalization_301[0][0]',\n",
            "                                                                  'activation_294[0][0]']         \n",
            "                                                                                                  \n",
            " activation_297 (Activation)    (None, 28, 28, 512)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_300 (Conv2D)            (None, 28, 28, 128)  65664       ['activation_297[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_302 (Batch  (None, 28, 28, 128)  512        ['conv2d_300[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_298 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_302[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_301 (Conv2D)            (None, 28, 28, 128)  147584      ['activation_298[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_303 (Batch  (None, 28, 28, 128)  512        ['conv2d_301[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_299 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_303[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_302 (Conv2D)            (None, 28, 28, 512)  66048       ['activation_299[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_304 (Batch  (None, 28, 28, 512)  2048       ['conv2d_302[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 28, 28, 512)  0           ['batch_normalization_304[0][0]',\n",
            "                                                                  'activation_297[0][0]']         \n",
            "                                                                                                  \n",
            " activation_300 (Activation)    (None, 28, 28, 512)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_303 (Conv2D)            (None, 28, 28, 128)  65664       ['activation_300[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_305 (Batch  (None, 28, 28, 128)  512        ['conv2d_303[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_301 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_305[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_304 (Conv2D)            (None, 28, 28, 128)  147584      ['activation_301[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_306 (Batch  (None, 28, 28, 128)  512        ['conv2d_304[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_302 (Activation)    (None, 28, 28, 128)  0           ['batch_normalization_306[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_305 (Conv2D)            (None, 28, 28, 512)  66048       ['activation_302[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_307 (Batch  (None, 28, 28, 512)  2048       ['conv2d_305[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 28, 28, 512)  0           ['batch_normalization_307[0][0]',\n",
            "                                                                  'activation_300[0][0]']         \n",
            "                                                                                                  \n",
            " activation_303 (Activation)    (None, 28, 28, 512)  0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_306 (Conv2D)            (None, 14, 14, 256)  131328      ['activation_303[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_308 (Batch  (None, 14, 14, 256)  1024       ['conv2d_306[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_304 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_308[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_307 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_304[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_309 (Batch  (None, 14, 14, 256)  1024       ['conv2d_307[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_305 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_309[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_308 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_305[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_309 (Conv2D)            (None, 14, 14, 1024  525312      ['activation_303[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_310 (Batch  (None, 14, 14, 1024  4096       ['conv2d_308[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_311 (Batch  (None, 14, 14, 1024  4096       ['conv2d_309[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 14, 14, 1024  0           ['batch_normalization_310[0][0]',\n",
            "                                )                                 'batch_normalization_311[0][0]']\n",
            "                                                                                                  \n",
            " activation_306 (Activation)    (None, 14, 14, 1024  0           ['add_7[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_310 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_306[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_312 (Batch  (None, 14, 14, 256)  1024       ['conv2d_310[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_307 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_312[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_311 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_307[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_313 (Batch  (None, 14, 14, 256)  1024       ['conv2d_311[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_308 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_313[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_312 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_308[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_314 (Batch  (None, 14, 14, 1024  4096       ['conv2d_312[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 14, 14, 1024  0           ['batch_normalization_314[0][0]',\n",
            "                                )                                 'activation_306[0][0]']         \n",
            "                                                                                                  \n",
            " activation_309 (Activation)    (None, 14, 14, 1024  0           ['add_8[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_313 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_309[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_315 (Batch  (None, 14, 14, 256)  1024       ['conv2d_313[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_310 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_315[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_314 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_310[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_316 (Batch  (None, 14, 14, 256)  1024       ['conv2d_314[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_311 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_316[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_315 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_311[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_317 (Batch  (None, 14, 14, 1024  4096       ['conv2d_315[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 14, 14, 1024  0           ['batch_normalization_317[0][0]',\n",
            "                                )                                 'activation_309[0][0]']         \n",
            "                                                                                                  \n",
            " activation_312 (Activation)    (None, 14, 14, 1024  0           ['add_9[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_316 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_312[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_318 (Batch  (None, 14, 14, 256)  1024       ['conv2d_316[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_313 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_318[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_317 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_313[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_319 (Batch  (None, 14, 14, 256)  1024       ['conv2d_317[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_314 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_319[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_318 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_314[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_320 (Batch  (None, 14, 14, 1024  4096       ['conv2d_318[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_320[0][0]',\n",
            "                                )                                 'activation_312[0][0]']         \n",
            "                                                                                                  \n",
            " activation_315 (Activation)    (None, 14, 14, 1024  0           ['add_10[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_319 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_315[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_321 (Batch  (None, 14, 14, 256)  1024       ['conv2d_319[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_316 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_321[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_320 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_316[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_322 (Batch  (None, 14, 14, 256)  1024       ['conv2d_320[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_317 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_322[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_321 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_317[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_323 (Batch  (None, 14, 14, 1024  4096       ['conv2d_321[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_323[0][0]',\n",
            "                                )                                 'activation_315[0][0]']         \n",
            "                                                                                                  \n",
            " activation_318 (Activation)    (None, 14, 14, 1024  0           ['add_11[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_322 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_318[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_324 (Batch  (None, 14, 14, 256)  1024       ['conv2d_322[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_319 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_324[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_323 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_319[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_325 (Batch  (None, 14, 14, 256)  1024       ['conv2d_323[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_320 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_325[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_324 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_320[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_326 (Batch  (None, 14, 14, 1024  4096       ['conv2d_324[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_326[0][0]',\n",
            "                                )                                 'activation_318[0][0]']         \n",
            "                                                                                                  \n",
            " activation_321 (Activation)    (None, 14, 14, 1024  0           ['add_12[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_325 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_321[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_327 (Batch  (None, 14, 14, 256)  1024       ['conv2d_325[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_322 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_327[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_326 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_322[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_328 (Batch  (None, 14, 14, 256)  1024       ['conv2d_326[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_323 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_328[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_327 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_323[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_329 (Batch  (None, 14, 14, 1024  4096       ['conv2d_327[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_329[0][0]',\n",
            "                                )                                 'activation_321[0][0]']         \n",
            "                                                                                                  \n",
            " activation_324 (Activation)    (None, 14, 14, 1024  0           ['add_13[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_328 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_324[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_330 (Batch  (None, 14, 14, 256)  1024       ['conv2d_328[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_325 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_330[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_329 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_325[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_331 (Batch  (None, 14, 14, 256)  1024       ['conv2d_329[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_326 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_331[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_330 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_326[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_332 (Batch  (None, 14, 14, 1024  4096       ['conv2d_330[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_332[0][0]',\n",
            "                                )                                 'activation_324[0][0]']         \n",
            "                                                                                                  \n",
            " activation_327 (Activation)    (None, 14, 14, 1024  0           ['add_14[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_331 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_327[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_333 (Batch  (None, 14, 14, 256)  1024       ['conv2d_331[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_328 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_333[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_332 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_328[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_334 (Batch  (None, 14, 14, 256)  1024       ['conv2d_332[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_329 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_334[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_333 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_329[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_335 (Batch  (None, 14, 14, 1024  4096       ['conv2d_333[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_335[0][0]',\n",
            "                                )                                 'activation_327[0][0]']         \n",
            "                                                                                                  \n",
            " activation_330 (Activation)    (None, 14, 14, 1024  0           ['add_15[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_334 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_330[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_336 (Batch  (None, 14, 14, 256)  1024       ['conv2d_334[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_331 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_336[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_335 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_331[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_337 (Batch  (None, 14, 14, 256)  1024       ['conv2d_335[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_332 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_337[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_336 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_332[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_338 (Batch  (None, 14, 14, 1024  4096       ['conv2d_336[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_338[0][0]',\n",
            "                                )                                 'activation_330[0][0]']         \n",
            "                                                                                                  \n",
            " activation_333 (Activation)    (None, 14, 14, 1024  0           ['add_16[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_337 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_333[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_339 (Batch  (None, 14, 14, 256)  1024       ['conv2d_337[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_334 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_339[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_338 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_334[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_340 (Batch  (None, 14, 14, 256)  1024       ['conv2d_338[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_335 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_340[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_339 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_335[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_341 (Batch  (None, 14, 14, 1024  4096       ['conv2d_339[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_341[0][0]',\n",
            "                                )                                 'activation_333[0][0]']         \n",
            "                                                                                                  \n",
            " activation_336 (Activation)    (None, 14, 14, 1024  0           ['add_17[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_340 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_336[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_342 (Batch  (None, 14, 14, 256)  1024       ['conv2d_340[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_337 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_342[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_341 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_337[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_343 (Batch  (None, 14, 14, 256)  1024       ['conv2d_341[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_338 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_343[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_342 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_338[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_344 (Batch  (None, 14, 14, 1024  4096       ['conv2d_342[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_344[0][0]',\n",
            "                                )                                 'activation_336[0][0]']         \n",
            "                                                                                                  \n",
            " activation_339 (Activation)    (None, 14, 14, 1024  0           ['add_18[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_343 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_339[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_345 (Batch  (None, 14, 14, 256)  1024       ['conv2d_343[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_340 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_345[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_344 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_340[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_346 (Batch  (None, 14, 14, 256)  1024       ['conv2d_344[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_341 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_346[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_345 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_341[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_347 (Batch  (None, 14, 14, 1024  4096       ['conv2d_345[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_347[0][0]',\n",
            "                                )                                 'activation_339[0][0]']         \n",
            "                                                                                                  \n",
            " activation_342 (Activation)    (None, 14, 14, 1024  0           ['add_19[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_346 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_342[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_348 (Batch  (None, 14, 14, 256)  1024       ['conv2d_346[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_343 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_348[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_347 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_343[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_349 (Batch  (None, 14, 14, 256)  1024       ['conv2d_347[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_344 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_349[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_348 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_344[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_350 (Batch  (None, 14, 14, 1024  4096       ['conv2d_348[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_350[0][0]',\n",
            "                                )                                 'activation_342[0][0]']         \n",
            "                                                                                                  \n",
            " activation_345 (Activation)    (None, 14, 14, 1024  0           ['add_20[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_349 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_345[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_351 (Batch  (None, 14, 14, 256)  1024       ['conv2d_349[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_346 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_351[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_350 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_346[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_352 (Batch  (None, 14, 14, 256)  1024       ['conv2d_350[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_347 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_352[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_351 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_347[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_353 (Batch  (None, 14, 14, 1024  4096       ['conv2d_351[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_353[0][0]',\n",
            "                                )                                 'activation_345[0][0]']         \n",
            "                                                                                                  \n",
            " activation_348 (Activation)    (None, 14, 14, 1024  0           ['add_21[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_352 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_348[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_354 (Batch  (None, 14, 14, 256)  1024       ['conv2d_352[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_349 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_354[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_353 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_349[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_355 (Batch  (None, 14, 14, 256)  1024       ['conv2d_353[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_350 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_355[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_354 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_350[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_356 (Batch  (None, 14, 14, 1024  4096       ['conv2d_354[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_356[0][0]',\n",
            "                                )                                 'activation_348[0][0]']         \n",
            "                                                                                                  \n",
            " activation_351 (Activation)    (None, 14, 14, 1024  0           ['add_22[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_355 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_351[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_357 (Batch  (None, 14, 14, 256)  1024       ['conv2d_355[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_352 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_357[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_356 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_352[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_358 (Batch  (None, 14, 14, 256)  1024       ['conv2d_356[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_353 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_358[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_357 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_353[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_359 (Batch  (None, 14, 14, 1024  4096       ['conv2d_357[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_359[0][0]',\n",
            "                                )                                 'activation_351[0][0]']         \n",
            "                                                                                                  \n",
            " activation_354 (Activation)    (None, 14, 14, 1024  0           ['add_23[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_358 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_354[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_360 (Batch  (None, 14, 14, 256)  1024       ['conv2d_358[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_355 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_360[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_359 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_355[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_361 (Batch  (None, 14, 14, 256)  1024       ['conv2d_359[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_356 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_361[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_360 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_356[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_362 (Batch  (None, 14, 14, 1024  4096       ['conv2d_360[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_362[0][0]',\n",
            "                                )                                 'activation_354[0][0]']         \n",
            "                                                                                                  \n",
            " activation_357 (Activation)    (None, 14, 14, 1024  0           ['add_24[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_361 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_357[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_363 (Batch  (None, 14, 14, 256)  1024       ['conv2d_361[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_358 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_363[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_362 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_358[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_364 (Batch  (None, 14, 14, 256)  1024       ['conv2d_362[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_359 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_364[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_363 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_359[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_365 (Batch  (None, 14, 14, 1024  4096       ['conv2d_363[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_365[0][0]',\n",
            "                                )                                 'activation_357[0][0]']         \n",
            "                                                                                                  \n",
            " activation_360 (Activation)    (None, 14, 14, 1024  0           ['add_25[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_364 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_360[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_366 (Batch  (None, 14, 14, 256)  1024       ['conv2d_364[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_361 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_366[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_365 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_361[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_367 (Batch  (None, 14, 14, 256)  1024       ['conv2d_365[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_362 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_367[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_366 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_362[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_368 (Batch  (None, 14, 14, 1024  4096       ['conv2d_366[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_368[0][0]',\n",
            "                                )                                 'activation_360[0][0]']         \n",
            "                                                                                                  \n",
            " activation_363 (Activation)    (None, 14, 14, 1024  0           ['add_26[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_367 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_363[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_369 (Batch  (None, 14, 14, 256)  1024       ['conv2d_367[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_364 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_369[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_368 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_364[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_370 (Batch  (None, 14, 14, 256)  1024       ['conv2d_368[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_365 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_370[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_369 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_365[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_371 (Batch  (None, 14, 14, 1024  4096       ['conv2d_369[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_27 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_371[0][0]',\n",
            "                                )                                 'activation_363[0][0]']         \n",
            "                                                                                                  \n",
            " activation_366 (Activation)    (None, 14, 14, 1024  0           ['add_27[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_370 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_366[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_372 (Batch  (None, 14, 14, 256)  1024       ['conv2d_370[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_367 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_372[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_371 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_367[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_373 (Batch  (None, 14, 14, 256)  1024       ['conv2d_371[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_368 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_373[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_372 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_368[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_374 (Batch  (None, 14, 14, 1024  4096       ['conv2d_372[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_28 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_374[0][0]',\n",
            "                                )                                 'activation_366[0][0]']         \n",
            "                                                                                                  \n",
            " activation_369 (Activation)    (None, 14, 14, 1024  0           ['add_28[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_373 (Conv2D)            (None, 14, 14, 256)  262400      ['activation_369[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_375 (Batch  (None, 14, 14, 256)  1024       ['conv2d_373[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_370 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_375[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_374 (Conv2D)            (None, 14, 14, 256)  590080      ['activation_370[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_376 (Batch  (None, 14, 14, 256)  1024       ['conv2d_374[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_371 (Activation)    (None, 14, 14, 256)  0           ['batch_normalization_376[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_375 (Conv2D)            (None, 14, 14, 1024  263168      ['activation_371[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_377 (Batch  (None, 14, 14, 1024  4096       ['conv2d_375[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_29 (Add)                   (None, 14, 14, 1024  0           ['batch_normalization_377[0][0]',\n",
            "                                )                                 'activation_369[0][0]']         \n",
            "                                                                                                  \n",
            " activation_372 (Activation)    (None, 14, 14, 1024  0           ['add_29[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_376 (Conv2D)            (None, 7, 7, 512)    524800      ['activation_372[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_378 (Batch  (None, 7, 7, 512)   2048        ['conv2d_376[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_373 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_378[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_377 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_373[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_379 (Batch  (None, 7, 7, 512)   2048        ['conv2d_377[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_374 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_379[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_378 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_374[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_379 (Conv2D)            (None, 7, 7, 2048)   2099200     ['activation_372[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_380 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_378[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_381 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_379[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_30 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_380[0][0]',\n",
            "                                                                  'batch_normalization_381[0][0]']\n",
            "                                                                                                  \n",
            " activation_375 (Activation)    (None, 7, 7, 2048)   0           ['add_30[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_380 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_375[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_382 (Batch  (None, 7, 7, 512)   2048        ['conv2d_380[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_376 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_382[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_381 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_376[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_383 (Batch  (None, 7, 7, 512)   2048        ['conv2d_381[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_377 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_383[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_382 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_377[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_384 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_382[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_31 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_384[0][0]',\n",
            "                                                                  'activation_375[0][0]']         \n",
            "                                                                                                  \n",
            " activation_378 (Activation)    (None, 7, 7, 2048)   0           ['add_31[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_383 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_378[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_385 (Batch  (None, 7, 7, 512)   2048        ['conv2d_383[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_379 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_385[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_384 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_379[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_386 (Batch  (None, 7, 7, 512)   2048        ['conv2d_384[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_380 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_386[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_385 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_380[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_387 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_385[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_32 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_387[0][0]',\n",
            "                                                                  'activation_378[0][0]']         \n",
            "                                                                                                  \n",
            " activation_381 (Activation)    (None, 7, 7, 2048)   0           ['add_32[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_386 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_381[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_388 (Batch  (None, 7, 7, 512)   2048        ['conv2d_386[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_382 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_388[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_387 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_382[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_389 (Batch  (None, 7, 7, 512)   2048        ['conv2d_387[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_383 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_389[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_388 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_383[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_390 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_388[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_33 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_390[0][0]',\n",
            "                                                                  'activation_381[0][0]']         \n",
            "                                                                                                  \n",
            " activation_384 (Activation)    (None, 7, 7, 2048)   0           ['add_33[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_389 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_384[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_391 (Batch  (None, 7, 7, 512)   2048        ['conv2d_389[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_385 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_391[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_390 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_385[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_392 (Batch  (None, 7, 7, 512)   2048        ['conv2d_390[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_386 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_392[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_391 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_386[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_393 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_391[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_34 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_393[0][0]',\n",
            "                                                                  'activation_384[0][0]']         \n",
            "                                                                                                  \n",
            " activation_387 (Activation)    (None, 7, 7, 2048)   0           ['add_34[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_392 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_387[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_394 (Batch  (None, 7, 7, 512)   2048        ['conv2d_392[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_388 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_394[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_393 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_388[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_395 (Batch  (None, 7, 7, 512)   2048        ['conv2d_393[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_389 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_395[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_394 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_389[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_396 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_394[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_35 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_396[0][0]',\n",
            "                                                                  'activation_387[0][0]']         \n",
            "                                                                                                  \n",
            " activation_390 (Activation)    (None, 7, 7, 2048)   0           ['add_35[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_395 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_390[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_397 (Batch  (None, 7, 7, 512)   2048        ['conv2d_395[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_391 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_397[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_396 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_391[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_398 (Batch  (None, 7, 7, 512)   2048        ['conv2d_396[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_392 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_398[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_397 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_392[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_399 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_397[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_36 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_399[0][0]',\n",
            "                                                                  'activation_390[0][0]']         \n",
            "                                                                                                  \n",
            " activation_393 (Activation)    (None, 7, 7, 2048)   0           ['add_36[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_398 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_393[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_400 (Batch  (None, 7, 7, 512)   2048        ['conv2d_398[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_394 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_400[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_399 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_394[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_401 (Batch  (None, 7, 7, 512)   2048        ['conv2d_399[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_395 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_401[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_400 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_395[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_402 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_400[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_37 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_402[0][0]',\n",
            "                                                                  'activation_393[0][0]']         \n",
            "                                                                                                  \n",
            " activation_396 (Activation)    (None, 7, 7, 2048)   0           ['add_37[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_401 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_396[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_403 (Batch  (None, 7, 7, 512)   2048        ['conv2d_401[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_397 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_403[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_402 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_397[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_404 (Batch  (None, 7, 7, 512)   2048        ['conv2d_402[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_398 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_404[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_403 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_398[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_405 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_403[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_38 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_405[0][0]',\n",
            "                                                                  'activation_396[0][0]']         \n",
            "                                                                                                  \n",
            " activation_399 (Activation)    (None, 7, 7, 2048)   0           ['add_38[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_404 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_399[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_406 (Batch  (None, 7, 7, 512)   2048        ['conv2d_404[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_400 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_406[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_405 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_400[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_407 (Batch  (None, 7, 7, 512)   2048        ['conv2d_405[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_401 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_407[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_406 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_401[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_408 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_406[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_39 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_408[0][0]',\n",
            "                                                                  'activation_399[0][0]']         \n",
            "                                                                                                  \n",
            " activation_402 (Activation)    (None, 7, 7, 2048)   0           ['add_39[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_407 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_402[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_409 (Batch  (None, 7, 7, 512)   2048        ['conv2d_407[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_403 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_409[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_408 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_403[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_410 (Batch  (None, 7, 7, 512)   2048        ['conv2d_408[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_404 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_410[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_409 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_404[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_411 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_409[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_40 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_411[0][0]',\n",
            "                                                                  'activation_402[0][0]']         \n",
            "                                                                                                  \n",
            " activation_405 (Activation)    (None, 7, 7, 2048)   0           ['add_40[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_410 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_405[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_412 (Batch  (None, 7, 7, 512)   2048        ['conv2d_410[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_406 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_412[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_411 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_406[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_413 (Batch  (None, 7, 7, 512)   2048        ['conv2d_411[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_407 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_413[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_412 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_407[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_414 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_412[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_41 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_414[0][0]',\n",
            "                                                                  'activation_405[0][0]']         \n",
            "                                                                                                  \n",
            " activation_408 (Activation)    (None, 7, 7, 2048)   0           ['add_41[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_413 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_408[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_415 (Batch  (None, 7, 7, 512)   2048        ['conv2d_413[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_409 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_415[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_414 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_409[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_416 (Batch  (None, 7, 7, 512)   2048        ['conv2d_414[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_410 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_416[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_415 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_410[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_417 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_415[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_42 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_417[0][0]',\n",
            "                                                                  'activation_408[0][0]']         \n",
            "                                                                                                  \n",
            " activation_411 (Activation)    (None, 7, 7, 2048)   0           ['add_42[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_416 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_411[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_418 (Batch  (None, 7, 7, 512)   2048        ['conv2d_416[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_412 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_418[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_417 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_412[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_419 (Batch  (None, 7, 7, 512)   2048        ['conv2d_417[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_413 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_419[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_418 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_413[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_420 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_418[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_43 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_420[0][0]',\n",
            "                                                                  'activation_411[0][0]']         \n",
            "                                                                                                  \n",
            " activation_414 (Activation)    (None, 7, 7, 2048)   0           ['add_43[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_419 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_414[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_421 (Batch  (None, 7, 7, 512)   2048        ['conv2d_419[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_415 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_421[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_420 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_415[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_422 (Batch  (None, 7, 7, 512)   2048        ['conv2d_420[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_416 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_422[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_421 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_416[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_423 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_421[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_44 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_423[0][0]',\n",
            "                                                                  'activation_414[0][0]']         \n",
            "                                                                                                  \n",
            " activation_417 (Activation)    (None, 7, 7, 2048)   0           ['add_44[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_422 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_417[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_424 (Batch  (None, 7, 7, 512)   2048        ['conv2d_422[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_418 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_424[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_423 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_418[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_425 (Batch  (None, 7, 7, 512)   2048        ['conv2d_423[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_419 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_425[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_424 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_419[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_426 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_424[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_45 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_426[0][0]',\n",
            "                                                                  'activation_417[0][0]']         \n",
            "                                                                                                  \n",
            " activation_420 (Activation)    (None, 7, 7, 2048)   0           ['add_45[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_425 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_420[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_427 (Batch  (None, 7, 7, 512)   2048        ['conv2d_425[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_421 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_427[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_426 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_421[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_428 (Batch  (None, 7, 7, 512)   2048        ['conv2d_426[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_422 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_428[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_427 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_422[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_429 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_427[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_46 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_429[0][0]',\n",
            "                                                                  'activation_420[0][0]']         \n",
            "                                                                                                  \n",
            " activation_423 (Activation)    (None, 7, 7, 2048)   0           ['add_46[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_428 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_423[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_430 (Batch  (None, 7, 7, 512)   2048        ['conv2d_428[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_424 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_430[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_429 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_424[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_431 (Batch  (None, 7, 7, 512)   2048        ['conv2d_429[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_425 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_431[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_430 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_425[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_432 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_430[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_47 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_432[0][0]',\n",
            "                                                                  'activation_423[0][0]']         \n",
            "                                                                                                  \n",
            " activation_426 (Activation)    (None, 7, 7, 2048)   0           ['add_47[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_431 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_426[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_433 (Batch  (None, 7, 7, 512)   2048        ['conv2d_431[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_427 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_433[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_432 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_427[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_434 (Batch  (None, 7, 7, 512)   2048        ['conv2d_432[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_428 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_434[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_433 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_428[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_435 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_433[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_48 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_435[0][0]',\n",
            "                                                                  'activation_426[0][0]']         \n",
            "                                                                                                  \n",
            " activation_429 (Activation)    (None, 7, 7, 2048)   0           ['add_48[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_434 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_429[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_436 (Batch  (None, 7, 7, 512)   2048        ['conv2d_434[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_430 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_436[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_435 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_430[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_437 (Batch  (None, 7, 7, 512)   2048        ['conv2d_435[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_431 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_437[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_436 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_431[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_438 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_436[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_49 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_438[0][0]',\n",
            "                                                                  'activation_429[0][0]']         \n",
            "                                                                                                  \n",
            " activation_432 (Activation)    (None, 7, 7, 2048)   0           ['add_49[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_437 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_432[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_439 (Batch  (None, 7, 7, 512)   2048        ['conv2d_437[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_433 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_439[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_438 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_433[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_440 (Batch  (None, 7, 7, 512)   2048        ['conv2d_438[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_434 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_440[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_439 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_434[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_441 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_439[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_50 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_441[0][0]',\n",
            "                                                                  'activation_432[0][0]']         \n",
            "                                                                                                  \n",
            " activation_435 (Activation)    (None, 7, 7, 2048)   0           ['add_50[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_440 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_435[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_442 (Batch  (None, 7, 7, 512)   2048        ['conv2d_440[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_436 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_442[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_441 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_436[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_443 (Batch  (None, 7, 7, 512)   2048        ['conv2d_441[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_437 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_443[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_442 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_437[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_444 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_442[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_444[0][0]',\n",
            "                                                                  'activation_435[0][0]']         \n",
            "                                                                                                  \n",
            " activation_438 (Activation)    (None, 7, 7, 2048)   0           ['add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_443 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_438[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_445 (Batch  (None, 7, 7, 512)   2048        ['conv2d_443[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_439 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_445[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_444 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_439[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_446 (Batch  (None, 7, 7, 512)   2048        ['conv2d_444[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_440 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_446[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_445 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_440[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_447 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_445[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_447[0][0]',\n",
            "                                                                  'activation_438[0][0]']         \n",
            "                                                                                                  \n",
            " activation_441 (Activation)    (None, 7, 7, 2048)   0           ['add_52[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_446 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_441[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_448 (Batch  (None, 7, 7, 512)   2048        ['conv2d_446[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_442 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_448[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_447 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_442[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_449 (Batch  (None, 7, 7, 512)   2048        ['conv2d_447[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_443 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_449[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_448 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_443[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_450 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_448[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_450[0][0]',\n",
            "                                                                  'activation_441[0][0]']         \n",
            "                                                                                                  \n",
            " activation_444 (Activation)    (None, 7, 7, 2048)   0           ['add_53[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_449 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_444[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_451 (Batch  (None, 7, 7, 512)   2048        ['conv2d_449[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_445 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_451[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_450 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_445[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_452 (Batch  (None, 7, 7, 512)   2048        ['conv2d_450[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_446 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_452[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_451 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_446[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_453 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_451[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_453[0][0]',\n",
            "                                                                  'activation_444[0][0]']         \n",
            "                                                                                                  \n",
            " activation_447 (Activation)    (None, 7, 7, 2048)   0           ['add_54[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_452 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_447[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_454 (Batch  (None, 7, 7, 512)   2048        ['conv2d_452[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_448 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_454[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_453 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_448[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_455 (Batch  (None, 7, 7, 512)   2048        ['conv2d_453[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_449 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_455[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_454 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_449[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_456 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_454[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_456[0][0]',\n",
            "                                                                  'activation_447[0][0]']         \n",
            "                                                                                                  \n",
            " activation_450 (Activation)    (None, 7, 7, 2048)   0           ['add_55[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_455 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_450[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_457 (Batch  (None, 7, 7, 512)   2048        ['conv2d_455[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_451 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_457[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_456 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_451[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_458 (Batch  (None, 7, 7, 512)   2048        ['conv2d_456[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_452 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_458[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_457 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_452[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_459 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_457[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_459[0][0]',\n",
            "                                                                  'activation_450[0][0]']         \n",
            "                                                                                                  \n",
            " activation_453 (Activation)    (None, 7, 7, 2048)   0           ['add_56[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_458 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_453[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_460 (Batch  (None, 7, 7, 512)   2048        ['conv2d_458[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_454 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_460[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_459 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_454[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_461 (Batch  (None, 7, 7, 512)   2048        ['conv2d_459[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_455 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_461[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_460 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_455[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_462 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_460[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_462[0][0]',\n",
            "                                                                  'activation_453[0][0]']         \n",
            "                                                                                                  \n",
            " activation_456 (Activation)    (None, 7, 7, 2048)   0           ['add_57[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_461 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_456[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_463 (Batch  (None, 7, 7, 512)   2048        ['conv2d_461[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_457 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_463[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_462 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_457[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_464 (Batch  (None, 7, 7, 512)   2048        ['conv2d_462[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_458 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_464[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_463 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_458[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_465 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_463[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_58 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_465[0][0]',\n",
            "                                                                  'activation_456[0][0]']         \n",
            "                                                                                                  \n",
            " activation_459 (Activation)    (None, 7, 7, 2048)   0           ['add_58[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_464 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_459[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_466 (Batch  (None, 7, 7, 512)   2048        ['conv2d_464[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_460 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_466[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_465 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_460[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_467 (Batch  (None, 7, 7, 512)   2048        ['conv2d_465[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_461 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_467[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_466 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_461[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_468 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_466[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_59 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_468[0][0]',\n",
            "                                                                  'activation_459[0][0]']         \n",
            "                                                                                                  \n",
            " activation_462 (Activation)    (None, 7, 7, 2048)   0           ['add_59[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_467 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_462[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_469 (Batch  (None, 7, 7, 512)   2048        ['conv2d_467[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_463 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_469[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_468 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_463[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_470 (Batch  (None, 7, 7, 512)   2048        ['conv2d_468[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_464 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_470[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_469 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_464[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_471 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_469[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_471[0][0]',\n",
            "                                                                  'activation_462[0][0]']         \n",
            "                                                                                                  \n",
            " activation_465 (Activation)    (None, 7, 7, 2048)   0           ['add_60[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_470 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_465[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_472 (Batch  (None, 7, 7, 512)   2048        ['conv2d_470[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_466 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_472[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_471 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_466[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_473 (Batch  (None, 7, 7, 512)   2048        ['conv2d_471[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_467 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_473[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_472 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_467[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_474 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_472[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_474[0][0]',\n",
            "                                                                  'activation_465[0][0]']         \n",
            "                                                                                                  \n",
            " activation_468 (Activation)    (None, 7, 7, 2048)   0           ['add_61[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_473 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_468[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_475 (Batch  (None, 7, 7, 512)   2048        ['conv2d_473[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_469 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_475[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_474 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_469[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_476 (Batch  (None, 7, 7, 512)   2048        ['conv2d_474[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_470 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_476[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_475 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_470[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_477 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_475[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_477[0][0]',\n",
            "                                                                  'activation_468[0][0]']         \n",
            "                                                                                                  \n",
            " activation_471 (Activation)    (None, 7, 7, 2048)   0           ['add_62[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_476 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_471[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_478 (Batch  (None, 7, 7, 512)   2048        ['conv2d_476[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_472 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_478[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_477 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_472[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_479 (Batch  (None, 7, 7, 512)   2048        ['conv2d_477[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_473 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_479[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_478 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_473[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_480 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_478[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_480[0][0]',\n",
            "                                                                  'activation_471[0][0]']         \n",
            "                                                                                                  \n",
            " activation_474 (Activation)    (None, 7, 7, 2048)   0           ['add_63[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_479 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_474[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_481 (Batch  (None, 7, 7, 512)   2048        ['conv2d_479[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_475 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_481[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_480 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_475[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_482 (Batch  (None, 7, 7, 512)   2048        ['conv2d_480[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_476 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_482[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_481 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_476[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_483 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_481[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_483[0][0]',\n",
            "                                                                  'activation_474[0][0]']         \n",
            "                                                                                                  \n",
            " activation_477 (Activation)    (None, 7, 7, 2048)   0           ['add_64[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_482 (Conv2D)            (None, 7, 7, 512)    1049088     ['activation_477[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_484 (Batch  (None, 7, 7, 512)   2048        ['conv2d_482[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_478 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_484[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_483 (Conv2D)            (None, 7, 7, 512)    2359808     ['activation_478[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_485 (Batch  (None, 7, 7, 512)   2048        ['conv2d_483[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_479 (Activation)    (None, 7, 7, 512)    0           ['batch_normalization_485[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_484 (Conv2D)            (None, 7, 7, 2048)   1050624     ['activation_479[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_486 (Batch  (None, 7, 7, 2048)  8192        ['conv2d_484[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 7, 7, 2048)   0           ['batch_normalization_486[0][0]',\n",
            "                                                                  'activation_477[0][0]']         \n",
            "                                                                                                  \n",
            " activation_480 (Activation)    (None, 7, 7, 2048)   0           ['add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['activation_480[0][0]']         \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 1000)         2049000     ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 192,276,840\n",
            "Trainable params: 191,968,744\n",
            "Non-trainable params: 308,096\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def resnet_block(inputs, filters, strides=1, use_projection=False):\n",
        "    # Identity mapping\n",
        "    identity = inputs\n",
        "\n",
        "    # First convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=1, strides=strides)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Second convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Third convolutional layer\n",
        "    x = layers.Conv2D(filters * 4, kernel_size=1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut to match dimensions\n",
        "    if use_projection:\n",
        "        identity = layers.Conv2D(filters * 4, kernel_size=1, strides=strides)(identity)\n",
        "        identity = layers.BatchNormalization()(identity)\n",
        "\n",
        "    # Add the identity and residual\n",
        "    x = layers.Add()([x, identity])\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_resnet200(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = resnet_block(x, filters=64, use_projection=True)\n",
        "    for _ in range(2):\n",
        "        x = resnet_block(x, filters=64)\n",
        "\n",
        "    x = resnet_block(x, filters=128, strides=2, use_projection=True)\n",
        "    for _ in range(3):\n",
        "        x = resnet_block(x, filters=128)\n",
        "\n",
        "    x = resnet_block(x, filters=256, strides=2, use_projection=True)\n",
        "    for _ in range(22):\n",
        "        x = resnet_block(x, filters=256)\n",
        "\n",
        "    x = resnet_block(x, filters=512, strides=2, use_projection=True)\n",
        "    for _ in range(35):\n",
        "        x = resnet_block(x, filters=512)\n",
        "\n",
        "    # Final layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 1000\n",
        "model = build_resnet200(input_shape, num_classes)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "CJ_TsuHCrSG8",
        "outputId": "64ababef-ab5c-4d8e-e193-7d0199f0793e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-5f3fe6dcb2ec>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    model = build_resnet200(input_shape, num_classes)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 11\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Set hyperparameters\n",
        "num_classes = 10\n",
        "input_shape = (224, 224, 3)\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Build ResNet-200 model\n",
        "def build_resnet200(input_shape, num_classes):\n",
        "    # Define ResNet-200 model architecture using TensorFlow or Keras\n",
        "    # ...\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# ...\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# ...\n",
        "\n",
        "# Build the ResNet-200 model\n",
        "model = build_resnet200(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_images, train_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "2nSiDh_juqlU",
        "outputId": "3ddaa78e-26e8-492a-b73c-2e75977de31b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-5b2eef1125ba>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Set hyperparameters\n",
        "num_classes = 10\n",
        "input_shape = (224, 224, 3)\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "# Build ResNet-200 model\n",
        "def build_resnet200(input_shape, num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    # ...\n",
        "\n",
        "    # Final layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Define ResNet-200 model architecture using TensorFlow or Keras\n",
        "model = build_resnet200(input_shape, num_classes)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# Assuming you have your own implementation to load and preprocess the dataset\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# Assuming you have your own implementation to split the dataset into training and testing sets\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_image_paths, train_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(test_image_paths, test_labels)\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_image_paths, test_labels)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn1XxIdPxmSs"
      },
      "outputs": [],
      "source": [
        "from keras.applications.resnet_v2 import ResNet200V2\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load ResNet200V2 model with pre-trained weights\n",
        "resnet200_model = ResNet200V2(weights='imagenet', include_top=False)\n",
        "\n",
        "# Add custom layers for classification\n",
        "x = resnet200_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create a new model with ResNet200V2 as the base and outputting the predictions\n",
        "model = Model(inputs=resnet200_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the pre-trained weights\n",
        "for layer in resnet200_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data using ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=(img_width, img_height),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                        target_size=(img_width, img_height),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        class_mode='categorical')\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_generator,\n",
        "          steps_per_epoch=train_generator.n // batch_size,\n",
        "          epochs=num_epochs,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=validation_generator.n // batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "JympRb7jxcLY",
        "outputId": "a70dfdad-6daa-45a1-bcd9-2bbf4fb462bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-65c8172558c2>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the model on your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1083\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1084\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), (<class 'list'> containing values of types {\"<class 'int'>\"})"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "# Load ResNet50 model with pre-trained weights\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers on top of the base model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on your dataset\n",
        "model.fit(train_image_paths, train_labels, epochs=10, validation_data=(test_image_paths, test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R9KpZZvyuDL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Build ResNet-50 model\n",
        "def build_resnet50(input_shape, num_classes):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Residual blocks\n",
        "    x = resnet_block(x, filters=64, strides=1, use_projection=True)\n",
        "    for _ in range(2):\n",
        "        x = resnet_block(x, filters=64)\n",
        "\n",
        "    x = resnet_block(x, filters=128, strides=2, use_projection=True)\n",
        "    for _ in range(3):\n",
        "        x = resnet_block(x, filters=128)\n",
        "\n",
        "    x = resnet_block(x, filters=256, strides=2, use_projection=True)\n",
        "    for _ in range(5):\n",
        "        x = resnet_block(x, filters=256)\n",
        "\n",
        "    x = resnet_block(x, filters=512, strides=2, use_projection=True)\n",
        "    for _ in range(3):\n",
        "        x = resnet_block(x, filters=512)\n",
        "\n",
        "    # Final layers\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "# Define the ResNet block\n",
        "def resnet_block(inputs, filters, strides=1, use_projection=False):\n",
        "    identity = inputs\n",
        "\n",
        "    # First convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=1, strides=strides)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Second convolutional layer\n",
        "    x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Third convolutional layer\n",
        "    x = layers.Conv2D(filters * 4, kernel_size=1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut to match dimensions\n",
        "    if use_projection:\n",
        "        identity = layers.Conv2D(filters * 4, kernel_size=1, strides=strides)(identity)\n",
        "        identity = layers.BatchNormalization()(identity)\n",
        "\n",
        "    # Add the identity and residual\n",
        "    x = layers.Add()([x, identity])\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "# Example usage\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 1000\n",
        "model = build_resnet50(input_shape, num_classes)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hkCbsGl90M5V",
        "outputId": "a13c341d-0b6e-4972-eebc-51ad4be3ebc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3723 images belonging to 8 classes.\n",
            "Found 3723 images belonging to 8 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e1779ac4660d>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Train the model on your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-23-e1779ac4660d>\", line 56, in <cell line: 56>\n      model.fit(train_generator, epochs=epochs, validation_data=test_generator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5565, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[32,5] labels_size=[32,8]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_1121235]"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the image data directories\n",
        "train_data_dir = '/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images'\n",
        "test_data_dir = '/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images'\n",
        "\n",
        "# Set the image size and batch size\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Set the number of classes\n",
        "num_classes = 5\n",
        "\n",
        "# Create ImageDataGenerators to load and preprocess the images\n",
        "train_data_generator = ImageDataGenerator(rescale=1.0/255)\n",
        "test_data_generator = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Load and preprocess the training images\n",
        "train_generator = train_data_generator.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# Load and preprocess the testing images\n",
        "test_generator = test_data_generator.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# Load ResNet50 model with pre-trained weights\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze all layers in the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers on top of the base model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on your dataset\n",
        "epochs = 1\n",
        "model.fit(train_generator, epochs=epochs, validation_data=test_generator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNO4Ofwz9ZSd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    BatchNormalization,\n",
        "    Activation,\n",
        "    Add,\n",
        "    UpSampling2D,\n",
        "    concatenate,\n",
        ")\n",
        "\n",
        "# Define DeepLabv3Plus\n",
        "def DeepLabv3Plus(input_shape, num_classes):\n",
        "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "\n",
        "    # Freeze base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # DeepLabv3+ architecture\n",
        "    x = base_model.output\n",
        "    low_level_features = base_model.get_layer('conv2_block3_out').output\n",
        "\n",
        "    x = Conv2D(256, (1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    low_level_features = Conv2D(48, (1, 1), padding='same')(low_level_features)  # Adjusted channels to 48\n",
        "    low_level_features = BatchNormalization()(low_level_features)\n",
        "    low_level_features = Activation('relu')(low_level_features)\n",
        "\n",
        "    x = UpSampling2D((4, 4))(x)\n",
        "    x = concatenate([x, low_level_features])\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    x = Conv2DTranspose(num_classes, (4, 4), strides=(2, 2), padding='same')(x)  # Adjusted transpose convolution size\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 10\n",
        "\n",
        "# Create DeepLabv3+ model\n",
        "deeplabv3plus_model = DeepLabv3Plus(input_shape, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "822dP03N9rCa"
      },
      "outputs": [],
      "source": [
        "def DeepLabv3Plus(input_shape, num_classes):\n",
        "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "\n",
        "    # Freeze base model\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # DeepLabv3+ architecture\n",
        "    x = base_model.output\n",
        "    low_level_features = base_model.get_layer('conv2_block3_out').output\n",
        "\n",
        "    x = Conv2D(256, (1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    low_level_features = Conv2D(48, (1, 1), padding='same')(low_level_features)\n",
        "    low_level_features = BatchNormalization()(low_level_features)\n",
        "    low_level_features = Activation('relu')(low_level_features)\n",
        "\n",
        "    # Adjust the spatial dimensions of low_level_features\n",
        "    low_level_features = UpSampling2D(size=(4, 4))(low_level_features)\n",
        "\n",
        "    # Crop low_level_features to match the spatial dimensions of x\n",
        "    target_size = x.shape[1:3]\n",
        "    cropping_dimensions = (\n",
        "        int((low_level_features.shape[1] - target_size[0]) // 2),\n",
        "        int((low_level_features.shape[2] - target_size[1]) // 2)\n",
        "    )\n",
        "    low_level_features = Cropping2D(cropping=cropping_dimensions)(low_level_features)\n",
        "\n",
        "    x = concatenate([x, low_level_features])\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    x = Conv2DTranspose(num_classes, (8, 8), strides=(4, 4), padding='same')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csxz7p93DG5T"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Define the DeepLabv3 model\n",
        "def DeepLabv3(input_shape, num_classes):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Define the DeepLabv3+ model\n",
        "def DeepLabv3Plus(input_shape, num_classes):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x_a = layers.AveragePooling2D(pool_size=(8, 8))(x)\n",
        "    x_a = layers.Conv2D(256, (1, 1), padding='same', activation='relu')(x_a)\n",
        "    x_a = layers.UpSampling2D(size=(8, 8), interpolation='bilinear')(x_a)\n",
        "    x_b = layers.Conv2D(48, (1, 1), padding='same', activation='relu')(x)\n",
        "    x_b = layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x_b)\n",
        "    x = layers.Concatenate()([x_a, x_b])\n",
        "    x = layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 21\n",
        "\n",
        "# Create DeepLabv3 model\n",
        "deeplabv3_model = DeepLabv3(input_shape, num_classes)\n",
        "\n",
        "# Create DeepLabv3+ model\n",
        "deeplabv3plus_model = DeepLabv3Plus(input_shape, num_classes)\n",
        "\n",
        "# Print model summaries\n",
        "deeplabv3_model.summary()\n",
        "deeplabv3plus_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak_-Jf1VFMI5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5sqeseZEdSe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from shutil import copyfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the DeepLabv3 model\n",
        "def DeepLabv3(input_shape, num_classes):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Define the DeepLabv3+ model\n",
        "def DeepLabv3Plus(input_shape, num_classes):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x_a = layers.AveragePooling2D(pool_size=(7, 7))(x)\n",
        "    x_a = layers.Conv2D(256, (1, 1), padding='same', activation='relu')(x_a)\n",
        "    x_a = layers.UpSampling2D(size=(7, 7), interpolation='bilinear')(x_a)\n",
        "    x_b = layers.Conv2D(48, (1, 1), padding='same', activation='relu')(x)\n",
        "    x_b = layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x_b)\n",
        "    x = layers.Concatenate()([x_a, x_b])\n",
        "    x = layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Set the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2\n",
        "\n",
        "# Set the path to the input folder containing images\n",
        "input_folder = '/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images'\n",
        "\n",
        "# Set the path to the output folder for training and testing data\n",
        "output_folder = '/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images'\n",
        "\n",
        "# Set the ratio for splitting the data into training and testing\n",
        "train_test_ratio = 0.8\n",
        "\n",
        "# Create output folder if it does not exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Get the list of image files in the input folder\n",
        "image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg') or f.endswith('.jpeg') or f.endswith('.png')]\n",
        "\n",
        "# Shuffle the image files\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Calculate the split index\n",
        "split_index = int(train_test_ratio * len(image_files))\n",
        "\n",
        "# Split the image files into training and testing sets\n",
        "train_files = image_files[:split_index]\n",
        "test_files = image_files[split_index:]\n",
        "\n",
        "# Move the training files to the training folder\n",
        "train_folder = os.path.join(output_folder, 'train')\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(input_folder, file)\n",
        "    dst_path = os.path.join(train_folder, file)\n",
        "    copyfile(src_path, dst_path)\n",
        "\n",
        "# Move the testing files to the testing folder\n",
        "test_folder = os.path.join(output_folder, 'test')\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(input_folder, file)\n",
        "    dst_path = os.path.join(test_folder, file)\n",
        "    copyfile(src_path, dst_path)\n",
        "\n",
        "# Create data generators\n",
        "# Create data generators for training and testing\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Set the batch size for training and testing\n",
        "batch_size = 32\n",
        "\n",
        "# Set the image size for resizing\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Generate training data from the training folder\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    train_folder,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Generate testing data from the testing folder\n",
        "test_data = test_data_gen.flow_from_directory(\n",
        "    test_folder,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Create DeepLabv3 model\n",
        "deeplabv3_model = DeepLabv3(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "deeplabv3_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "deeplabv3_model.fit(train_data, epochs=epochs)\n",
        "\n",
        "# Evaluate the model on testing data\n",
        "loss, accuracy = deeplabv3_model.evaluate(test_data)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Create DeepLabv3+ model\n",
        "deeplabv3plus_model = DeepLabv3Plus(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "deeplabv3plus_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "deeplabv3plus_model.fit(train_data, epochs=epochs)\n",
        "\n",
        "# Evaluate the model on testing data\n",
        "loss, accuracy = deeplabv3plus_model.evaluate(test_data)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxjX1jC1f9BT"
      },
      "outputs": [],
      "source": [
        "!pip install yolov3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsbjUSQogPXg",
        "outputId": "39cdf825-e8d0-4379-f6a1-cb4441bc51b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-05 19:10:43--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-05 19:10:43 (79.8 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n",
            "--2023-05-05 19:10:43--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  40.0MB/s    in 6.3s    \n",
            "\n",
            "2023-05-05 19:10:50 (37.5 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oObDWlX-EdwS",
        "outputId": "a534cb64-0607-41d5-f293-25b0bf0f349d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ca16b26110ec>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the output layer names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Iterate over the test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-ca16b26110ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the output layer names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Iterate over the test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the YOLOv3 network\n",
        "net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\")\n",
        "\n",
        "# Get the output layer names\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "# Iterate over the test images\n",
        "for image_path, label in zip(test_image_paths, test_labels):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Resize the image for YOLOv3 input size\n",
        "    resized_image = cv2.resize(image, (416, 416))\n",
        "\n",
        "    # Normalize the image\n",
        "    normalized_image = resized_image / 255.0\n",
        "\n",
        "    # Reshape the image to match the input shape of YOLOv3\n",
        "    input_blob = np.expand_dims(normalized_image, axis=0)\n",
        "\n",
        "    # Set the input blob for the network\n",
        "    net.setInput(input_blob)\n",
        "\n",
        "    # Perform forward pass and get the network outputs\n",
        "    layer_outputs = net.forward(output_layers)\n",
        "\n",
        "    # Process the output detections\n",
        "    # ...\n",
        "\n",
        "    # Display the image with bounding boxes\n",
        "    cv2.imshow(\"Object Detection\", image)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfzhUDKghO85",
        "outputId": "7e3deabe-056a-4f80-d27c-710f9334d7dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "93/93 [==============================] - 311s 3s/step - loss: 1.2528 - accuracy: 0.5509\n",
            "Epoch 2/10\n",
            "93/93 [==============================] - 310s 3s/step - loss: 1.0558 - accuracy: 0.6107\n",
            "Epoch 3/10\n",
            "93/93 [==============================] - 311s 3s/step - loss: 0.9947 - accuracy: 0.6446\n",
            "Epoch 4/10\n",
            "93/93 [==============================] - 311s 3s/step - loss: 0.9915 - accuracy: 0.6419\n",
            "Epoch 5/10\n",
            "93/93 [==============================] - 306s 3s/step - loss: 0.9731 - accuracy: 0.6565\n",
            "Epoch 6/10\n",
            "93/93 [==============================] - 306s 3s/step - loss: 0.9590 - accuracy: 0.6589\n",
            "Epoch 7/10\n",
            "93/93 [==============================] - 308s 3s/step - loss: 0.9407 - accuracy: 0.6619\n",
            "Epoch 8/10\n",
            "93/93 [==============================] - 310s 3s/step - loss: 0.9477 - accuracy: 0.6551\n",
            "Epoch 9/10\n",
            "93/93 [==============================] - 307s 3s/step - loss: 0.9323 - accuracy: 0.6650\n",
            "Epoch 10/10\n",
            "93/93 [==============================] - 305s 3s/step - loss: 0.9157 - accuracy: 0.6687\n",
            "94/94 [==============================] - 79s 837ms/step - loss: 125.0172 - accuracy: 0.6968\n",
            "Training Loss: 125.01722717285156\n",
            "Training Accuracy: 0.6967763304710388\n",
            "24/24 [==============================] - 19s 786ms/step - loss: 133.2346 - accuracy: 0.6658\n",
            "Test Loss: 133.23458862304688\n",
            "Test Accuracy: 0.6657717823982239\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "Predicted Label: 3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set the path to the main folder containing the subfolders\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# List all the subfolders within the main folder\n",
        "subfolders = [folder for folder in os.listdir(main_folder_path) if os.path.isdir(os.path.join(main_folder_path, folder))]\n",
        "\n",
        "# Initialize lists to store image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each subfolder and collect image paths and labels\n",
        "for label, folder in enumerate(subfolders):\n",
        "    folder_path = os.path.join(main_folder_path, folder)\n",
        "    file_paths = glob.glob(os.path.join(folder_path, \"*.png\"))  # Assuming the images are in PNG format\n",
        "\n",
        "    image_paths.extend(file_paths)\n",
        "    labels.extend([label] * len(file_paths))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set image dimensions\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Load and preprocess the images\n",
        "train_images = []\n",
        "for image_path in train_image_paths:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (img_width, img_height))\n",
        "    train_images.append(image)\n",
        "train_images = np.array(train_images)\n",
        "\n",
        "test_images = []\n",
        "for image_path in test_image_paths:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (img_width, img_height))\n",
        "    test_images.append(image)\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Convert labels to categorical\n",
        "num_classes = len(subfolders)\n",
        "train_labels_categorical = np.eye(num_classes)[train_labels]\n",
        "test_labels_categorical = np.eye(num_classes)[test_labels]\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "# model.fit(datagen.flow(train_images, train_labels_categorical, batch_size=batch_size),\n",
        "#           steps_per_epoch=len(train_images) // batch_size,\n",
        "#           epochs=epochs)\n",
        "# # Evaluate the model on the testing set\n",
        "# loss, accuracy = model.evaluate(test_images, test_labels_categorical)\n",
        "# print(\"Test Loss:\", loss)\n",
        "# print(\"Test Accuracy:\", accuracy)\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(train_images, train_labels_categorical, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(train_images) // batch_size,\n",
        "                    epochs=epochs)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_accuracy = model.evaluate(train_images, train_labels_categorical)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels_categorical)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "\n",
        "# Make predictions on a single image\n",
        "image_path = test_image_paths[0]\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, (img_width, img_height))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = image / 255.0\n",
        "\n",
        "prediction = model.predict(image)\n",
        "predicted_label = np.argmax(prediction)\n",
        "\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the training history\n",
        "loss_history = history.history['loss']\n",
        "accuracy_history = history.history['accuracy']\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(range(epochs), loss_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training accuracy\n",
        "plt.plot(range(epochs), accuracy_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Plot the testing accuracy\n",
        "# plt.plot(range(epochs), history.history['val_accuracy'])\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Testing Accuracy')\n",
        "# plt.title('Testing Accuracy')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "JYqykLR4EOFm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "outputId": "c0ed2573-2b22-4cb3-85ac-36715e2fea9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predictions = model.predict(test_images)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate the metrics\n",
        "report = classification_report(test_labels, predicted_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "9g7s1rAxEQtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b85f29-6ab9-48c0-de89-4d96b3cbe9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 19s 789ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.02      0.04        91\n",
            "           1       0.33      0.06      0.11        48\n",
            "           2       0.00      0.00      0.00        51\n",
            "           3       0.46      0.85      0.60       189\n",
            "           4       0.88      0.90      0.89       366\n",
            "\n",
            "    accuracy                           0.67       745\n",
            "   macro avg       0.36      0.37      0.33       745\n",
            "weighted avg       0.59      0.67      0.60       745\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(test_labels, predicted_labels)\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(\"Correlation Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "ZHHD12IbET9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ee835a-79c8-48ad-b0b5-8a6f140796d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n",
            "[[  2   1   0  81   7]\n",
            " [  2   3   0  34   9]\n",
            " [  2   1   0  41   7]\n",
            " [  4   4   0 161  20]\n",
            " [  4   0   0  32 330]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/drive/MyDrive/testt/Copy of 0a3202889f4d.png\"\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, (img_width, img_height))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = image / 255.0\n",
        "\n",
        "# Make predictions on the image\n",
        "prediction = model.predict(image)\n",
        "predicted_label = np.argmax(prediction)\n",
        "\n",
        "# Get the class name for the predicted label\n",
        "class_name = subfolders[predicted_label]\n",
        "\n",
        "# Display the result\n",
        "print(\"Predicted Class:\", class_name)\n"
      ],
      "metadata": {
        "id": "sSDe9C9rw_Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089496cb-49ff-46a1-9707-0236e9fa0be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step\n",
            "Predicted Class: Moderate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeViHu4JEvLn",
        "outputId": "4b1141e8-3769-4d49-c510-3240b09bba4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autoviz\n",
            "  Downloading autoviz-0.1.603-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from autoviz) (0.13.5)\n",
            "Requirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from autoviz) (0.12.2)\n",
            "Requirement already satisfied: fsspec>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from autoviz) (2023.4.0)\n",
            "Requirement already satisfied: xgboost>=0.82 in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.7.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from autoviz) (0.17.1)\n",
            "Requirement already satisfied: holoviews>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.15.4)\n",
            "Collecting pyamg\n",
            "  Downloading pyamg-5.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bokeh>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from autoviz) (2.4.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from autoviz) (3.8.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.3 in /usr/local/lib/python3.10/dist-packages (from autoviz) (3.7.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from autoviz) (7.34.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.2.2)\n",
            "Requirement already satisfied: panel>=0.12.6 in /usr/local/lib/python3.10/dist-packages (from autoviz) (0.14.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from autoviz) (4.5.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from autoviz) (1.8.2.2)\n",
            "Collecting hvplot>=0.7.3\n",
            "  Downloading hvplot-0.8.3-py2.py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from autoviz) (2.0.1)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->autoviz) (6.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->autoviz) (6.2)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->autoviz) (23.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->autoviz) (8.4.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh>=2.4.2->autoviz) (3.1.2)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.14.6->autoviz) (3.0.1)\n",
            "Requirement already satisfied: param<2.0,>=1.9.3 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.14.6->autoviz) (1.13.0)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.14.6->autoviz) (2.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.3->autoviz) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->autoviz) (2022.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (4.65.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (3.4.3)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (6.0.0)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from panel>=0.12.6->autoviz) (67.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost>=0.82->autoviz) (1.10.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (3.0.38)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (5.7.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (0.1.6)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (2.14.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (0.7.5)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->autoviz) (4.4.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->autoviz) (7.7.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.3-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->autoviz) (6.5.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->autoviz) (6.1.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->autoviz) (6.4.8)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->autoviz) (5.5.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->autoviz) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->autoviz) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->autoviz) (2022.10.31)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->autoviz) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->autoviz) (0.5.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->autoviz) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh>=2.4.2->autoviz) (2.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->autoviz) (1.16.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->autoviz) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->autoviz) (0.2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=0.12.6->autoviz) (0.5.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->autoviz) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->autoviz) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->autoviz) (3.0.7)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->autoviz) (3.6.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (4.11.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (0.7.1)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (5.8.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (0.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (4.9.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (0.2.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (0.7.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (1.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (5.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->autoviz) (0.8.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (1.8.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (1.5.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (21.3.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (23.2.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (0.16.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->autoviz) (0.17.1)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.12.6->autoviz) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.12.6->autoviz) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.12.6->autoviz) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.12.6->autoviz) (2022.12.7)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->autoviz) (3.3.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->autoviz) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->autoviz) (2.16.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->autoviz) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->autoviz) (2.4.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->autoviz) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->autoviz) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->autoviz) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->autoviz) (2.21)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=5d491eef28de6f8e525e4fbdd1cf796d8721fdc900d77f733a23b22c57729fab\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: qtpy, jedi, emoji, pyamg, qtconsole, hvplot, jupyter, autoviz\n",
            "Successfully installed autoviz-0.1.603 emoji-2.2.0 hvplot-0.8.3 jedi-0.18.2 jupyter-1.0.0 pyamg-5.0.0 qtconsole-5.4.3 qtpy-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from autoviz.AutoViz_Class import AutoViz_Class\n",
        "\n",
        "# Set the path to the main folder containing the subfolders\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "# Initialize AutoViz class\n",
        "AV = AutoViz_Class()\n",
        "\n",
        "# Collect the file paths of all the image files\n",
        "image_files = []\n",
        "for root, dirs, files in os.walk(main_folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\"):\n",
        "            image_files.append(os.path.join(root, file))\n",
        "\n",
        "# Create a DataFrame with a single column containing the file paths\n",
        "df = pd.DataFrame({\"filename\": image_files})SS\n",
        "\n",
        "# Visualize the dataset\n",
        "AV.AutoViz(filename=None, sep=\",\", depVar=\"\", dfte=df, header=0, verbose=1, lowess=False,\n",
        "           chart_format=\"svg\", max_rows_analyzed=None, max_cols_analyzed=None)\n"
      ],
      "metadata": {
        "id": "bb6npgCvEsh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sweetviz"
      ],
      "metadata": {
        "id": "DoYpxl-bFPN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sweetviz as sv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "main_folder_path = \"/content/drive/MyDrive/Diabetic Retinopathy 224x224 Gaussian Filtered/gaussian_filtered_images/gaussian_filtered_images\"\n",
        "\n",
        "image_files = []\n",
        "for root, dirs, files in os.walk(main_folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\"):\n",
        "            image_files.append(os.path.join(root, file))\n",
        "\n",
        "df = pd.DataFrame({\"filename\": image_files})\n",
        "\n",
        "report = sv.analyze(df)\n",
        "# report.show_html()\n",
        "# report.show_text()\n",
        "report.show_notebook()\n"
      ],
      "metadata": {
        "id": "gnZc3z2KExs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('diabetic.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "7ytgxOT3GYH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### color to grayscale\n",
        "import cv2\n",
        "import os\n",
        "### Load the input image\n",
        "folder1 = \"path of data folder\"\n",
        "folder2 = \"new folder path\"\n",
        "for filename in os.listdir(folder1):\n",
        "    image = cv2.imread(os.path.join(folder1, filename))\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    cv2.imwrite(os.path.join(folder2, filename), gray_image)"
      ],
      "metadata": {
        "id": "ycQIYKHNH3E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### code to crop images\n",
        "import cv2\n",
        "import os\n",
        "### Load the input image\n",
        "folder1 = \"path of data folder\"\n",
        "folder2 = \"new folder path\"\n",
        "for filename in os.listdir(folder1):\n",
        "    image = cv2.imread(os.path.join(folder1, filename))\n",
        "    x, y, z = image.shape\n",
        "    new_image = image[0:y, 376:x - 376]\n",
        "    cv2.imwrite(os.path.join(folder2, filename), new_image)"
      ],
      "metadata": {
        "id": "kOBFAWFQhHkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "# Load the input image\n",
        "folder1 = \"C:/Users/Anmol/Desktop/DATASET/TRAIN/O\"\n",
        "folder2 = \"C:/Users/Anmol/Desktop/New folder/TRAIN/O/\"\n",
        "\n",
        "for filename in os.listdir(folder1):\n",
        "    image = cv2.imread(os.path.join(folder1, filename))\n",
        "    x, y, z = image.shape\n",
        "    new_image = cv2.resize(image, (128, 128), interpolation = cv2.INTER_CUBIC)\n",
        "    cv2.imwrite(os.path.join(folder2, filename), new_image)"
      ],
      "metadata": {
        "id": "HowumpGghOO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}